[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "Quantitative Finance Lab\nWelcome to the quantitative finance lab. This book documents methods, experiments, and results.\n\nNotebooks will be added later under /notebooks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantitative Finance Lab</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "This project is a workspace for quantitative finance experiments and reproducible research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "",
    "text": "1.1 Fixed-coupon bond cashflows\nA standard coupon bond is defined by: - notional \\(N\\) (principal. the money that you get back) - annual coupon rate \\(c\\) (the interest rate of the bond) - maturity \\(T\\) (the time that bond ends and you get back principal and interest) - coupon frequency \\(f\\) (the amount of payments per year)\nCoupon payment each period is \\(\\dfrac{c}{f}N\\).\nPayment times are \\(t_i=\\dfrac{i}{f}\\) for \\(i=1,2,\\dots,n\\)      where \\(n=fT\\)\nCashflows for each period \\(i\\) is \\(CF_i=\\dfrac{c}{f}N\\)         \\(i=1,\\dots,n-1\\)\nfinal cash flow: \\(CF_n=\\dfrac{c}{f}N+N\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "3) Discount Factors, Zero Rates, and Forward Rates",
    "text": "3) Discount Factors, Zero Rates, and Forward Rates\n\n3.1 Discount factor\n\\(D(t)\\) is the present value of receiving 1 unit of currency at time \\(t\\). For example what does 1 dollar in 20 years worth now.\n\n\n3.2 Zero rate (continuous compounding)\n\\(z(t)\\) is the constant rate that discounts a payment in \\(t\\) to present. for example, if we want to know discount factor of 1 dollar in 20 years we need an annual rate to compute the present value. that’s zero rate.\nDefine \\(z(t)\\) by \\(D(t)=e^{-z(t)t}\\)            \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\n\n3.3 Instantaneous forward rate\n\\(f(t)\\) is the slope of \\(z(t)\\) which tells us how much the rate of return is very close to the time of maturity. it is used because zero rate is smooth and in instant time we need to have exact forward rate\n\\(f(t)=-\\dfrac{d}{dt}\\ln D(t)\\)            \\(D(t)=\\exp\\left(-\\int_0^t f(u)\\,du\\right)\\)\n\n\n3.4 Discrete forward over an interval\nFor \\(t_1&lt;t_2\\), the continuously-compounded forward rate for the interval is \\(F(t_1,t_2)=\\dfrac{\\ln D(t_1)-\\ln D(t_2)}{t_2-t_1}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "4) Par Yield Implied by a Curve",
    "text": "4) Par Yield Implied by a Curve\nThis is used for building yield curve and validation to see if the predicted rate is close to real rate based on curve. what is the rate that makes the price of bond (PV of cashflows) equal to 1?\nGiven a curve \\(D(t)\\), the par coupon rate for maturity \\(T\\) and frequency \\(f\\) solves\n\\(1=\\sum_{i=1}^{n}\\dfrac{c}{f}D(t_i)+D(T)\\)            \\(t_i=i/f\\), \\(n=fT\\).\n\\(1=\\dfrac{c}{f}\\sum_{i=1}^{n}D(t_i)+D(T)\\)\nand finally we get to \\(c=f\\,\\dfrac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nShort maturities (&lt;1Y) often use money-market conventions because they are mostly single payment. Two common ones: - continuous: \\(y=-\\dfrac{\\ln D(T)}{T}\\) - simple: \\(y=\\dfrac{1/D(T)-1}{T}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "5) Bootstrapping Discount Factors from Par Yields",
    "text": "5) Bootstrapping Discount Factors from Par Yields\nBootstrapping constructs \\(D(T)\\) at market tenors from observed par yields. in this way we can have a function of time to discount a payment in any maturity based on the real yields that we have.\n\n5.1 Short end (&lt;1Y) convention\nFor \\(T&lt;1\\) we use the money market convention again. for calculating discount factor:\n\ncontinuous convention: \\(D(T)=e^{-y(T)T}\\)\nsimple convention: \\(D(T)=\\dfrac{1}{1+y(T)T}\\)\n\n\n\n5.2 Bootstrapping for coupon tenors (T ≥ 1)\nLet \\(c=y(T)\\) be the market par yield at maturity \\(T\\) (used as coupon rate). With frequency \\(f\\) and cashflow times \\(t_i=i/f\\):\nPar condition (normalized notional 1): \\(1=\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)+\\left(1+\\dfrac{c}{f}\\right)D(T)\\)\nSolve for the new unknown \\(D(T)\\): \\(D(T)=\\dfrac{1-\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)}{1+\\dfrac{c}{f}}\\)\nif we have the earlier coupons discount factor, we know everything on the right side of equation. so we can solve it and get to \\(D(T)\\)\nit’s called bootstrapping because we first compute the \\(D(T&lt;1Y)\\) with short end convention, then we use that to compute \\(D(1Y)\\) and then use them for \\(D(2Y)\\) until the last maturity (30Y)\n\n\n5.3 Interpolating discount factors at coupon dates\nBootstrapping needs \\(D(t_i)\\) at coupon dates, but you often only have DFs at pillar maturities.\nA robust choice is log-linear interpolation: If \\(T_a&lt;t&lt;T_b\\), then\n\\(\\ln D(t)=\\ln D(T_a)+\\dfrac{t-T_a}{T_b-T_a}\\left(\\ln D(T_b)-\\ln D(T_a)\\right)\\)\nSo \\(D(t)=\\exp\\left(\\ln D(t)\\right)\\)\n\ndf_dec = df.copy()\ndf_dec[tenor_cols] = df_dec[tenor_cols] / 100.0\n\nshort_end_convention = \"continuous\"\nf = 2\nmin_d = 1e-12\n\n\ndef labels_to_T(labels):\n    T = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        if labu.endswith(\"M\"):\n            T.append(int(labu[:-1]) / 12.0)\n        else:\n            T.append(float(int(labu[:-1])))\n    return np.array(T, dtype=float)\n\n\ndef get_par_from_row(row):\n    y = row[tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    labels = [tenor_cols[i] for i in range(len(tenor_cols)) if mask[i]]\n    if len(labels) == 0:\n        return None\n\n    par = y.values[mask].astype(float)\n    T = labels_to_T(labels)\n\n    i = np.argsort(T)\n    T = T[i]\n    par = par[i]\n    labels = [labels[i] for i in i]\n\n    return T, par, labels\n\n\ndef get_par_for_date(date):\n    row = df_dec.loc[date]\n    return get_par_from_row(row)\n\n\ndef short_end_df(Ti, ri):\n    if short_end_convention == \"continuous\":\n        return math.exp(-ri * Ti)\n    return 1.0 / (1.0 + ri * Ti)\n\n\ndef price_error_loglinear(d_T, Ti, t_prev, d_prev, times_interp, c, pv_known):\n    d_T = max(float(d_T), min_d)\n    pv_interp = 0.0\n    if len(times_interp) &gt; 0:\n        w = (times_interp - t_prev) / (Ti - t_prev)\n        log_d = (1 - w) * np.log(d_prev) + w * np.log(d_T)\n        d_interp = np.exp(log_d)\n        pv_interp = np.sum((c / f) * d_interp)\n    return pv_known + pv_interp + d_T - 1.0\n\n\ndef solve_df_long_end(Ti, ri, d_map):\n    c = float(ri)\n    n = int(round(Ti * f))\n    times = np.array([k / f for k in range(1, n + 1)], dtype=float)\n\n\n    known_T = np.array(sorted(d_map.keys()), dtype=float)\n    known_D = np.array([d_map[t] for t in known_T], dtype=float)\n    known_D = np.clip(known_D, min_d, None)\n\n\n    t_prev = known_T[-1]\n    d_prev = known_D[-1]\n\n    times_known = times[times &lt;= t_prev + 1e-12]\n    times_interp = times[times &gt; t_prev + 1e-12]\n\n    pv_known = 0.0\n    if len(times_known) &gt; 0:\n        log_known_D = np.log(known_D)\n        log_df_known = np.interp(times_known, known_T, log_known_D)\n        d_known = np.exp(log_df_known)\n        pv_known = np.sum((c / f) * d_known)\n\n    lo = min_d\n    hi = d_prev\n    f_lo = price_error_loglinear(lo, Ti, t_prev, d_prev, times_interp, c, pv_known)\n    f_hi = price_error_loglinear(hi, Ti, t_prev, d_prev, times_interp, c, pv_known)\n\n    if f_lo * f_hi &gt; 0:\n    \n        log_known_D = np.log(known_D)\n        log_df_cpn = np.interp(\n            times[:-1],\n            known_T,\n            log_known_D,\n            left=log_known_D[0],\n            right=log_known_D[-1],\n        )\n        d_cpn = np.exp(log_df_cpn)\n        pv_coupons = np.sum((c / f) * d_cpn)\n        d_T = (1.0 - pv_coupons) / (1.0 + c / f)\n    else:\n        for _ in range(100):\n            mid = 0.5 * (lo + hi)\n            f_mid = price_error_loglinear(mid, Ti, t_prev, d_prev, times_interp, c, pv_known)\n            if f_lo * f_mid &lt;= 0:\n                hi = mid\n                f_hi = f_mid\n            else:\n                lo = mid\n                f_lo = f_mid\n            if abs(hi - lo) &lt; 1e-12:\n                break\n        d_T = 0.5 * (lo + hi)\n\n    return d_T\n\n\ndef bootstrap_from_inputs(T, par, labels, date=None):\n    d_map = {}\n\n    # short convention\n    for Ti, ri in zip(T, par, strict=True):\n        if Ti &lt; 1.0:\n            d_T = short_end_df(Ti, ri)\n            d_map[Ti] = max(float(d_T), min_d)\n            continue\n\n        d_T = solve_df_long_end(Ti, ri, d_map)\n        if (not np.isfinite(d_T)) or (d_T &lt;= 0):\n            d_T = min_d\n        d_map[Ti] = max(float(d_T), min_d)\n\n    dfs = np.array([d_map[t] for t in T], dtype=float)\n\n    return {\n        \"date\": date,\n        \"T\": T,\n        \"par\": par,\n        \"labels\": labels,\n        \"dfs\": dfs,\n    }\n\n\ndef bootstrap_pillars(date):\n    result = get_par_for_date(date)\n    if result is None:\n        return None\n\n    T, par, labels = result\n    return bootstrap_from_inputs(T, par, labels, date=date)\n\n\n# we bootstrap discount factors at the last available date for now\nbase_date = df_dec.index[-1]\npillars = bootstrap_pillars(base_date)\n\nT = pillars[\"T\"]\npar = pillars[\"par\"]\nlabels = pillars[\"labels\"]\ndfs = pillars[\"dfs\"]\n\nprint(\"Base date:\", base_date.date())\nprint(\"Tenors used:\", labels)\nprint(\"First 5 pillar DFs:\", dfs)\n\nplt.figure()\nplt.plot(T, dfs, marker=\"o\")\nplt.title(\"Bootstrapped Discount Factors\")\nplt.xlabel(\"Maturity T\")\nplt.ylabel(\"Discount Factor D(t)\")\nplt.show()\n\nBase date: 2026-01-28\nTenors used: ['1M', '2M', '3M', '4M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\nFirst 5 pillar DFs: [0.99687157 0.99383574 0.99084219 0.98774241 0.98201372 0.96571989\n 0.93185753 0.89680276 0.82670445 0.75353422 0.65216175 0.37090153\n 0.22566195]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "6) Turning Bootstrapped Pillars into a Full Curve",
    "text": "6) Turning Bootstrapped Pillars into a Full Curve\nAfter bootstrapping we have pillars \\((T_j, D(T_j))\\) or \\((T_j, z(T_j))\\). Now we want to define continuous functions \\(D(t)\\) and \\(z(t)\\) for all \\(t\\).\n\n6.1 Method A: Log-linear discount factors\nInterpolate \\(\\ln D(t)\\) linearly between pillars (just like for the T we had. we do the same thing between them):\n\\(\\ln D(t)=\\text{linear interp of } \\{\\ln D(T_j)\\}\\)\nThen \\(D(t)=\\exp(\\ln D(t))\\)\nand \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\ndef loglinear_curve(T, dfs):\n    loglinear_log_dfs = np.log(dfs)\n\n    loglinear_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    loglinear_log_df_grid = np.interp(\n        loglinear_grid,\n        T,\n        loglinear_log_dfs,\n        left=loglinear_log_dfs[0],\n        right=loglinear_log_dfs[-1],\n    )\n    loglinear_df_grid = np.exp(loglinear_log_df_grid)\n\n    # zero rate\n    loglinear_z_grid = -np.log(loglinear_df_grid) / loglinear_grid\n\n    # instantaneous forward rate\n    loglinear_fwd_grid = -np.gradient(np.log(loglinear_df_grid), loglinear_grid)\n\n    def loglinear_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(\n            t,\n            T,\n            loglinear_log_dfs,\n            left=loglinear_log_dfs[0],\n            right=loglinear_log_dfs[-1],\n        )\n        return np.exp(log_df)\n\n    return {\n        \"name\": \"Log-linear DF\",\n        \"grid\": loglinear_grid,\n        \"df_grid\": loglinear_df_grid,\n        \"z_grid\": loglinear_z_grid,\n        \"fwd_grid\": loglinear_fwd_grid,\n        \"df_func\": loglinear_df_func,\n        \"log_dfs\": loglinear_log_dfs,\n    }\n\n\nloglinear_curve_data = loglinear_curve(T, dfs)\nloglinear_grid = loglinear_curve_data[\"grid\"]\nloglinear_df_grid = loglinear_curve_data[\"df_grid\"]\nloglinear_z_grid = loglinear_curve_data[\"z_grid\"]\nloglinear_fwd_grid = loglinear_curve_data[\"fwd_grid\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"loglinear\"] = loglinear_curve_data\n\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_z_grid * 100.0)\nplt.title(\"Zero Curve (log-linear)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Zero Rate (%)\")\nplt.show()\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_fwd_grid * 100.0)\nplt.title(\"Instantaneous Forward Rate (derivative of ln(DF))\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Forward Rate (%)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "7) Curve Models Using Zero-Rate Smoothing",
    "text": "7) Curve Models Using Zero-Rate Smoothing\nInstead of interpolating \\(D\\), we can interpolate \\(z\\) and recover \\(D(t)=e^{-z(t)t}\\).\n\n7.1 PCHIP on zero rates (Piecewise Cubic Hermite Interpolating Polynomial)\nGiven nodes \\(x_j=T_j\\) and \\(y_j=z(T_j)\\), PCHIP builds a piecewise cubic polynomial on each interval:\n\\(p_j(t)=a_j(t-x_j)^3+b_j(t-x_j)^2+c_j(t-x_j)+d_j\\) for \\(t\\in[x_j,x_{j+1}]\\)\nConstraints include: - \\(p_j(x_j)=y_j\\) and \\(p_j(x_{j+1})=y_{j+1}\\) - first derivatives are chosen by shape-preserving slope rules to reduce overshoot\nwe define \\(z(t)=p_j(t)\\)\nthen \\(D(t)=e^{-z(t)t}\\)\n\nfrom scipy.interpolate import PchipInterpolator\n\n\ndef pchip_curve(T, dfs):\n    pchip_zeros = -np.log(np.clip(dfs, min_d, None)) / T\n\n    pchip_z = PchipInterpolator(T, pchip_zeros, extrapolate=True)\n    pchip_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    pchip_z_grid = pchip_z(pchip_grid)\n    pchip_df_grid = np.exp(-pchip_z_grid * pchip_grid)\n\n    pchip_fwd_grid = -np.gradient(\n        np.log(np.clip(pchip_df_grid, min_d, None)), pchip_grid\n    )\n\n    def pchip_df_func(t):\n        t = np.array(t, dtype=float)\n        z = pchip_z(t)\n        return np.exp(-z * t)\n\n    return {\n        \"name\": \"PCHIP zero\",\n        \"grid\": pchip_grid,\n        \"df_grid\": pchip_df_grid,\n        \"z_grid\": pchip_z_grid,\n        \"fwd_grid\": pchip_fwd_grid,\n        \"df_func\": pchip_df_func,\n        \"pillar_zeros\": pchip_zeros,\n    }\n\n\npchip_curve_data = pchip_curve(T, dfs)\ncurves[\"pchip\"] = pchip_curve_data\n\n\nplt.figure()\nplt.plot(T, pchip_curve_data[\"pillar_zeros\"] * 100.0, \"o\", label=\"pillar zeros\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"z_grid\"] * 100.0, \"-\", label=\"PCHIP zero\")\nplt.title(\"Zero Curve (PCHIP)\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(T, dfs, \"o\", label=\"Pillar DFs\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"df_grid\"], \"-\", label=\"DF from PCHIP\")\nplt.title(\"Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"Instantaneous Forward Rate\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "8) Nelson–Siegel–Svensson (NSS) yield curve",
    "text": "8) Nelson–Siegel–Svensson (NSS) yield curve\nwe represent the continuous-compounded zero rate curve \\(z(t)\\) with a small number of parameters, then derive discount factors, par yields and forwards\n\n8.1 NSS zero-rate function\nFor maturity \\(t&gt;0\\), the NSS zero rate is:\n\\(z(t)=\\beta_0 +\\beta_1\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}\\right) +\\beta_2\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}-e^{-t/\\tau_1}\\right) +\\beta_3\\left(\\frac{1-e^{-t/\\tau_2}}{t/\\tau_2}-e^{-t/\\tau_2}\\right)\\)\nParameters:\n\n\\(\\beta_0\\) = long-run “level”\n\\(\\beta_1\\) = “slope” (short-end effect)\n\\(\\beta_2\\) = medium-term “curvature” (first hump)\n\\(\\beta_3\\) = additional curvature (second hump)\n\\(\\tau_1,\\tau_2&gt;0\\) control where humps occur\n\n\ndef nss_zero(t, b0,b1,b2,b3,tau1,tau2):\n    t = np.array(t, dtype=float)\n    x1 = t / tau1\n    x2 = t / tau2\n    L1 = (1.0 - np.exp(-x1)) / x1\n    C1 = L1 - np.exp(-x1)\n    C2 = (1.0 - np.exp(-x2)) / x2 - np.exp(-x2)\n    return b0 + b1*L1 + b2*C1 + b3*C2\n\n\n\n8.3 Par yield implied by NSS\nFor a coupon bond with maturity \\(T\\) and coupon frequency \\(f\\), coupon rate \\(c(T)\\) is the rate that makes the bond price equal to par (normalize notional to 1):\n\\(1=\\sum_{i=1}^{n}\\frac{c(T)}{f}D(t_i)+D(T)\\)\nSolve for \\(c(T)\\):\n\\(c(T)=f\\cdot\\frac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nFor short maturities (money-market style), a common mapping is:\ncontinuous: \\(y(T)=-\\ln D(T)/T\\)\nsimple: \\(y(T)=(1/D(T)-1)/T\\)\nbut first we have to have \\(D(t)\\)\n\ndef par_from_d(df_func, T_list, f=2):\n    T_arr = np.asarray(T_list, dtype=float)\n    out = np.full_like(T_arr, np.nan, dtype=float)\n\n    step = 1.0 / float(f)\n\n    for k, Tk in enumerate(T_arr):\n        if not np.isfinite(Tk) or Tk &lt;= 0:\n            continue\n\n        D_T = float(np.asarray(df_func([Tk],), dtype=float)[0])\n        D_T = max(D_T, min_d)\n\n        if Tk &lt; 1.0:\n            if short_end_convention == \"simple\":\n                out[k] = (1.0 / D_T - 1.0) / Tk\n            else:\n                out[k] = -np.log(D_T) / Tk\n            continue\n\n        n_full = int(np.floor(Tk * f + 1e-12))\n        times = np.arange(step, n_full * step + 1e-12, step)\n        if len(times) == 0 or abs(times[-1] - Tk) &gt; 1e-10:\n            times = np.append(times, Tk)\n\n        accr = np.diff(np.concatenate([[0.0], times]))\n        dfs = np.asarray(df_func(times), dtype=float)\n        dfs = np.clip(dfs, min_d, None)\n\n        denom = float(np.sum(accr * dfs))\n        out[k] = (1.0 - dfs[-1]) / denom if denom &gt; 0 else np.nan\n\n    return out\n\n\n\n8.2 Discount factor and forward from NSS\nOnce we have \\(z(t)\\), using continuous compounding:\n\\(D(t)=e^{-z(t)t}\\)\nand the instantaneous forward rate is:\n\\(f(t)=-\\frac{d}{dt}\\ln D(t)\\)\nWith NSS you often compute \\(f(t)\\) numerically on a grid: \\(f(t_i)\\approx -\\frac{\\ln D(t_{i+1})-\\ln D(t_{i-1})}{t_{i+1}-t_{i-1}}\\)\n\n\n\n8.4 Calibrating NSS to market par yields\nGiven observed par yields \\(y^{mkt}(T_j)\\) at tenors \\(T_j\\), we choose parameters \\(\\theta=(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\tau_1,\\tau_2)\\) to minimize a least-squares objective:\n\\(\\min_{\\theta}\\sum_{j}\\left(c^{model}(T_j;\\theta)-y^{mkt}(T_j)\\right)^2\\)\nOptionally add regularization to discourage extreme shapes: \\(\\lambda\\sum_j (z''(t_j))^2\\) or bounds on parameters.\n\nfrom scipy.optimize import minimize\n\n\ndef nss_curve(T, par):\n    def obj(theta):\n        b0, b1, b2, b3, tau1, tau2 = theta\n        z = nss_zero(T, b0, b1, b2, b3, tau1, tau2)\n        dfs = np.exp(-z * T)\n\n        # we use log-linear DF interpolation on pillars for keeping it positive\n        log_dfs = np.log(np.clip(dfs, min_d, None))\n\n        def df_func(t):\n            t = np.array(t, dtype=float)\n            log_df = np.interp(t, T, log_dfs, left=log_dfs[0], right=log_dfs[-1])\n            return np.exp(log_df)\n\n        par_model = par_from_d(df_func, T)\n        err = par_model - par\n        return float(np.mean(err**2))\n\n    # initializing with a first guess. for long run level we need something like the long-run yield level. that's why we use long term yields as guess\n    b0_0 = float(np.nanmedian(par[-3:])) if len(par) &gt;= 3 else float(np.nanmedian(par))\n    x0 = np.array([b0_0, -0.02, 0.02, 0.01, 1.5, 5.0], dtype=float)\n\n    # we use a type of quasi-Newton method for nonlinear optimization of parameters\n    pred = minimize(obj, x0, method=\"L-BFGS-B\")\n    theta = pred.x\n\n    nss_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    nss_z_grid = nss_zero(nss_grid, *theta)\n    nss_df_grid = np.exp(-nss_z_grid * nss_grid)\n\n    nss_fwd_grid = -np.gradient(\n        np.log(np.clip(nss_df_grid, min_d, None)), nss_grid\n    )\n\n    def nss_df_func(t):\n        t = np.array(t, dtype=float)\n        return np.exp(-nss_zero(t, *theta) * t)\n\n    curve = {\n        \"name\": \"NSS\",\n        \"grid\": nss_grid,\n        \"df_grid\": nss_df_grid,\n        \"z_grid\": nss_z_grid,\n        \"fwd_grid\": nss_fwd_grid,\n        \"df_func\": nss_df_func,\n        \"theta\": theta,\n    }\n\n\n    z_p = nss_zero(T, *theta)\n    d_p = np.exp(-z_p * T)\n    log_d_p = np.log(np.clip(d_p, min_d, None))\n    def df_func_p(tt):\n        return np.exp(\n            np.interp(np.array(tt, float), T, log_d_p, left=log_d_p[0], right=log_d_p[-1])\n        )\n    par_fit = par_from_d(df_func_p, T)\n\n    return curve, par_fit, pred\n\n\nnss_curve_data, par_fit, pred = nss_curve(T, par)\ntheta = nss_curve_data[\"theta\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"nss\"] = nss_curve_data\n\nprint(\"final MSE:\", pred.fun)\nprint(\"theta = [b0,b1,b2,b3,tau1,tau2] =\", np.round(theta, 6))\n\nplt.figure()\nplt.plot(T, par * 100.0, \"o\", label=\"Market par\")\nplt.plot(T, par_fit * 100.0, \"-o\", label=\"NSS implied par\")\nplt.title(\"NSS Fit to Par Yields\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Par Yield\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"NSS Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"NSS Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nfinal MSE: 3.5093289069366877e-07\ntheta = [b0,b1,b2,b3,tau1,tau2] = [ 0.053115 -0.014698 -0.031572 -0.007968  1.500251  5.000021]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "9) QP curve: smooth discount factors under exact par-bond fit",
    "text": "9) QP curve: smooth discount factors under exact par-bond fit\nAnother approach for building a yield curve that:\n\nmatches par-bond pricing equations exactly (like bootstrapping) or near-exactly (like NSS),\nis smooth,\nwill result in a positive and increasing DF curve.\n\ncan come from a Quadratic Program (QP) if the variables are discount factors on a grid and constraints are linear.\n\n9.1 Variables\nWe Pick a grid of cashflow times (like semiannual up to 30Y): \\(t_1,t_2,\\dots,t_M\\)\nwe want to get to $ = (D(t_1),,D(t_M)) $\n\n\n9.2 constraints\nFor a maturity \\(T\\) (present on the grid), par yield \\(c\\) and frequency \\(f\\):\n\\(1=\\sum_{i=1}^{n}\\frac{c}{f}D(t_i)+D(T)\\)\nThis is linear in \\(D(\\cdot)\\), so it becomes one row of: \\(A\\mathbf{d}=\\mathbf{1}\\)\nPositivity: \\(D(t_k)\\ge D_{min}\\)\nMonotone decreasing: \\(D(t_{k+1})\\le D(t_k)\\)\nThese are linear inequalities, so the problem stays convex and QP-solvable.\n\nimport cvxpy as cp\n\n\ndef qp_build_t_grid(T_obs, f):\n    T_max = float(np.max(T_obs))\n    n_grid = int(round(T_max * f))\n    t_grid = np.unique(\n        np.concatenate(\n            [\n                np.array([i / f for i in range(1, n_grid + 1)], dtype=float),\n                T_obs,\n            ]\n        )\n    )\n    t_grid = np.array(sorted(t_grid), dtype=float)\n    grid_index = {float(np.round(t, 10)): i for i, t in enumerate(t_grid)}\n    return t_grid, grid_index\n\n\ndef qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d):\n    d = cp.Variable(len(t_grid))\n\n    constraints = []\n    constraints += [d &gt;= min_d]\n    constraints += [d[1:] &lt;= d[:-1]]\n\n    for Tk, yk in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            key = float(np.round(Tk, 10))\n            if key in grid_index:\n                i = grid_index[key]\n                df_target = float(np.exp(-yk * Tk))\n                constraints += [d[i] == df_target]\n\n    for Tk, ck in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            continue\n        keyT = float(np.round(Tk, 10))\n        if keyT not in grid_index:\n            continue\n        iT = grid_index[keyT]\n        n = int(round(Tk * f))\n\n        coupon_idx = []\n        for j in range(1, n + 1):\n            key = float(np.round(j / f, 10))\n            coupon_idx.append(grid_index[key])\n\n        constraints += [cp.sum((ck / f) * d[coupon_idx]) + d[iT] == 1.0]\n\n    return d, constraints\n\n\n\n\n\n\n9.3 Smoothness objective (quadratic)\nA simple convex smoothness penalty is the squared second difference of Discount Factors:\n\\(\\min_{\\mathbf{d}} \\ |\\Delta^2\\mathbf{d}|_2^2\\)\nwhere \\(\\Delta^2 d_k = d_{k+2}-2d_{k+1}+d_k\\). (Discrete version)\nThis makes the optimizer prefer sequences of discount factors that have small curvature everywhere, which results a smooth DF curve with fewer oscillations and jumps.\nwe can also add a mild “keep close to a prior curve” penalty: \\(\\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\nTotal objective: \\(\\min_{\\mathbf{d}} \\ \\lambda|\\Delta^2\\mathbf{d}|_2^2 + \\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\n\n\n9.4 zero and forward curves\nOnce we have \\(D(t)\\) on a grid:\n\\(z(t_k)=-\\ln D(t_k)/t_k\\)\n\\(f(t_k)\\approx -\\dfrac{\\ln D(t_{k+1})-\\ln D(t_k)}{t_{k+1}-t_k}\\)\n\ndef qp_solve(t_grid, d, constraints, par_mkt, f, min_d):\n    lam = 1e4\n    eps = 1e-4\n    prior_rate = (\n        float(np.nanmedian(par_mkt[-3:])) if len(par_mkt) &gt;= 3 else float(np.nanmedian(par_mkt))\n    )\n    d_prior = np.exp(-prior_rate * t_grid)\n\n    d2 = d[2:] - 2 * d[1:-1] + d[:-2]\n    obj = cp.Minimize(lam * cp.sum_squares(d2) + eps * cp.sum_squares(d - d_prior))\n\n    prog = cp.Problem(obj, constraints)\n    prog.solve(solver=cp.OSQP)\n\n    d_sol = np.array(d.value).astype(float)\n    d_sol = np.clip(d_sol, min_d, None)\n\n    return d_sol, prog.status, prog.value\n\n\ndef qp_build_curve(t_grid, d_sol):\n    qp_grid = np.linspace(max(1 / 12, t_grid.min()), 30.0, 1000)\n    qp_log_d = np.log(d_sol)\n    qp_log_df_grid = np.interp(qp_grid, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n\n    qp_df_grid = np.exp(qp_log_df_grid)\n    qp_z_grid = -np.log(qp_df_grid) / np.maximum(qp_grid, 1e-8)\n    qp_fwd_grid = -np.gradient(np.log(qp_df_grid), qp_grid)\n\n    def qp_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(t, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n        return np.exp(log_df)\n\n    curve = {\n        \"name\": \"QP DF\",\n        \"grid\": qp_grid,\n        \"df_grid\": qp_df_grid,\n        \"z_grid\": qp_z_grid,\n        \"fwd_grid\": qp_fwd_grid,\n        \"df_func\": qp_df_func,\n    }\n\n    return curve\n\n\ndef qp_curve(labels, par_mkt, f=2, min_d=1e-10):\n    T_obs = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        T_obs.append(int(labu[:-1]) / 12.0 if labu.endswith(\"M\") else float(int(labu[:-1])))\n    T_obs = np.array(T_obs, dtype=float)\n\n    idx = np.argsort(T_obs)\n    T_obs = T_obs[idx]\n    par_mkt = par_mkt[idx]\n    labels = [labels[i] for i in idx]\n\n    t_grid, grid_index = qp_build_t_grid(T_obs, f)\n    d, constraints = qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d)\n    d_sol, status, value = qp_solve(t_grid, d, constraints, par_mkt, f, min_d)\n\n    curve = qp_build_curve(t_grid, d_sol)\n\n    state = {\n        \"t_grid\": t_grid,\n        \"d_sol\": d_sol,\n        \"constraints\": constraints,\n        \"status\": status,\n        \"value\": value,\n    }\n\n    return curve, state\n\n\nqp_curve_data, qp_state = qp_curve(labels, par)\n\ncurves[\"qp\"] = qp_curve_data\n\nprint(\"status:\", qp_state[\"status\"], \",  value:\", qp_state[\"value\"])\n\n\nplt.figure()\nplt.plot(qp_state[\"t_grid\"], qp_state[\"d_sol\"], \"o\", markersize=3, label=\"QP DF\")\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"df_grid\"], \"-\", label=\"Discount Factor (log-linear)\")\nplt.title(\"QP Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"QP Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"QP Instantaneous Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nstatus: optimal ,  value: 1.2368745747617473",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "11) Synthetic Bond Issuance (Issued at Par)",
    "text": "11) Synthetic Bond Issuance (Issued at Par)\nbecause we don’t have official data for this part, We simulate a monthly issuance program using par yields to create a rolling book of bonds so we can analyze our models and implement the next topics. Each month, we pretend the Treasury issues new par bonds at a few maturities (like 2Y, 5Y, 10Y, 30Y). The coupon of each new bond is set to that month’s par yield at that maturity (from the dataset). Because coupon = par yield at issuance, each new bond starts at price approximately equal to 1 (par).\n\nDetails of the bond\n\nfor each month \\(t_0\\) we create a new bond with maturity \\(T\\) and frequency \\(f\\)\n\\(f\\) = semiannual\n\\(T = {2Y, 5Y, 10Y, 30Y}\\)\ncoupon: \\(c_d(T)=y_d(T)\\) (the market par yield at that date and maturity)\nnotional \\(N=1\\)\n\nBasically we buy bonds with 4 different maturities every month and keep it and get interest every month until maturity of those bonds. so we have 4 books for 4 different bonds (maturities). and our portfolio is based on these four books.\nSo each month the outgoing cashflow is buying the bonds and ingoing is all the interest and maybe principal of all the bonds that we have bought.\n\nissue_maturities = [2, 5, 10, 30]\nissue_labels = {2: \"2Y\", 5: \"5Y\", 10: \"10Y\", 30: \"30Y\"}\n\n\nmonth_end_curve = df_dec[tenor_cols].resample(\"M\").last()\nissue_dates = month_end_curve.index\n\n\ndef yearfrac(t0, t1):\n    return (t1 - t0).days / 365\n\n\ndef bond_cashflows(c, T, f=2):\n    times = np.arange(1 / f, T + 1e-9, 1 / f)\n    cfs = np.full_like(times, c / f)\n    cfs[-1] += 1.0\n    return times, cfs\n\n\ndef price_bond(df_func, times, cfs, delta):\n    mask = times &gt; delta + 1e-12\n    if not np.any(mask):\n        return 0.0\n    t_rem = times[mask] - delta\n    cf_rem = cfs[mask]\n    return float(np.sum(cf_rem * df_func(t_rem)))\n\n\n\nissuance_book = {T: [] for T in issue_maturities}\nfor d in issue_dates:\n    row = month_end_curve.loc[d]\n    for T in issue_maturities:\n        label = issue_labels[T]\n        c = float(row.get(label, np.nan))\n        if not np.isfinite(c):\n            continue\n        times, cfs = bond_cashflows(c, T, f)\n        issuance_book[T].append({\n            \"issue_date\": d,\n            \"coupon\": c,\n            \"times\": times,\n            \"cfs\": cfs,\n        })\n\nissuance_summary = pd.DataFrame({\n    \"n_bonds\": {T: len(issuance_book[T]) for T in issue_maturities},\n    \"first_issue\": {T: issuance_book[T][0][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n    \"last_issue\": {T: issuance_book[T][-1][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n})\n\nprint(\"Issuance summary:\")\ndisplay(issuance_summary)\n\nIssuance summary:\n\n\n\n\n\n\n\n\n\nn_bonds\nfirst_issue\nlast_issue\n\n\n\n\n2\n433\n1990-01-31\n2026-01-31\n\n\n5\n433\n1990-01-31\n2026-01-31\n\n\n10\n433\n1990-01-31\n2026-01-31\n\n\n30\n386\n1990-01-31\n2026-01-31",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "12) Pricing an Issued Bond at a Later Date",
    "text": "12) Pricing an Issued Bond at a Later Date\nIf issue date be \\(t_0\\) and valuation date be \\(t\\). Elapsed time is the amount of time that has been passed from the bond issued at \\(t_0\\) in valuation time \\(t\\): \\(\\Delta=\\tau(t_0,t)\\)\nreminder: \\(\\tau(t_0,t_1)=\\dfrac{\\text{days}(t_0,t_1)}{365}\\)\nOriginal scheduled payment times from issue are \\(t_i=i/f\\). we only price the bonds that \\(t_i&gt;\\Delta\\) because these are the cashflows that have accured up until time \\(t\\) and the remaining time to payment from valuation date for each cashflow is: \\(\\tau_i(t)=t_i-\\Delta\\)\nPrice using the curve at valuation date \\(t\\): \\(P_t=\\sum_{i:t_i&gt;\\Delta} CF_i\\,D_t(\\tau_i(t))\\)\nFor a book of issues \\(\\mathcal{B}_t\\) with weights \\(w_b\\) (we don’t have weights here): \\(PV_t=\\sum_{b\\in \\mathcal{B}_t} w_b\\,P_t(b)\\)\n\ndef book_pv_cutoff(valuation_date, df_func, cutoff_date):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], valuation_date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ndef shifted_df_func(df_func, shift_func):\n    def _f(t):\n        t = np.array(t, dtype=float)\n        return df_func(t) * np.exp(-shift_func(t) * t)\n    return _f\n\n\ndef key_bump_func(key, bump_bp=1.0):\n    values = np.zeros(len(issue_maturities), dtype=float)\n    key_idx = issue_maturities.index(key)\n    values[key_idx] = bump_bp / 10000.0\n\n    def shift(t):\n        t = np.array(t, dtype=float)\n        return np.interp(t, issue_maturities, values, left=0.0, right=0.0)\n\n    return shift\n\n\ndef curve_date_for(d):\n    if d in df_dec.index:\n        return d\n    idx = df_dec.index.searchsorted(d, side=\"right\") - 1\n    if idx &lt; 0:\n        return None\n    return df_dec.index[idx]\n\n\ndef book_pv(date, df_func):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ncurves_cache = {}\n\n\ndef get_curves_for(date):\n    if date in curves_cache:\n        return curves_cache[date]\n    out = build_curves_for_date(date)\n    if out is None:\n        curves_cache[date] = None\n        return None\n    _, curves_d, _ = out\n    curves_cache[date] = curves_d\n    return curves_d\n\n\n\n\nmetrics_rows = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df0 = curve[\"df_func\"]\n        pv0, _ = book_pv_cutoff(date, df0, cutoff_date=date)\n        metrics_rows.append({\"date\": date, \"method\": method, \"pv\": pv0})\n\nmetrics_df = pd.DataFrame(metrics_rows).set_index([\"date\", \"method\"]).sort_index()\n\n\n\nmethods = [\"loglinear\", \"pchip\", \"nss\", \"qp\"]\n\npv_total = (\n    metrics_df.reset_index()\n    .pivot(index=\"date\", columns=\"method\", values=\"pv\")\n    .sort_index()\n)\n\npv_rows = []\nfailed = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df_func = curve[\"df_func\"]\n        _, buckets = book_pv_cutoff(date, df_func, cutoff_date=date)\n        for T in issue_maturities:\n            pv_rows.append({\"date\": date, \"method\": method, \"maturity\": T, \"pv\": buckets.get(T, 0.0)})\n\npv_df = pd.DataFrame(pv_rows)\npv_buckets = pv_df.pivot_table(index=\"date\", columns=[\"method\", \"maturity\"], values=\"pv\").sort_index()\n\nprint(\"Total PV by method (last day):\")\ndisplay(pv_total)\n\nprint(\"Bucket PV (last date)\")\nlast_date = pv_df[\"date\"].max()\nlast_bucket = pv_df[pv_df[\"date\"] == last_date].pivot_table(index=\"maturity\", columns=\"method\", values=\"pv\")\ndisplay(last_bucket)\n\nplt.figure()\nfor method in pv_total.columns:\n    plt.plot(pv_total.index, pv_total[method], label=method)\nplt.title(\"Synthetic Book Total PV (Monthly)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nplt.figure()\nlast_bucket.plot(kind=\"bar\", ax=plt.gca())\nplt.title(f\"Each bucket PV by Method ({last_date.date()})\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nTotal PV by method (last day):\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\ndate\n\n\n\n\n\n\n\n\n1990-01-31\n3.999816\n3.999349\n4.001830\n4.004113\n\n\n1990-02-28\n8.007579\n8.006109\n8.012242\n8.016726\n\n\n1990-03-31\n12.003136\n12.000914\n12.004738\n12.017158\n\n\n1990-04-30\n15.838917\n15.837035\n15.838465\n15.859415\n\n\n1990-05-31\n20.350563\n20.347476\n20.353669\n20.375817\n\n\n...\n...\n...\n...\n...\n\n\n2025-09-30\n484.048043\n485.413949\n485.289343\n486.914561\n\n\n2025-10-31\n486.071558\n486.930897\n487.319872\n488.950688\n\n\n2025-11-30\n487.714383\n489.811404\n489.252157\n491.052541\n\n\n2025-12-31\n482.340428\n484.351486\n483.847004\n485.803466\n\n\n2026-01-31\n481.069937\n482.642714\n482.367938\n484.081282\n\n\n\n\n433 rows × 4 columns\n\n\n\nBucket PV (last date)\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\nmaturity\n\n\n\n\n\n\n\n\n2\n24.258462\n24.266940\n24.265987\n24.267663\n\n\n5\n60.518216\n60.565183\n60.549000\n60.551390\n\n\n10\n115.921913\n116.063170\n116.097292\n116.098827\n\n\n30\n280.371346\n281.747421\n281.455658\n283.163402",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "title": "1) Bonds, Coupons, Treasuries, and Par Yields",
    "section": "15) Risk and sensitivity measures of bonds",
    "text": "15) Risk and sensitivity measures of bonds\n\n15.1 PV01/ DV01\nfor measuring the risk of these bonds, we can use the sensitivity of the price of bond to a little change in interest rates.\nPV01 is the change in PV of the bond when the curve bumps by 1 basis point. DV01 is the value in dollar unit of that which is kind of the same concept\n\\(\\dfrac{dP}{dy}=-\\sum_{i=1}^n CF_i,t_i,D(t_i)\\)\nFor approximation, we take \\(\\Delta=0.0001\\) (1 basis point move), Parallel-shifted discount factor will be:\n\\(D^{up}(t)=\\exp(-(z(t)+\\Delta)t)\\)\nPV01 will be:\n\\(PV01=P_0-P_{up}\\)\n\n\n15.2 Convexity\nconvexity is basically the sensitivity of PV01 to a little change in curve. and it’s the second order partial derivative of price from yield\n\\(\\dfrac{d^2P}{dy^2}=\\sum_{i=1}^n CF_i,t_i^2,D(t_i)\\)\nUsing central differences with shift size \\(\\Delta\\):\n\\(Conv=\\dfrac{P_{down}+P_{up}-2P_0}{P_0\\Delta^2}\\) where \\(P_{up}\\) uses \\(+\\Delta\\) and \\(P_{down}\\) uses \\(-\\Delta\\).\n\n\n\n15.3 Key Rate Duration (KRD)\nthe problem is that we assume that yields parallel shift. but they can twist. if 2Y goes up it doesn’t mean 10Y will go up exactly the same amount. (Duration is a normalized version of PV01)\nSo we choose key tenors \\(k_1&lt;k_2&lt;\\dots&lt;k_m\\). as our key rates and measure the sensitivity of them while the other tenors in the curve stay the same.\nTent / triangular bump shape\nDefine localized bump functions \\(b_j(t)\\) such that \\(b_j(K_j)=1\\) and \\(b_j(t)=0\\) outside a neighborhood.\nA triangular bump with edges \\(L_j&lt;R_j\\):\n\\(b_j(t)=0\\) for \\(t\\le L_j\\)\n\\(b_j(t)=\\dfrac{t-L_j}{K_j-L_j}\\) for \\(L_j&lt;t\\le K_j\\)\n\\(b_j(t)=\\dfrac{R_j-t}{R_j-K_j}\\) for \\(K_j&lt;t\\le R_j\\)\n\\(b_j(t)=0\\) for \\(t&gt;R_j\\)\nBumped zero curve (only around key \\(k_j\\)):\n\\(z^{(j)}(t)=z(t)+\\Delta b_j(t)\\)\nSo bumped discount factors:\n\\(D^{(j)}(t)=\\exp(-(z(t)+\\Delta b_j(t))t)\\)\nPrice under the key bump:\n\\(P^{(j)}(\\Delta)=\\sum_{i=1}^n CF_i,D(t_i),e^{-\\Delta,b_j(t_i),t_i}\\)\nKRD definition (finite-difference)\n\\(KRD_j=\\dfrac{P_0-P^{(j)}}{P_0\\Delta}\\)\n\n\nrisk_rows = []\nkrd_rows = []\n\nfor (date, method), row in metrics_df.iterrows():\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    df_func = curves_d[method][\"df_func\"]\n    pv0 = row[\"pv\"]\n\n    shift_up = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up, _ = book_pv(date, shift_up)\n    pv_dn, _ = book_pv(date, shift_dn)\n\n    pv01 = (pv_dn - pv_up) / 2.0\n    convexity = (pv_up + pv_dn - 2.0 * pv0) / (pv0 * (0.0001 ** 2)) if pv0 != 0 else np.nan\n\n    risk_rows.append({\n        \"date\": date,\n        \"method\": method,\n        \"pv01\": pv01,\n        \"convexity\": convexity,\n    })\n\n    for k in issue_maturities:\n        bump = key_bump_func(k, bump_bp=1.0)\n        df_bump = shifted_df_func(df_func, bump)\n        pv_bump, _ = book_pv(date, df_bump)\n        krd = (pv0 - pv_bump) / 0.0001\n        krd_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"key\": k,\n            \"krd\": krd,\n        })\n\nrisk_df = pd.DataFrame(risk_rows).set_index([\"date\", \"method\"]).sort_index()\nmetrics_df = metrics_df.join(risk_df)\n\nkrd_df = pd.DataFrame(krd_rows).set_index([\"date\", \"method\", \"key\"]).sort_index()\n\nprint(\"Risk metrics (last date)\")\ndisplay(metrics_df[[\"pv01\", \"convexity\"]].tail(4))\n\nprint(\"KRD for last date\")\ndisplay(krd_df.tail(4))\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"pv01\"], label=method)\nplt.title(\"PV01 of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV01\")\nplt.legend()\nplt.show()\n\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"convexity\"], label=method)\nplt.title(\"Convexity of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Convexity\")\nplt.legend()\nplt.show()\n\n\n\nmethods = sorted(metrics_df.index.get_level_values(\"method\").unique())\nkrd_panel = krd_df.reset_index()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\naxes = axes.flatten()\nfig.subplots_adjust(right=0.88)\n\nfor ax, method in zip(axes, methods, strict=True):\n    data = krd_panel[krd_panel[\"method\"] == method]\n    pivot = data.pivot(index=\"date\", columns=\"key\", values=\"krd\").reindex(columns=issue_maturities)\n    im = ax.imshow(pivot.values.T, aspect=\"auto\", origin=\"lower\")\n    ax.set_title(method)\n    ax.set_yticks(range(len(issue_maturities)))\n    ax.set_yticklabels([f\"{k}Y\" for k in issue_maturities])\n\n    tick_idx = np.linspace(0, len(pivot.index) - 1, 6).astype(int)\n    ax.set_xticks(tick_idx)\n    ax.set_xticklabels([pivot.index[i].strftime(\"%Y\") for i in tick_idx])\n\ncax = fig.add_axes([0.90, 0.15, 0.02, 0.7])\nfig.colorbar(im, cax=cax, label=\"KRD\")\nfig.suptitle(\"KRD Heatmap Across Time\", y=1.02)\nplt.tight_layout(rect=[0, 0, 0.88, 1])\nplt.show()\n\nRisk metrics (last date)\n\n\n\n\n\n\n\n\n\n\npv01\nconvexity\n\n\ndate\nmethod\n\n\n\n\n\n\n2026-01-31\nloglinear\n0.357311\n107.577415\n\n\nnss\n0.359623\n108.064523\n\n\npchip\n0.358863\n107.715321\n\n\nqp\n0.362471\n108.928968\n\n\n\n\n\n\n\nKRD for last date\n\n\n\n\n\n\n\n\n\n\n\nkrd\n\n\ndate\nmethod\nkey\n\n\n\n\n\n2026-01-31\nqp\n2\n199.706778\n\n\n5\n584.865209\n\n\n10\n1608.487463\n\n\n30\n1104.026025",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1) Bonds, Coupons, Treasuries, and Par Yields</span>"
    ]
  }
]