[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "What this is\nQuantitative-Finance-Lab is a project series where each topic is developed end-to-end:\nThe goal is to build a portfolio that is both research-grade (well explained) and engineering-grade (reusable, testable, maintainable).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#what-this-is",
    "href": "index.html#what-this-is",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "Clear narrative: intuition → math → implementation\nReproducible experiments: consistent metrics, plots, and comparisons\nLibrary-first engineering: reusable components are extracted into quantfinlab",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Quantitative Finance Lab",
    "section": "Start here",
    "text": "Start here\n\nRead the Overview for conventions, reproducibility notes, and how projects are structured.\nThen open a project notebook and follow it top-to-bottom.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Quantitative Finance Lab",
    "section": "Projects",
    "text": "Projects\n\n\n\n01 — Yield Curve, Bond Pricing & Risk\n\nYield curve construction (discount factors / zero rates)\nBond pricing and key risk measures\nPractical fixed-income utilities you can reuse elsewhere\n\nOpen Project 01\n\n\n\n02 — Portfolio Optimization (Mean–Variance)\n\nMean–variance optimization pipeline\nCovariance estimators and model comparisons\nBacktest-style evaluation and performance diagnostics\n\nOpen Project 02",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#the-quantfinlab-library",
    "href": "index.html#the-quantfinlab-library",
    "title": "Quantitative Finance Lab",
    "section": "The quantfinlab library",
    "text": "The quantfinlab library\nAs the series grows, reusable code is consolidated into quantfinlab:\n\nshared fixed-income helpers\nportfolio utilities\nplotting/diagnostic helpers\ncommon conventions (metrics, configs, I/O)\n\nThe intent is simple: notebooks explain and experiment; the library implements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Quantitative Finance Lab",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nThis repo follows a reproducibility-first workflow (formatting/linting/testing/CI).\nLarge datasets are typically not committed. If a notebook expects data files, place them under a local data/ directory (gitignored) and follow any project-specific notes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Quantitative Finance Lab",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis repository is for research and educational purposes. Nothing here is investment advice, and results may vary by dataset, assumptions, costs, constraints, and implementation details.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Purpose\nThis lab is designed to be a long-running series of quantitative finance projects, where each project is:\nThe long-term output is a portfolio of work that demonstrates both research depth and software discipline.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#purpose",
    "href": "overview.html#purpose",
    "title": "Overview",
    "section": "",
    "text": "Readable (clear narrative, equations, and interpretation)\n\nReproducible (same inputs → same results, consistent metrics)\n\nReusable (core logic extracted into a Python package)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#how-projects-are-structured",
    "href": "overview.html#how-projects-are-structured",
    "title": "Overview",
    "section": "How projects are structured",
    "text": "How projects are structured\nEach project is written as a Quarto notebook and typically follows:\n\nMotivation & problem statement\nMathematical formulation\nImplementation details\nExperiments and comparisons\nDiagnostics + failure modes\nSummary of results & takeaways\n\nAs patterns emerge, reusable components are moved into the quantfinlab package so future projects can build on stable foundations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#repository-layout-conceptual",
    "href": "overview.html#repository-layout-conceptual",
    "title": "Overview",
    "section": "Repository layout (conceptual)",
    "text": "Repository layout (conceptual)\n\nnotebooks/\nProject notebooks (Quarto). These are the public-facing research artifacts.\nquantfinlab/\nReusable Python library modules shared across projects.\ndocs/\nRendered website output (GitHub Pages).\n.github/workflows/, pyproject.toml, .pre-commit-config.yaml\nCode quality + reproducibility tooling (lint/format/tests/CI).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#the-quantfinlab-philosophy",
    "href": "overview.html#the-quantfinlab-philosophy",
    "title": "Overview",
    "section": "The quantfinlab philosophy",
    "text": "The quantfinlab philosophy\nThe design rule is:\n\nNotebooks are clients. The library is the product.\n\nIf code is used in more than one place—or is critical enough to deserve tests—it should live in quantfinlab.\nExamples of what belongs in the library: - curve construction helpers, bond pricing primitives, risk measures - portfolio constraints, optimization wrappers, objective/penalty building blocks - standardized performance metrics and plotting utilities - data validation, common transformations, and safe defaults",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#data-reproducibility",
    "href": "overview.html#data-reproducibility",
    "title": "Overview",
    "section": "Data & reproducibility",
    "text": "Data & reproducibility\nMany finance datasets are large or licensed, so the repository generally avoids committing raw data.\nRecommended approach - Keep datasets in a local data/ directory (gitignored). - Use clear filenames and a short data note inside each notebook: - required file(s) - expected columns - frequency and timezone assumptions - missing data handling expectations\nStrong reproducibility standard - A notebook should either: - run end-to-end from a clean clone once data is present, or - fail with a clear, friendly message explaining what’s missing and where to put it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#evaluation-conventions-recommended",
    "href": "overview.html#evaluation-conventions-recommended",
    "title": "Overview",
    "section": "Evaluation conventions (recommended)",
    "text": "Evaluation conventions (recommended)\nTo keep comparisons consistent across projects, the lab aims to standardize:\n\nReturn definition (close-to-close vs open-to-open, etc.)\nRebalance convention (timing, lookahead prevention)\nTransaction costs model (turnover-based, spread-based, fees)\nMetrics\nAt minimum: CAGR, volatility, Sharpe, max drawdown, turnover, hit rate\nWhen relevant: tail metrics (VaR/CVaR), exposure diagnostics, constraint activity\n\nIf a metric can be misleading, the notebook should highlight it and include at least one complementary diagnostic plot/table.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#quality-checklist-used-in-this-lab",
    "href": "overview.html#quality-checklist-used-in-this-lab",
    "title": "Overview",
    "section": "Quality checklist used in this lab",
    "text": "Quality checklist used in this lab\nA project is considered “complete” when it includes:\n\nclear assumptions and constraints\n\nsanity checks and edge-case discussion\n\ncomparison baselines (simple but strong references)\n\nreproducible plots/tables (consistent formatting and labels)\n\nreusable implementation extracted to quantfinlab where appropriate\n\nminimal tests for core functions when feasible",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#roadmap-high-level",
    "href": "overview.html#roadmap-high-level",
    "title": "Overview",
    "section": "Roadmap (high level)",
    "text": "Roadmap (high level)\n\nExpand the library into clearer submodules (fixed income, portfolio, risk, datasets)\nAdd test coverage for high-value finance primitives\nAdd standardized experiment configs + results tables at the end of each project\nAdd small sample datasets for “out-of-the-box” runs when possible\nAdd additional projects in areas like:\n\nrisk attribution and factor models\ntransaction cost modeling and turnover control\nrobust optimization and regularization\nstress testing and scenario analysis\nyield curve strategies and bond portfolio PnL attribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#disclaimer",
    "href": "overview.html#disclaimer",
    "title": "Overview",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis content is for education and research. It is not investment advice. Any results depend heavily on data quality, assumptions, costs, constraints, and implementation details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "title": "1. Fixed income and yield curve construction",
    "section": "",
    "text": "1) Bonds, Coupons, Treasuries, and Par Yields\nIn this project we will implement",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bonds-coupons-treasuries-and-par-yields",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bonds-coupons-treasuries-and-par-yields",
    "title": "1. Fixed income and yield curve construction",
    "section": "",
    "text": "1.1 Fixed-coupon bond cashflows\nA standard coupon bond is defined by: - notional \\(N\\) (principal. the money that you get back) - annual coupon rate \\(c\\) (the interest rate of the bond) - maturity \\(T\\) (the time that bond ends and you get back principal and interest) - coupon frequency \\(f\\) (the amount of payments per year)\nCoupon payment each period is \\(\\dfrac{c}{f}N\\).\nPayment times are \\(t_i=\\dfrac{i}{f}\\) for \\(i=1,2,\\dots,n\\)      where \\(n=fT\\)\nCashflows for each period \\(i\\) is \\(CF_i=\\dfrac{c}{f}N\\)         \\(i=1,\\dots,n-1\\)\nfinal cash flow: \\(CF_n=\\dfrac{c}{f}N+N\\)\n\n\n\n1.2 Price from discount factors\nFor calculating price of a bond we need to discount all the cash flows to today value to compute the present value of the bond. for that we need to have a discount rate for every \\(t\\) that a cash flow accurs. this comes from a continous function of time which is discount curve \\(D(t)\\), the price is \\(P=\\sum_{i=1}^{n} CF_i\\,D(t_i)\\)\n\n\n\n1.3 Year fraction from dates\nGiven dates \\(d_0\\) and \\(d_1\\), a day-count year fraction is \\(\\tau(d_0,d_1)=\\dfrac{\\text{days}(d_0,d_1)}{365}\\)\n\n\n\n1.4 Tenor mapping\nTenor labels map to maturities: - \\(k\\) M → \\(T=k/12\\) - \\(k\\) Y → \\(T=k\\)\nCollect maturities into a numeric vector: \\(\\mathbf{T}=(T_1,T_2,\\dots,T_m)\\)\nObserved market par yields: \\(\\mathbf{y}=(y_1,y_2,\\dots,y_m)\\) with \\(y_j=y(T_j)\\)\n\n\n\n1.5 What “Treasury par yield” means\nA par yield at maturity \\(T\\) is the coupon rate \\(c(T)\\) that makes a standard coupon bond price equal to par: \\(P(T,c(T))=N\\) If we normalize \\(N=1\\), then par means \\(P=1\\).\nFor discounting, we use these yields but the problem is we have some of the standard maturities which may not be the same as other bonds. so that’s why we have to make this yield curve from the treasury maturities to be able to discount any time to present.\nyou can download the data used in this notebook here (treasury par yields from 1990 to 2026)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#imports-and-plotting-style-and-loading-data",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#imports-and-plotting-style-and-loading-data",
    "title": "1. Fixed income and yield curve construction",
    "section": "Imports and plotting style and loading data",
    "text": "Imports and plotting style and loading data\n\nimport math\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom cycler import cycler\n\nwarnings.filterwarnings(\"ignore\")\n\n\ncolors = [\"#069AF3\",\"#FE420F\", \"#00008B\", \"#008080\", \"#7BC8F6\",\"#800080\",\"#0072B2\",\"#008000\",\"#CC79A7\", \"#DC143C\", \"#04D8B2\"]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\nplt.rcParams.update({\n    \"figure.figsize\": (6, 3),\n    \"figure.dpi\": 300,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 6,\n})\n\n\ndf = pd.read_csv(r\"E:\\daneshgah\\quantitative-finance-lab\\data\\par-yield-curve-rates-1990-2026.csv\")\n\ncol_map = {\"date\": \"Date\",\"1 mo\": \"1M\",\"2 mo\": \"2M\",\"3 mo\": \"3M\",\"4 mo\": \"4M\",\"6 mo\": \"6M\",\"1 yr\": \"1Y\",\"2 yr\": \"2Y\",\"3 yr\": \"3Y\",\"5 yr\": \"5Y\",\"7 yr\": \"7Y\",\"10 yr\": \"10Y\",\"20 yr\": \"20Y\",\"30 yr\": \"30Y\"}\n\ndf = df.rename(columns={k.lower(): v for k, v in col_map.items()})\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"Date\"], ).set_index(\"Date\").sort_index()\n\ndf\n\n\n\n\n\n\n\n\n1M\n2M\n3M\n4M\n6M\n1Y\n2Y\n3Y\n5Y\n7Y\n10Y\n20Y\n30Y\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-02\nNaN\nNaN\n7.83\nNaN\n7.89\n7.81\n7.87\n7.90\n7.87\n7.98\n7.94\nNaN\n8.00\n\n\n1990-01-03\nNaN\nNaN\n7.89\nNaN\n7.94\n7.85\n7.94\n7.96\n7.92\n8.04\n7.99\nNaN\n8.04\n\n\n1990-01-04\nNaN\nNaN\n7.84\nNaN\n7.90\n7.82\n7.92\n7.93\n7.91\n8.02\n7.98\nNaN\n8.04\n\n\n1990-01-05\nNaN\nNaN\n7.79\nNaN\n7.85\n7.79\n7.90\n7.94\n7.92\n8.03\n7.99\nNaN\n8.06\n\n\n1990-01-08\nNaN\nNaN\n7.79\nNaN\n7.88\n7.81\n7.90\n7.95\n7.92\n8.05\n8.02\nNaN\n8.09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2026-01-22\n3.79\n3.72\n3.71\n3.67\n3.61\n3.53\n3.61\n3.68\n3.85\n4.05\n4.26\n4.79\n4.84\n\n\n2026-01-23\n3.78\n3.72\n3.70\n3.67\n3.61\n3.53\n3.60\n3.67\n3.84\n4.03\n4.24\n4.78\n4.82\n\n\n2026-01-26\n3.77\n3.70\n3.67\n3.67\n3.62\n3.52\n3.56\n3.66\n3.82\n4.02\n4.22\n4.75\n4.80\n\n\n2026-01-27\n3.77\n3.70\n3.67\n3.66\n3.61\n3.50\n3.53\n3.65\n3.81\n4.03\n4.24\n4.79\n4.83\n\n\n2026-01-28\n3.76\n3.71\n3.68\n3.70\n3.63\n3.52\n3.56\n3.66\n3.83\n4.05\n4.26\n4.81\n4.85\n\n\n\n\n9024 rows × 13 columns\n\n\n\n\ntenor_cols = [\"1M\",\"2M\",\"3M\",\"4M\",\"6M\",\"1Y\",\"2Y\",\"3Y\",\"5Y\",\"7Y\",\"10Y\",\"20Y\",\"30Y\"]\nfor c in tenor_cols:\n    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\nfirst_valid = df[tenor_cols].apply(lambda s: s.first_valid_index())\navailability = pd.DataFrame({\n    \"tenor\": tenor_cols,\n    \"first_valid_date\": [first_valid[t] for t in tenor_cols],\n})\navailability[\"first_valid_date\"] = pd.to_datetime(availability[\"first_valid_date\"])\navailability = availability.sort_values(\"first_valid_date\")\n\n\n\nprint(\"\\nData shape:\", df.shape)\nprint(\"Date range:\", df.index.min().date(), \"to\", df.index.max().date())\ndisplay(df[tenor_cols].describe().T)\n\n\nplt.figure()\nfor c in df.columns:\n    plt.plot(df.index, df[c], label=c)\nplt.title(\"Par Yields Over Time\")\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Date\")\nplt.legend(ncol= 4)\nplt.show()\n\n\nprint(\"First available date per tenor:\")\ndisplay(availability)\n\n\nData shape: (9024, 13)\nDate range: 1990-01-02 to 2026-01-28\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n1M\n6124.0\n1.669061\n1.834318\n0.00\n0.08\n0.98\n2.700\n6.02\n\n\n2M\n1819.0\n2.731045\n2.092637\n0.00\n0.14\n2.43\n4.635\n5.61\n\n\n3M\n9020.0\n2.793844\n2.279473\n0.00\n0.23\n2.74\n5.010\n8.26\n\n\n4M\n817.0\n4.826524\n0.600693\n3.58\n4.35\n4.77\n5.440\n5.64\n\n\n6M\n9023.0\n2.908257\n2.296244\n0.02\n0.41\n3.00\n5.100\n8.49\n\n\n1Y\n9023.0\n3.004545\n2.274365\n0.04\n0.56\n3.12\n5.025\n8.64\n\n\n2Y\n9023.0\n3.243374\n2.267325\n0.09\n0.94\n3.35\n4.990\n9.05\n\n\n3Y\n9023.0\n3.422187\n2.212582\n0.10\n1.37\n3.55\n5.050\n9.11\n\n\n5Y\n9023.0\n3.761102\n2.107014\n0.19\n1.81\n3.73\n5.380\n9.10\n\n\n7Y\n9023.0\n4.037397\n2.028457\n0.36\n2.22\n3.93\n5.600\n9.12\n\n\n10Y\n9023.0\n4.251091\n1.937830\n0.52\n2.61\n4.17\n5.710\n9.09\n\n\n20Y\n8084.0\n4.377290\n1.629305\n0.87\n2.89\n4.53\n5.540\n8.30\n\n\n30Y\n8029.0\n4.735390\n1.879145\n0.99\n3.08\n4.56\n6.130\n9.18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst available date per tenor:\n\n\n\n\n\n\n\n\n\ntenor\nfirst_valid_date\n\n\n\n\n2\n3M\n1990-01-02\n\n\n4\n6M\n1990-01-02\n\n\n5\n1Y\n1990-01-02\n\n\n6\n2Y\n1990-01-02\n\n\n7\n3Y\n1990-01-02\n\n\n8\n5Y\n1990-01-02\n\n\n9\n7Y\n1990-01-02\n\n\n10\n10Y\n1990-01-02\n\n\n12\n30Y\n1990-01-02\n\n\n11\n20Y\n1993-10-01\n\n\n0\n1M\n2001-07-31\n\n\n1\n2M\n2018-10-16\n\n\n3\n4M\n2022-10-19\n\n\n\n\n\n\n\n\nsample_dates = [\n    df.index[0],\n    df.index[len(df)//2],\n    df.index[-252],\n    df.index[df.index &lt;= pd.Timestamp(\"2007-03-01\")][-1],\n    df.index[-1]\n]\n\nx = np.arange(len(tenor_cols))  \n\nplt.figure()\nfor d in sample_dates:\n    y = df.loc[d, tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    plt.plot(x[mask], y.values[mask], marker=\"o\", label=d.strftime(\"%Y-%m-%d\"))\n\nplt.title(\"Yield Curve Snapshots (Par Yields)\")\nplt.xticks(x, tenor_cols)\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Tenor\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "title": "1. Fixed income and yield curve construction",
    "section": "3) Discount Factors, Zero Rates, and Forward Rates",
    "text": "3) Discount Factors, Zero Rates, and Forward Rates\n\n3.1 Discount factor\n\\(D(t)\\) is the present value of receiving 1 unit of currency at time \\(t\\). For example what does 1 dollar in 20 years worth now.\n\n\n3.2 Zero rate (continuous compounding)\n\\(z(t)\\) is the constant rate that discounts a payment in \\(t\\) to present. for example, if we want to know discount factor of 1 dollar in 20 years we need an annual rate to compute the present value. that’s zero rate.\nDefine \\(z(t)\\) by \\[\nD(t)=e^{-z(t)t}\n\\] \\[\nz(t)=-\\dfrac{\\ln D(t)}{t}\n\\]\n\n\n3.3 Instantaneous forward rate\n\\(f(t)\\) is the slope of \\(z(t)\\) which tells us how much the rate of return is very close to the time of maturity. it is used because zero rate is smooth and in instant time we need to have exact forward rate\n\\[\nf(t)=-\\dfrac{d}{dt}\\ln D(t)\n\\] \\[\nD(t)=\\exp\\left(-\\int_0^t f(u)\\,du\\right)\n\\]\n\n\n3.4 Discrete forward over an interval\nFor \\(t_1&lt;t_2\\), the continuously-compounded forward rate for the interval is \\[\nF(t_1,t_2)=\\dfrac{\\ln D(t_1)-\\ln D(t_2)}{t_2-t_1}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "4) Par Yield Implied by a Curve",
    "text": "4) Par Yield Implied by a Curve\nThis is used for building yield curve and validation to see if the predicted rate is close to real rate based on curve. what is the rate that makes the price of bond (PV of cashflows) equal to 1?\nGiven a curve \\(D(t)\\), the par coupon rate for maturity \\(T\\) and frequency \\(f\\) solves\n\\(1=\\sum_{i=1}^{n}\\dfrac{c}{f}D(t_i)+D(T)\\)            \\(t_i=i/f\\), \\(n=fT\\).\n\\(1=\\dfrac{c}{f}\\sum_{i=1}^{n}D(t_i)+D(T)\\)\nand finally we get to \\(c=f\\,\\dfrac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nShort maturities (&lt;1Y) often use money-market conventions because they are mostly single payment. Two common ones: - continuous: \\(y=-\\dfrac{\\ln D(T)}{T}\\) - simple: \\(y=\\dfrac{1/D(T)-1}{T}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "title": "1. Fixed income and yield curve construction",
    "section": "5) Bootstrapping Discount Factors from Par Yields",
    "text": "5) Bootstrapping Discount Factors from Par Yields\nBootstrapping constructs \\(D(T)\\) at market tenors from observed par yields. in this way we can have a function of time to discount a payment in any maturity based on the real yields that we have.\n\n5.1 Short end (&lt;1Y) convention\nFor \\(T&lt;1\\) we use the money market convention again. for calculating discount factor:\n\ncontinuous convention: \\(D(T)=e^{-y(T)T}\\)\nsimple convention: \\(D(T)=\\dfrac{1}{1+y(T)T}\\)\n\n\n\n5.2 Bootstrapping for coupon tenors (T ≥ 1)\nLet \\(c=y(T)\\) be the market par yield at maturity \\(T\\) (used as coupon rate). With frequency \\(f\\) and cashflow times \\(t_i=i/f\\):\nPar condition (normalized notional 1): \\[\n1=\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)+\\left(1+\\dfrac{c}{f}\\right)D(T)\n\\]\nSolve for the new unknown \\(D(T)\\): \\[D(T)=\\dfrac{1-\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)}{1+\\dfrac{c}{f}}\n\\]\nif we have the earlier coupons discount factor, we know everything on the right side of equation. so we can solve it and get to \\(D(T)\\)\nit’s called bootstrapping because we first compute the \\(D(T&lt;1Y)\\) with short end convention, then we use that to compute \\(D(1Y)\\) and then use them for \\(D(2Y)\\) until the last maturity (30Y)\n\n\n5.3 Interpolating discount factors at coupon dates\nBootstrapping needs \\(D(t_i)\\) at coupon dates, but you often only have DFs at pillar maturities.\nA robust choice is log-linear interpolation: If \\(T_a&lt;t&lt;T_b\\), then\n\\[\\ln D(t)=\\ln D(T_a)+\\dfrac{t-T_a}{T_b-T_a}\\left(\\ln D(T_b)-\\ln D(T_a)\\right)\n\\]\nSo \\[D(t)=\\exp\\left(\\ln D(t)\\right)\n\\]\n\ndf_dec = df.copy()\ndf_dec[tenor_cols] = df_dec[tenor_cols] / 100.0\n\nshort_end_convention = \"continuous\"\nf = 2\nmin_d = 1e-12\n\n\ndef labels_to_T(labels):\n    T = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        if labu.endswith(\"M\"):\n            T.append(int(labu[:-1]) / 12.0)\n        else:\n            T.append(float(int(labu[:-1])))\n    return np.array(T, dtype=float)\n\n\ndef get_par_from_row(row):\n    y = row[tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    labels = [tenor_cols[i] for i in range(len(tenor_cols)) if mask[i]]\n    if len(labels) == 0:\n        return None\n\n    par = y.values[mask].astype(float)\n    T = labels_to_T(labels)\n\n    i = np.argsort(T)\n    T = T[i]\n    par = par[i]\n    labels = [labels[i] for i in i]\n\n    return T, par, labels\n\n\ndef get_par_for_date(date):\n    row = df_dec.loc[date]\n    return get_par_from_row(row)\n\n\ndef short_end_df(Ti, ri):\n    if short_end_convention == \"continuous\":\n        return math.exp(-ri * Ti)\n    return 1.0 / (1.0 + ri * Ti)\n\n\ndef price_error_loglinear(d_T, Ti, t_prev, d_prev, times_interp, c, pv_known):\n    d_T = max(float(d_T), min_d)\n    pv_interp = 0.0\n    if len(times_interp) &gt; 0:\n        w = (times_interp - t_prev) / (Ti - t_prev)\n        log_d = (1 - w) * np.log(d_prev) + w * np.log(d_T)\n        d_interp = np.exp(log_d)\n        pv_interp = np.sum((c / f) * d_interp)\n    return pv_known + pv_interp + d_T - 1.0\n\n\ndef solve_df_long_end(Ti, ri, d_map):\n    c = float(ri)\n    n = int(round(Ti * f))\n    times = np.array([k / f for k in range(1, n + 1)], dtype=float)\n\n\n    known_T = np.array(sorted(d_map.keys()), dtype=float)\n    known_D = np.array([d_map[t] for t in known_T], dtype=float)\n    known_D = np.clip(known_D, min_d, None)\n\n\n    t_prev = known_T[-1]\n    d_prev = known_D[-1]\n\n    times_known = times[times &lt;= t_prev + 1e-12]\n    times_interp = times[times &gt; t_prev + 1e-12]\n\n    pv_known = 0.0\n    if len(times_known) &gt; 0:\n        log_known_D = np.log(known_D)\n        log_df_known = np.interp(times_known, known_T, log_known_D)\n        d_known = np.exp(log_df_known)\n        pv_known = np.sum((c / f) * d_known)\n\n    lo = min_d\n    hi = d_prev\n    f_lo = price_error_loglinear(lo, Ti, t_prev, d_prev, times_interp, c, pv_known)\n    f_hi = price_error_loglinear(hi, Ti, t_prev, d_prev, times_interp, c, pv_known)\n\n    if f_lo * f_hi &gt; 0:\n    \n        log_known_D = np.log(known_D)\n        log_df_cpn = np.interp(\n            times[:-1],\n            known_T,\n            log_known_D,\n            left=log_known_D[0],\n            right=log_known_D[-1],\n        )\n        d_cpn = np.exp(log_df_cpn)\n        pv_coupons = np.sum((c / f) * d_cpn)\n        d_T = (1.0 - pv_coupons) / (1.0 + c / f)\n    else:\n        for _ in range(100):\n            mid = 0.5 * (lo + hi)\n            f_mid = price_error_loglinear(mid, Ti, t_prev, d_prev, times_interp, c, pv_known)\n            if f_lo * f_mid &lt;= 0:\n                hi = mid\n                f_hi = f_mid\n            else:\n                lo = mid\n                f_lo = f_mid\n            if abs(hi - lo) &lt; 1e-12:\n                break\n        d_T = 0.5 * (lo + hi)\n\n    return d_T\n\n\ndef bootstrap_from_inputs(T, par, labels, date=None):\n    d_map = {}\n\n    # short convention\n    for Ti, ri in zip(T, par, strict=True):\n        if Ti &lt; 1.0:\n            d_T = short_end_df(Ti, ri)\n            d_map[Ti] = max(float(d_T), min_d)\n            continue\n\n        d_T = solve_df_long_end(Ti, ri, d_map)\n        if (not np.isfinite(d_T)) or (d_T &lt;= 0):\n            d_T = min_d\n        d_map[Ti] = max(float(d_T), min_d)\n\n    dfs = np.array([d_map[t] for t in T], dtype=float)\n\n    return {\n        \"date\": date,\n        \"T\": T,\n        \"par\": par,\n        \"labels\": labels,\n        \"dfs\": dfs,\n    }\n\n\ndef bootstrap_pillars(date):\n    result = get_par_for_date(date)\n    if result is None:\n        return None\n\n    T, par, labels = result\n    return bootstrap_from_inputs(T, par, labels, date=date)\n\n\n# we bootstrap discount factors at the last available date for now\nbase_date = df_dec.index[-1]\npillars = bootstrap_pillars(base_date)\n\nT = pillars[\"T\"]\npar = pillars[\"par\"]\nlabels = pillars[\"labels\"]\ndfs = pillars[\"dfs\"]\n\nprint(\"Base date:\", base_date.date())\nprint(\"Tenors used:\", labels)\nprint(\"First 5 pillar DFs:\", dfs)\n\nplt.figure()\nplt.plot(T, dfs, marker=\"o\")\nplt.title(\"Bootstrapped Discount Factors\")\nplt.xlabel(\"Maturity T\")\nplt.ylabel(\"Discount Factor D(t)\")\nplt.show()\n\nBase date: 2026-01-28\nTenors used: ['1M', '2M', '3M', '4M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\nFirst 5 pillar DFs: [0.99687157 0.99383574 0.99084219 0.98774241 0.98201372 0.96571989\n 0.93185753 0.89680276 0.82670445 0.75353422 0.65216175 0.37090153\n 0.22566195]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "6) Turning Bootstrapped Pillars into a Full Curve",
    "text": "6) Turning Bootstrapped Pillars into a Full Curve\nAfter bootstrapping we have pillars \\((T_j, D(T_j))\\) or \\((T_j, z(T_j))\\). Now we want to define continuous functions \\(D(t)\\) and \\(z(t)\\) for all \\(t\\).\n\n6.1 Method A: Log-linear discount factors\nInterpolate \\(\\ln D(t)\\) linearly between pillars (just like for the T we had. we do the same thing between them):\n\\(\\ln D(t)=\\text{linear interp of } \\{\\ln D(T_j)\\}\\)\nThen \\(D(t)=\\exp(\\ln D(t))\\)\nand \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\ndef loglinear_curve(T, dfs):\n    loglinear_log_dfs = np.log(dfs)\n\n    loglinear_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    loglinear_log_df_grid = np.interp(\n        loglinear_grid,\n        T,\n        loglinear_log_dfs,\n        left=loglinear_log_dfs[0],\n        right=loglinear_log_dfs[-1],\n    )\n    loglinear_df_grid = np.exp(loglinear_log_df_grid)\n\n    # zero rate\n    loglinear_z_grid = -np.log(loglinear_df_grid) / loglinear_grid\n\n    # instantaneous forward rate\n    loglinear_fwd_grid = -np.gradient(np.log(loglinear_df_grid), loglinear_grid)\n\n    def loglinear_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(\n            t,\n            T,\n            loglinear_log_dfs,\n            left=loglinear_log_dfs[0],\n            right=loglinear_log_dfs[-1],\n        )\n        return np.exp(log_df)\n\n    return {\n        \"name\": \"Log-linear DF\",\n        \"grid\": loglinear_grid,\n        \"df_grid\": loglinear_df_grid,\n        \"z_grid\": loglinear_z_grid,\n        \"fwd_grid\": loglinear_fwd_grid,\n        \"df_func\": loglinear_df_func,\n        \"log_dfs\": loglinear_log_dfs,\n    }\n\n\nloglinear_curve_data = loglinear_curve(T, dfs)\nloglinear_grid = loglinear_curve_data[\"grid\"]\nloglinear_df_grid = loglinear_curve_data[\"df_grid\"]\nloglinear_z_grid = loglinear_curve_data[\"z_grid\"]\nloglinear_fwd_grid = loglinear_curve_data[\"fwd_grid\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"loglinear\"] = loglinear_curve_data\n\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_z_grid * 100.0)\nplt.title(\"Zero Curve (log-linear)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Zero Rate (%)\")\nplt.show()\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_fwd_grid * 100.0)\nplt.title(\"Instantaneous Forward Rate (derivative of ln(DF))\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Forward Rate (%)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "title": "1. Fixed income and yield curve construction",
    "section": "7) Curve Models Using Zero-Rate Smoothing",
    "text": "7) Curve Models Using Zero-Rate Smoothing\nInstead of interpolating \\(D\\), we can interpolate \\(z\\) and recover \\(D(t)=e^{-z(t)t}\\).\n\n7.1 PCHIP on zero rates (Piecewise Cubic Hermite Interpolating Polynomial)\nGiven nodes \\(x_j=T_j\\) and \\(y_j=z(T_j)\\), PCHIP builds a piecewise cubic polynomial on each interval:\n\\[\np_j(t)=a_j(t-x_j)^3+b_j(t-x_j)^2+c_j(t-x_j)+d_j  \\quad t\\in[x_j,x_{j+1}]\n\\]\nConstraints include: - \\(p_j(x_j)=y_j\\) and \\(p_j(x_{j+1})=y_{j+1}\\) - first derivatives are chosen by shape-preserving slope rules to reduce overshoot\nwe define \\(z(t)=p_j(t)\\)\nthen \\(D(t)=e^{-z(t)t}\\)\n\nfrom scipy.interpolate import PchipInterpolator\n\n\ndef pchip_curve(T, dfs):\n    pchip_zeros = -np.log(np.clip(dfs, min_d, None)) / T\n\n    pchip_z = PchipInterpolator(T, pchip_zeros, extrapolate=True)\n    pchip_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    pchip_z_grid = pchip_z(pchip_grid)\n    pchip_df_grid = np.exp(-pchip_z_grid * pchip_grid)\n\n    pchip_fwd_grid = -np.gradient(\n        np.log(np.clip(pchip_df_grid, min_d, None)), pchip_grid\n    )\n\n    def pchip_df_func(t):\n        t = np.array(t, dtype=float)\n        z = pchip_z(t)\n        return np.exp(-z * t)\n\n    return {\n        \"name\": \"PCHIP zero\",\n        \"grid\": pchip_grid,\n        \"df_grid\": pchip_df_grid,\n        \"z_grid\": pchip_z_grid,\n        \"fwd_grid\": pchip_fwd_grid,\n        \"df_func\": pchip_df_func,\n        \"pillar_zeros\": pchip_zeros,\n    }\n\n\npchip_curve_data = pchip_curve(T, dfs)\ncurves[\"pchip\"] = pchip_curve_data\n\n\nplt.figure()\nplt.plot(T, pchip_curve_data[\"pillar_zeros\"] * 100.0, \"o\", label=\"pillar zeros\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"z_grid\"] * 100.0, \"-\", label=\"PCHIP zero\")\nplt.title(\"Zero Curve (PCHIP)\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(T, dfs, \"o\", label=\"Pillar DFs\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"df_grid\"], \"-\", label=\"DF from PCHIP\")\nplt.title(\"Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"Instantaneous Forward Rate\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "8) Nelson–Siegel–Svensson (NSS) yield curve",
    "text": "8) Nelson–Siegel–Svensson (NSS) yield curve\nwe represent the continuous-compounded zero rate curve \\(z(t)\\) with a small number of parameters, then derive discount factors, par yields and forwards\n\n8.1 NSS zero-rate function\nFor maturity \\(t&gt;0\\), the NSS zero rate is:\n\\[\nz(t)=\\beta_0 +\\beta_1\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}\\right) +\\beta_2\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}-e^{-t/\\tau_1}\\right) +\\beta_3\\left(\\frac{1-e^{-t/\\tau_2}}{t/\\tau_2}-e^{-t/\\tau_2}\\right)\n\\]\nParameters:\n\n\\(\\beta_0\\) = long-run “level”\n\\(\\beta_1\\) = “slope” (short-end effect)\n\\(\\beta_2\\) = medium-term “curvature” (first hump)\n\\(\\beta_3\\) = additional curvature (second hump)\n\\(\\tau_1,\\tau_2&gt;0\\) control where humps occur\n\n\ndef nss_zero(t, b0,b1,b2,b3,tau1,tau2):\n    t = np.array(t, dtype=float)\n    x1 = t / tau1\n    x2 = t / tau2\n    L1 = (1.0 - np.exp(-x1)) / x1\n    C1 = L1 - np.exp(-x1)\n    C2 = (1.0 - np.exp(-x2)) / x2 - np.exp(-x2)\n    return b0 + b1*L1 + b2*C1 + b3*C2\n\n\n\n8.3 Par yield implied by NSS\nFor a coupon bond with maturity \\(T\\) and coupon frequency \\(f\\), coupon rate \\(c(T)\\) is the rate that makes the bond price equal to par (normalize notional to 1):\n\\[\n1=\\sum_{i=1}^{n}\\frac{c(T)}{f}D(t_i)+D(T)\n\\]\nSolve for \\(c(T)\\):\n\\[\nc(T)=f\\cdot\\frac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\n\\]\nFor short maturities (money-market style), a common mapping is:\ncontinuous: \\(y(T)=-\\ln D(T)/T\\)\nsimple: \\(y(T)=(1/D(T)-1)/T\\)\nbut first we have to have \\(D(t)\\)\n\ndef par_from_d(df_func, T_list, f=2):\n    T_arr = np.asarray(T_list, dtype=float)\n    out = np.full_like(T_arr, np.nan, dtype=float)\n\n    step = 1.0 / float(f)\n\n    for k, Tk in enumerate(T_arr):\n        if not np.isfinite(Tk) or Tk &lt;= 0:\n            continue\n\n        D_T = float(np.asarray(df_func([Tk],), dtype=float)[0])\n        D_T = max(D_T, min_d)\n\n        if Tk &lt; 1.0:\n            if short_end_convention == \"simple\":\n                out[k] = (1.0 / D_T - 1.0) / Tk\n            else:\n                out[k] = -np.log(D_T) / Tk\n            continue\n\n        n_full = int(np.floor(Tk * f + 1e-12))\n        times = np.arange(step, n_full * step + 1e-12, step)\n        if len(times) == 0 or abs(times[-1] - Tk) &gt; 1e-10:\n            times = np.append(times, Tk)\n\n        accr = np.diff(np.concatenate([[0.0], times]))\n        dfs = np.asarray(df_func(times), dtype=float)\n        dfs = np.clip(dfs, min_d, None)\n\n        denom = float(np.sum(accr * dfs))\n        out[k] = (1.0 - dfs[-1]) / denom if denom &gt; 0 else np.nan\n\n    return out\n\n\n\n8.2 Discount factor and forward from NSS\nOnce we have \\(z(t)\\), using continuous compounding:\n\\(D(t)=e^{-z(t)t}\\)\nand the instantaneous forward rate is:\n\\(f(t)=-\\frac{d}{dt}\\ln D(t)\\)\nWith NSS you often compute \\(f(t)\\) numerically on a grid: \\[f(t_i)\\approx -\\frac{\\ln D(t_{i+1})-\\ln D(t_{i-1})}{t_{i+1}-t_{i-1}}\n\\]\n\n\n\n8.4 Calibrating NSS to market par yields\nGiven observed par yields \\(y^{mkt}(T_j)\\) at tenors \\(T_j\\), we choose parameters \\(\\theta=(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\tau_1,\\tau_2)\\) to minimize a least-squares objective:\n\\[\\min_{\\theta}\\sum_{j}\\left(c^{model}(T_j;\\theta)-y^{mkt}(T_j)\\right)^2\n\\]\nOptionally add regularization to discourage extreme shapes: \\(\\lambda\\sum_j (z''(t_j))^2\\) or bounds on parameters.\n\nfrom scipy.optimize import minimize\n\n\ndef nss_curve(T, par):\n    def obj(theta):\n        b0, b1, b2, b3, tau1, tau2 = theta\n        z = nss_zero(T, b0, b1, b2, b3, tau1, tau2)\n        dfs = np.exp(-z * T)\n\n        # we use log-linear DF interpolation on pillars for keeping it positive\n        log_dfs = np.log(np.clip(dfs, min_d, None))\n\n        def df_func(t):\n            t = np.array(t, dtype=float)\n            log_df = np.interp(t, T, log_dfs, left=log_dfs[0], right=log_dfs[-1])\n            return np.exp(log_df)\n\n        par_model = par_from_d(df_func, T)\n        err = par_model - par\n        return float(np.mean(err**2))\n\n    # initializing with a first guess. for long run level we need something like the long-run yield level. that's why we use long term yields as guess\n    b0_0 = float(np.nanmedian(par[-3:])) if len(par) &gt;= 3 else float(np.nanmedian(par))\n    x0 = np.array([b0_0, -0.02, 0.02, 0.01, 1.5, 5.0], dtype=float)\n\n    # we use a type of quasi-Newton method for nonlinear optimization of parameters\n    pred = minimize(obj, x0, method=\"L-BFGS-B\")\n    theta = pred.x\n\n    nss_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    nss_z_grid = nss_zero(nss_grid, *theta)\n    nss_df_grid = np.exp(-nss_z_grid * nss_grid)\n\n    nss_fwd_grid = -np.gradient(\n        np.log(np.clip(nss_df_grid, min_d, None)), nss_grid\n    )\n\n    def nss_df_func(t):\n        t = np.array(t, dtype=float)\n        return np.exp(-nss_zero(t, *theta) * t)\n\n    curve = {\n        \"name\": \"NSS\",\n        \"grid\": nss_grid,\n        \"df_grid\": nss_df_grid,\n        \"z_grid\": nss_z_grid,\n        \"fwd_grid\": nss_fwd_grid,\n        \"df_func\": nss_df_func,\n        \"theta\": theta,\n    }\n\n\n    z_p = nss_zero(T, *theta)\n    d_p = np.exp(-z_p * T)\n    log_d_p = np.log(np.clip(d_p, min_d, None))\n    def df_func_p(tt):\n        return np.exp(\n            np.interp(np.array(tt, float), T, log_d_p, left=log_d_p[0], right=log_d_p[-1])\n        )\n    par_fit = par_from_d(df_func_p, T)\n\n    return curve, par_fit, pred\n\n\nnss_curve_data, par_fit, pred = nss_curve(T, par)\ntheta = nss_curve_data[\"theta\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"nss\"] = nss_curve_data\n\nprint(\"final MSE:\", pred.fun)\nprint(\"theta = [b0,b1,b2,b3,tau1,tau2] =\", np.round(theta, 6))\n\nplt.figure()\nplt.plot(T, par * 100.0, \"o\", label=\"Market par\")\nplt.plot(T, par_fit * 100.0, \"-o\", label=\"NSS implied par\")\nplt.title(\"NSS Fit to Par Yields\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Par Yield\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"NSS Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"NSS Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nfinal MSE: 3.5093289069366877e-07\ntheta = [b0,b1,b2,b3,tau1,tau2] = [ 0.053115 -0.014698 -0.031572 -0.007968  1.500251  5.000021]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "title": "1. Fixed income and yield curve construction",
    "section": "9) QP curve: smooth discount factors under exact par-bond fit",
    "text": "9) QP curve: smooth discount factors under exact par-bond fit\nAnother approach for building a yield curve that:\n\nmatches par-bond pricing equations exactly (like bootstrapping) or near-exactly (like NSS),\nis smooth,\nwill result in a positive and increasing DF curve.\n\ncan come from a Quadratic Program (QP) if the variables are discount factors on a grid and constraints are linear.\n\n9.1 Variables\nWe Pick a grid of cashflow times (like semiannual up to 30Y): \\(t_1,t_2,\\dots,t_M\\)\nwe want to get to \\[\n\\mathbf{d} = (D(t_1),\\dots,D(t_M))\n\\]\n\n\n9.2 constraints\nFor a maturity \\(T\\) (present on the grid), par yield \\(c\\) and frequency \\(f\\):\n\\[\n1=\\sum_{i=1}^{n}\\frac{c}{f}D(t_i)+D(T)\n\\]\nThis is linear in \\(D(\\cdot)\\), so it becomes one row of: \\(A\\mathbf{d}=\\mathbf{1}\\)\nPositivity: \\(D(t_k)\\ge D_{min}\\)\nMonotone decreasing: \\(D(t_{k+1})\\le D(t_k)\\)\nThese are linear inequalities, so the problem stays convex and QP-solvable.\n\nimport cvxpy as cp\n\n\ndef qp_build_t_grid(T_obs, f):\n    T_max = float(np.max(T_obs))\n    n_grid = int(round(T_max * f))\n    t_grid = np.unique(\n        np.concatenate(\n            [\n                np.array([i / f for i in range(1, n_grid + 1)], dtype=float),\n                T_obs,\n            ]\n        )\n    )\n    t_grid = np.array(sorted(t_grid), dtype=float)\n    grid_index = {float(np.round(t, 10)): i for i, t in enumerate(t_grid)}\n    return t_grid, grid_index\n\n\ndef qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d):\n    d = cp.Variable(len(t_grid))\n\n    constraints = []\n    constraints += [d &gt;= min_d]\n    constraints += [d[1:] &lt;= d[:-1]]\n\n    for Tk, yk in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            key = float(np.round(Tk, 10))\n            if key in grid_index:\n                i = grid_index[key]\n                df_target = float(np.exp(-yk * Tk))\n                constraints += [d[i] == df_target]\n\n    for Tk, ck in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            continue\n        keyT = float(np.round(Tk, 10))\n        if keyT not in grid_index:\n            continue\n        iT = grid_index[keyT]\n        n = int(round(Tk * f))\n\n        coupon_idx = []\n        for j in range(1, n + 1):\n            key = float(np.round(j / f, 10))\n            coupon_idx.append(grid_index[key])\n\n        constraints += [cp.sum((ck / f) * d[coupon_idx]) + d[iT] == 1.0]\n\n    return d, constraints\n\n\n\n\n\n\n9.3 Smoothness objective (quadratic)\nA simple convex smoothness penalty is the squared second difference of Discount Factors:\n\\(\\min_{\\mathbf{d}} \\ |\\Delta^2\\mathbf{d}|_2^2\\)\nwhere \\(\\Delta^2 d_k = d_{k+2}-2d_{k+1}+d_k\\). (Discrete version)\nThis makes the optimizer prefer sequences of discount factors that have small curvature everywhere, which results a smooth DF curve with fewer oscillations and jumps.\nwe can also add a mild “keep close to a prior curve” penalty: \\(\\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\nTotal objective: \\(\\min_{\\mathbf{d}} \\ \\lambda|\\Delta^2\\mathbf{d}|_2^2 + \\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\n\n\n9.4 zero and forward curves\nOnce we have \\(D(t)\\) on a grid:\n\\(z(t_k)=-\\ln D(t_k)/t_k\\)\n\\(f(t_k)\\approx -\\dfrac{\\ln D(t_{k+1})-\\ln D(t_k)}{t_{k+1}-t_k}\\)\n\ndef qp_solve(t_grid, d, constraints, par_mkt, f, min_d):\n    lam = 1e4\n    eps = 1e-4\n    prior_rate = (\n        float(np.nanmedian(par_mkt[-3:])) if len(par_mkt) &gt;= 3 else float(np.nanmedian(par_mkt))\n    )\n    d_prior = np.exp(-prior_rate * t_grid)\n\n    d2 = d[2:] - 2 * d[1:-1] + d[:-2]\n    obj = cp.Minimize(lam * cp.sum_squares(d2) + eps * cp.sum_squares(d - d_prior))\n\n    prog = cp.Problem(obj, constraints)\n    prog.solve(solver=cp.OSQP)\n\n    d_sol = np.array(d.value).astype(float)\n    d_sol = np.clip(d_sol, min_d, None)\n\n    return d_sol, prog.status, prog.value\n\n\ndef qp_build_curve(t_grid, d_sol):\n    qp_grid = np.linspace(max(1 / 12, t_grid.min()), 30.0, 1000)\n    qp_log_d = np.log(d_sol)\n    qp_log_df_grid = np.interp(qp_grid, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n\n    qp_df_grid = np.exp(qp_log_df_grid)\n    qp_z_grid = -np.log(qp_df_grid) / np.maximum(qp_grid, 1e-8)\n    qp_fwd_grid = -np.gradient(np.log(qp_df_grid), qp_grid)\n\n    def qp_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(t, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n        return np.exp(log_df)\n\n    curve = {\n        \"name\": \"QP DF\",\n        \"grid\": qp_grid,\n        \"df_grid\": qp_df_grid,\n        \"z_grid\": qp_z_grid,\n        \"fwd_grid\": qp_fwd_grid,\n        \"df_func\": qp_df_func,\n    }\n\n    return curve\n\n\ndef qp_curve(labels, par_mkt, f=2, min_d=1e-10):\n    T_obs = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        T_obs.append(int(labu[:-1]) / 12.0 if labu.endswith(\"M\") else float(int(labu[:-1])))\n    T_obs = np.array(T_obs, dtype=float)\n\n    idx = np.argsort(T_obs)\n    T_obs = T_obs[idx]\n    par_mkt = par_mkt[idx]\n    labels = [labels[i] for i in idx]\n\n    t_grid, grid_index = qp_build_t_grid(T_obs, f)\n    d, constraints = qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d)\n    d_sol, status, value = qp_solve(t_grid, d, constraints, par_mkt, f, min_d)\n\n    curve = qp_build_curve(t_grid, d_sol)\n\n    state = {\n        \"t_grid\": t_grid,\n        \"d_sol\": d_sol,\n        \"constraints\": constraints,\n        \"status\": status,\n        \"value\": value,\n    }\n\n    return curve, state\n\n\nqp_curve_data, qp_state = qp_curve(labels, par)\n\ncurves[\"qp\"] = qp_curve_data\n\nprint(\"status:\", qp_state[\"status\"], \",  value:\", qp_state[\"value\"])\n\n\nplt.figure()\nplt.plot(qp_state[\"t_grid\"], qp_state[\"d_sol\"], \"o\", markersize=3, label=\"QP DF\")\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"df_grid\"], \"-\", label=\"Discount Factor (log-linear)\")\nplt.title(\"QP Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"QP Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"QP Instantaneous Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nstatus: optimal ,  value: 1.2368745747617473",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "title": "1. Fixed income and yield curve construction",
    "section": "11) Synthetic Bond Issuance (Issued at Par)",
    "text": "11) Synthetic Bond Issuance (Issued at Par)\nbecause we don’t have official data for this part, We simulate a monthly issuance program using par yields to create a rolling book of bonds so we can analyze our models and implement the next topics. Each month, we pretend the Treasury issues new par bonds at a few maturities (like 2Y, 5Y, 10Y, 30Y). The coupon of each new bond is set to that month’s par yield at that maturity (from the dataset). Because coupon = par yield at issuance, each new bond starts at price approximately equal to 1 (par).\n\nDetails of the bond\n\nfor each month \\(t_0\\) we create a new bond with maturity \\(T\\) and frequency \\(f\\)\n\\(f\\) = semiannual\n\\(T = {2Y, 5Y, 10Y, 30Y}\\)\ncoupon: \\(c_d(T)=y_d(T)\\) (the market par yield at that date and maturity)\nnotional \\(N=1\\)\n\nBasically we buy bonds with 4 different maturities every month and keep it and get interest every month until maturity of those bonds. so we have 4 books for 4 different bonds (maturities). and our portfolio is based on these four books.\nSo each month the outgoing cashflow is buying the bonds and ingoing is all the interest and maybe principal of all the bonds that we have bought.\n\nissue_maturities = [2, 5, 10, 30]\nissue_labels = {2: \"2Y\", 5: \"5Y\", 10: \"10Y\", 30: \"30Y\"}\n\n\nmonth_end_curve = df_dec[tenor_cols].resample(\"ME\").last()\nissue_dates = month_end_curve.index\n\n\ndef yearfrac(t0, t1):\n    return (t1 - t0).days / 365\n\n\ndef bond_cashflows(c, T, f=2):\n    times = np.arange(1 / f, T + 1e-9, 1 / f)\n    cfs = np.full_like(times, c / f)\n    cfs[-1] += 1.0\n    return times, cfs\n\n\ndef price_bond(df_func, times, cfs, delta):\n    mask = times &gt; delta + 1e-12\n    if not np.any(mask):\n        return 0.0\n    t_rem = times[mask] - delta\n    cf_rem = cfs[mask]\n    return float(np.sum(cf_rem * df_func(t_rem)))\n\n\n\nissuance_book = {T: [] for T in issue_maturities}\nfor d in issue_dates:\n    row = month_end_curve.loc[d]\n    for T in issue_maturities:\n        label = issue_labels[T]\n        c = float(row.get(label, np.nan))\n        if not np.isfinite(c):\n            continue\n        times, cfs = bond_cashflows(c, T, f)\n        issuance_book[T].append({\n            \"issue_date\": d,\n            \"coupon\": c,\n            \"times\": times,\n            \"cfs\": cfs,\n        })\n\nissuance_summary = pd.DataFrame({\n    \"n_bonds\": {T: len(issuance_book[T]) for T in issue_maturities},\n    \"first_issue\": {T: issuance_book[T][0][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n    \"last_issue\": {T: issuance_book[T][-1][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n})\n\nprint(\"Issuance summary:\")\ndisplay(issuance_summary)\n\nIssuance summary:\n\n\n\n\n\n\n\n\n\nn_bonds\nfirst_issue\nlast_issue\n\n\n\n\n2\n433\n1990-01-31\n2026-01-31\n\n\n5\n433\n1990-01-31\n2026-01-31\n\n\n10\n433\n1990-01-31\n2026-01-31\n\n\n30\n386\n1990-01-31\n2026-01-31",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "title": "1. Fixed income and yield curve construction",
    "section": "12) Pricing an Issued Bond at a Later Date",
    "text": "12) Pricing an Issued Bond at a Later Date\nIf issue date be \\(t_0\\) and valuation date be \\(t\\). Elapsed time is the amount of time that has been passed from the bond issued at \\(t_0\\) in valuation time \\(t\\): \\(\\Delta=\\tau(t_0,t)\\)\nreminder: \\(\\tau(t_0,t_1)=\\dfrac{\\text{days}(t_0,t_1)}{365}\\)\nOriginal scheduled payment times from issue are \\(t_i=i/f\\). we only price the bonds that \\(t_i&gt;\\Delta\\) because these are the cashflows that have accured up until time \\(t\\) and the remaining time to payment from valuation date for each cashflow is: \\(\\tau_i(t)=t_i-\\Delta\\)\nPrice using the curve at valuation date \\(t\\): \\(P_t=\\sum_{i:t_i&gt;\\Delta} CF_i\\,D_t(\\tau_i(t))\\)\nFor a book of issues \\(\\mathcal{B}_t\\) with weights \\(w_b\\) (we don’t have weights here): \\(PV_t=\\sum_{b\\in \\mathcal{B}_t} w_b\\,P_t(b)\\)\n\ndef book_pv_cutoff(valuation_date, df_func, cutoff_date):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], valuation_date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ndef shifted_df_func(df_func, shift_func):\n    def _f(t):\n        t = np.array(t, dtype=float)\n        return df_func(t) * np.exp(-shift_func(t) * t)\n    return _f\n\n\ndef key_bump_func(key, bump_bp=1.0):\n    values = np.zeros(len(issue_maturities), dtype=float)\n    key_idx = issue_maturities.index(key)\n    values[key_idx] = bump_bp / 10000.0\n\n    def shift(t):\n        t = np.array(t, dtype=float)\n        return np.interp(t, issue_maturities, values, left=0.0, right=0.0)\n\n    return shift\n\n\ndef curve_date_for(d):\n    if d in df_dec.index:\n        return d\n    idx = df_dec.index.searchsorted(d, side=\"right\") - 1\n    if idx &lt; 0:\n        return None\n    return df_dec.index[idx]\n\n\ndef book_pv(date, df_func):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ncurves_cache = {}\n\n\ndef get_curves_for(date):\n    if date in curves_cache:\n        return curves_cache[date]\n    out = build_curves_for_date(date)\n    if out is None:\n        curves_cache[date] = None\n        return None\n    _, curves_d, _ = out\n    curves_cache[date] = curves_d\n    return curves_d\n\n\n\n\nmetrics_rows = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df0 = curve[\"df_func\"]\n        pv0, _ = book_pv_cutoff(date, df0, cutoff_date=date)\n        metrics_rows.append({\"date\": date, \"method\": method, \"pv\": pv0})\n\nmetrics_df = pd.DataFrame(metrics_rows).set_index([\"date\", \"method\"]).sort_index()\n\n\n\nmethods = [\"loglinear\", \"pchip\", \"nss\", \"qp\"]\n\npv_total = (\n    metrics_df.reset_index()\n    .pivot(index=\"date\", columns=\"method\", values=\"pv\")\n    .sort_index()\n)\n\npv_rows = []\nfailed = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df_func = curve[\"df_func\"]\n        _, buckets = book_pv_cutoff(date, df_func, cutoff_date=date)\n        for T in issue_maturities:\n            pv_rows.append({\"date\": date, \"method\": method, \"maturity\": T, \"pv\": buckets.get(T, 0.0)})\n\npv_df = pd.DataFrame(pv_rows)\npv_buckets = pv_df.pivot_table(index=\"date\", columns=[\"method\", \"maturity\"], values=\"pv\").sort_index()\n\nprint(\"Total PV by method (last day):\")\ndisplay(pv_total)\n\nprint(\"Bucket PV (last date)\")\nlast_date = pv_df[\"date\"].max()\nlast_bucket = pv_df[pv_df[\"date\"] == last_date].pivot_table(index=\"maturity\", columns=\"method\", values=\"pv\")\ndisplay(last_bucket)\n\nplt.figure()\nfor method in pv_total.columns:\n    plt.plot(pv_total.index, pv_total[method], label=method)\nplt.title(\"Synthetic Book Total PV (Monthly)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nplt.figure()\nlast_bucket.plot(kind=\"bar\", ax=plt.gca())\nplt.title(f\"Each bucket PV by Method ({last_date.date()})\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nTotal PV by method (last day):\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\ndate\n\n\n\n\n\n\n\n\n1990-01-31\n3.999816\n3.999349\n4.001830\n4.004113\n\n\n1990-02-28\n8.007579\n8.006109\n8.012242\n8.016726\n\n\n1990-03-31\n12.003136\n12.000914\n12.004738\n12.017158\n\n\n1990-04-30\n15.838917\n15.837035\n15.838465\n15.859415\n\n\n1990-05-31\n20.350563\n20.347476\n20.353669\n20.375817\n\n\n...\n...\n...\n...\n...\n\n\n2025-09-30\n484.048043\n485.413949\n485.289343\n486.914561\n\n\n2025-10-31\n486.071558\n486.930897\n487.319872\n488.950688\n\n\n2025-11-30\n487.714383\n489.811404\n489.252157\n491.052541\n\n\n2025-12-31\n482.340428\n484.351486\n483.847004\n485.803466\n\n\n2026-01-31\n481.069937\n482.642714\n482.367938\n484.081282\n\n\n\n\n433 rows × 4 columns\n\n\n\nBucket PV (last date)\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\nmaturity\n\n\n\n\n\n\n\n\n2\n24.258462\n24.266940\n24.265987\n24.267663\n\n\n5\n60.518216\n60.565183\n60.549000\n60.551390\n\n\n10\n115.921913\n116.063170\n116.097292\n116.098827\n\n\n30\n280.371346\n281.747421\n281.455658\n283.163402",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "title": "1. Fixed income and yield curve construction",
    "section": "15) Risk and sensitivity measures of bonds",
    "text": "15) Risk and sensitivity measures of bonds\n\n15.1 PV01/ DV01\nfor measuring the risk of these bonds, we can use the sensitivity of the price of bond to a little change in interest rates.\nPV01 is the change in PV of the bond when the curve bumps by 1 basis point. DV01 is the value in dollar unit of that which is kind of the same concept\n\\(\\dfrac{dP}{dy}=-\\sum_{i=1}^n CF_i,t_i,D(t_i)\\)\nFor approximation, we take \\(\\Delta=0.0001\\) (1 basis point move), Parallel-shifted discount factor will be:\n\\(D^{up}(t)=\\exp(-(z(t)+\\Delta)t)\\)\nPV01 will be:\n\\(PV01=P_0-P_{up}\\)\n\n\n15.2 Convexity\nconvexity is basically the sensitivity of PV01 to a little change in curve. and it’s the second order partial derivative of price from yield\n\\(\\dfrac{d^2P}{dy^2}=\\sum_{i=1}^n CF_i,t_i^2,D(t_i)\\)\nUsing central differences with shift size \\(\\Delta\\):\n\\(Conv=\\dfrac{P_{down}+P_{up}-2P_0}{P_0\\Delta^2}\\) where \\(P_{up}\\) uses \\(+\\Delta\\) and \\(P_{down}\\) uses \\(-\\Delta\\).\n\n\n\n15.3 Key Rate Duration (KRD)\nthe problem is that we assume that yields parallel shift. but they can twist. if 2Y goes up it doesn’t mean 10Y will go up exactly the same amount. (Duration is a normalized version of PV01)\nSo we choose key tenors \\(k_1&lt;k_2&lt;\\dots&lt;k_m\\). as our key rates and measure the sensitivity of them while the other tenors in the curve stay the same.\nTent / triangular bump shape\nDefine localized bump functions \\(b_j(t)\\) such that \\(b_j(K_j)=1\\) and \\(b_j(t)=0\\) outside a neighborhood.\nA triangular bump with edges \\(L_j&lt;R_j\\):\n\\(b_j(t)=0\\) for \\(t\\le L_j\\)\n\\(b_j(t)=\\dfrac{t-L_j}{K_j-L_j}\\) for \\(L_j&lt;t\\le K_j\\)\n\\(b_j(t)=\\dfrac{R_j-t}{R_j-K_j}\\) for \\(K_j&lt;t\\le R_j\\)\n\\(b_j(t)=0\\) for \\(t&gt;R_j\\)\nBumped zero curve (only around key \\(k_j\\)):\n\\(z^{(j)}(t)=z(t)+\\Delta b_j(t)\\)\nSo bumped discount factors:\n\\(D^{(j)}(t)=\\exp(-(z(t)+\\Delta b_j(t))t)\\)\nPrice under the key bump:\n\\(P^{(j)}(\\Delta)=\\sum_{i=1}^n CF_i,D(t_i),e^{-\\Delta,b_j(t_i),t_i}\\)\nKRD definition (finite-difference)\n\\(KRD_j=\\dfrac{P_0-P^{(j)}}{P_0\\Delta}\\)\n\n\nrisk_rows = []\nkrd_rows = []\n\nfor (date, method), row in metrics_df.iterrows():\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    df_func = curves_d[method][\"df_func\"]\n    pv0 = row[\"pv\"]\n\n    shift_up = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up, _ = book_pv(date, shift_up)\n    pv_dn, _ = book_pv(date, shift_dn)\n\n    pv01 = (pv_dn - pv_up) / 2.0\n    convexity = (pv_up + pv_dn - 2.0 * pv0) / (pv0 * (0.0001 ** 2)) if pv0 != 0 else np.nan\n\n    risk_rows.append({\n        \"date\": date,\n        \"method\": method,\n        \"pv01\": pv01,\n        \"convexity\": convexity,\n    })\n\n    for k in issue_maturities:\n        bump = key_bump_func(k, bump_bp=1.0)\n        df_bump = shifted_df_func(df_func, bump)\n        pv_bump, _ = book_pv(date, df_bump)\n        krd = (pv0 - pv_bump) / 0.0001\n        krd_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"key\": k,\n            \"krd\": krd,\n        })\n\nrisk_df = pd.DataFrame(risk_rows).set_index([\"date\", \"method\"]).sort_index()\nmetrics_df = metrics_df.join(risk_df)\n\nkrd_df = pd.DataFrame(krd_rows).set_index([\"date\", \"method\", \"key\"]).sort_index()\n\nprint(\"Risk metrics (last date)\")\ndisplay(metrics_df[[\"pv01\", \"convexity\"]].tail(4))\n\nprint(\"KRD for last date\")\ndisplay(krd_df.tail(4))\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"pv01\"], label=method)\nplt.title(\"PV01 of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV01\")\nplt.legend()\nplt.show()\n\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"convexity\"], label=method)\nplt.title(\"Convexity of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Convexity\")\nplt.legend()\nplt.show()\n\n\n\nmethods = sorted(metrics_df.index.get_level_values(\"method\").unique())\nkrd_panel = krd_df.reset_index()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\naxes = axes.flatten()\nfig.subplots_adjust(right=0.88)\n\nfor ax, method in zip(axes, methods, strict=True):\n    data = krd_panel[krd_panel[\"method\"] == method]\n    pivot = data.pivot(index=\"date\", columns=\"key\", values=\"krd\").reindex(columns=issue_maturities)\n    im = ax.imshow(pivot.values.T, aspect=\"auto\", origin=\"lower\")\n    ax.set_title(method)\n    ax.set_yticks(range(len(issue_maturities)))\n    ax.set_yticklabels([f\"{k}Y\" for k in issue_maturities])\n\n    tick_idx = np.linspace(0, len(pivot.index) - 1, 6).astype(int)\n    ax.set_xticks(tick_idx)\n    ax.set_xticklabels([pivot.index[i].strftime(\"%Y\") for i in tick_idx])\n\ncax = fig.add_axes([0.90, 0.15, 0.02, 0.7])\nfig.colorbar(im, cax=cax, label=\"KRD\")\nfig.suptitle(\"KRD Heatmap Across Time\", y=1.02)\nplt.tight_layout(rect=[0, 0, 0.88, 1])\nplt.show()\n\nRisk metrics (last date)\n\n\n\n\n\n\n\n\n\n\npv01\nconvexity\n\n\ndate\nmethod\n\n\n\n\n\n\n2026-01-31\nloglinear\n0.357311\n107.577415\n\n\nnss\n0.359623\n108.064523\n\n\npchip\n0.358863\n107.715321\n\n\nqp\n0.362471\n108.928968\n\n\n\n\n\n\n\nKRD for last date\n\n\n\n\n\n\n\n\n\n\n\nkrd\n\n\ndate\nmethod\nkey\n\n\n\n\n\n2026-01-31\nqp\n2\n199.706778\n\n\n5\n584.865209\n\n\n10\n1608.487463\n\n\n30\n1104.026025",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html",
    "href": "notebooks/02_portfolio_optimization_MV_models.html",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "",
    "text": "1) Introduction\nThis notebook builds and backtests long-only portfolios under a realistic workflow:\nIn this project we have a dataset for all the stocks in Nasdaq from 1970 to 2026. we want to pick 100 stocks from them and see how we can weight them to reach a stable positive return that would be considered a better investment decision than just investing in all the stocks in equal weights.\nFor comparing the strategies and different models we can use return and risk. we always want higher return and lower risk. lower risk decreases the probability of negative or too negative return and higher return is basiacally what we get on our money. there is a trade off between wanting higher return and lower risk. we can’t minimize risk and reach the highest return. if we want lower risk we have to accept lower return.\nIn this project we use some of the models for weighting assets so we can reach the best portfolios in the risk-return trade off. we use other models and approaches in future projects\nWe trade \\(N\\) stocks, indexed by \\(i \\in \\{1,\\dots,N\\}\\), at daily dates \\(t\\).\nMarket data - Close price: \\(P_{t,i}\\) - Volume (shares): \\(V_{t,i}\\) - Dollar volume (we use these to identify the most liquid stocks in each rebalance period): \\(DV_{t,i} = P_{t,i} V_{t,i}\\)\nReturns - Simple return: \\(r_{t,i} = \\frac{P_{t,i}}{P_{t-1,i}} - 1\\) - Log return: \\(r_{t,i} = \\log(P_{t,i}) - \\log(P_{t-1,i})\\) (we use simple here)\nWe stack daily returns into a vector \\(r_t \\in \\mathbb{R}^N\\) and an estimation matrix \\(R_t \\in \\mathbb{R}^{T \\times N}\\) using the last \\(T\\) days before a rebalance.\nPortfolio - Portfolio weights (held through each period \\(t\\)): \\(w_t \\in \\mathbb{R}^N\\) - Budget constraint (sum of all the weights should be 1. we don’t use leverage for this project): \\(\\mathbf{1}^\\top w_t = 1\\)\nRisk-free rate is used for calculation of Sharpe Ratio - Annual: \\(r_f^{(ann)}\\) - Daily (compounded): \\(r_f^{(d)} = (1+r_f^{(ann)})^{1/252} - 1\\)\nAnnualization - If \\(\\mu^{(d)}\\) is a daily mean return vector, then \\(\\mu^{(ann)} = 252\\,\\mu^{(d)}\\) - If \\(\\Sigma^{(d)}\\) is a daily covariance matrix, then \\(\\Sigma^{(ann)} = 252\\,\\Sigma^{(d)}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#introduction",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#introduction",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "",
    "text": "Long-only constraint: \\(w_t \\ge 0\\)\nOptional cap (for making models diverse more and don’t overfit on some assets, we define a \\(w_{\\max}\\) which is the max weight an asset can get in the portfolio): \\(w_{t,i} \\le w_{\\max}\\)\n\n\n\n\n\nImports and plotting style\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nfrom scipy.optimize import minimize\nfrom sklearn.covariance import LedoitWolf, OAS\nfrom cycler import cycler\n\ncolors = [\"#069AF3\",\"#FE420F\", \"#00008B\", \"#008080\", \"#800080\",\n          \"#7BC8F6\", \"#0072B2\",\"#04D8B2\", \"#CC79A7\", \"#FF8072\", \"#9614fa\", \"#DC143C\"]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\nplt.rcParams.update({\n    \"figure.figsize\": (6, 3),\n    \"figure.dpi\": 300,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 10,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 10,\n})\n\ndef make_color_map(names, palette=colors):\n    names = list(names)\n    return {name: palette[i % len(palette)] for i, name in enumerate(names)}\n\n\n\ninitializing the parameters and data\n\nrf = 0.04\n\nblend = {\n    \"MV (SampleCov)\": 0.15,\n    \"MV (LedoitWolf)\": 0.15,\n    \"MV (OAS)\": 0.15,\n    \"MV (EWMA)\": 0.15,\n    \"Ridge MV\": 0.15,\n\n    \"MaxSharpe\": 0.10,               \n    \"MaxSharpe (FrontierGrid)\": 0.10,  \n\n    \"MinVar (SampleCov)\": 0.20,\n    \"MinVar (LedoitWolf)\": 0.20,\n    \"MinVar (OAS)\": 0.20,\n    \"MinVar (EWMA)\": 0.20,\n}\n\nstrategy_cov_key = {\n    \"EW\": \"LedoitWolf\",\n\n    \"MinVar (SampleCov)\": \"SampleCov\",\n    \"MinVar (LedoitWolf)\": \"LedoitWolf\",\n    \"MinVar (OAS)\": \"OAS\",\n    \"MinVar (EWMA)\": \"EWMA\",\n\n    \"MV (SampleCov)\": \"SampleCov\",\n    \"MV (LedoitWolf)\": \"LedoitWolf\",\n    \"MV (OAS)\": \"OAS\",\n    \"MV (EWMA)\": \"EWMA\",\n\n    \"Ridge MV\": \"LedoitWolf\",\n\n    \"MaxSharpe\": \"LedoitWolf\",                \n    \"MaxSharpe (FrontierGrid)\": \"LedoitWolf\",  \n}\nstrategy_names = list(strategy_cov_key.keys())\nstrategy_colors = make_color_map(strategy_names)\n\n\ndef print_warn(msg):\n    print(f\"[WARN] {msg}\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#load-data-and-compute-returns",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#load-data-and-compute-returns",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "2) Load data and compute returns",
    "text": "2) Load data and compute returns\nthe data used in this project can be downloaded from here (Stooq US (nasdaq) daily market data)\n\ndf = pd.read_parquet(r\"..\\data\\nasdaq_all_close_volume.parquet\")\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n\nclose_map, vol_map = {}, {}\nfor c in df.columns:\n    c = str(c)\n    if c == \"Date\" or \"__\" not in c:\n        continue\n    t, f = c.rsplit(\"__\", 1)\n    f = f.lower()\n    if f == \"close\":\n        close_map[t] = c\n    elif f == \"volume\":\n        vol_map[t] = c\n\ncommon = sorted(set(close_map).intersection(vol_map))\n\nclose_prices = df[[close_map[t] for t in common]].copy(); close_prices.columns = common\nvolumes = df[[vol_map[t] for t in common]].copy(); volumes.columns = common\nclose_prices.index = df[\"Date\"]\nvolumes.index = df[\"Date\"]\n\nclose_prices = close_prices.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).astype(np.float32)\nvolumes = volumes.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).astype(np.float32)\n\nstart = pd.Timestamp(\"2016-01-01\")\nclose_prices = close_prices.loc[close_prices.index &gt;= start]\nvolumes = volumes.loc[volumes.index &gt;= start]\nend = close_prices.index.max()\nclose_prices = close_prices.loc[close_prices.index &lt;= end]\nvolumes = volumes.loc[volumes.index &lt;= end]\n\nidx = close_prices.index.intersection(volumes.index)\ncols = close_prices.columns.intersection(volumes.columns)\nclose_prices = close_prices.loc[idx, cols]\nvolumes = volumes.loc[idx, cols]\nfirst_close = close_prices.apply(pd.Series.first_valid_index)\nfirst_vol   = volumes.apply(pd.Series.first_valid_index)\n\nfirst_date = pd.concat([first_close, first_vol], axis=1).max(axis=1)\n\nreturns = close_prices.pct_change(fill_method=None)\nreturns = returns.replace([np.inf, -np.inf], np.nan).astype(np.float32)\n\nprint(\"close_prices:\", close_prices.shape, \"volumes:\", volumes.shape, \"returns:\", returns.shape)\nprint(\"Date range:\", returns.index.min().date(), \"to\", returns.index.max().date())\n\nclose_prices: (2532, 4382) volumes: (2532, 4382) returns: (2532, 4382)\nDate range: 2016-01-04 to 2026-01-28",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#rebalance-dates",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#rebalance-dates",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "3) Rebalance dates",
    "text": "3) Rebalance dates\nfor optimizing a portfolio we have to use past data (like mu and cov estimation) to optimize the model on past and use the optimal weights in future and expect to get same results as what we got from past. we will never get the same results unless market exactly repeats itself. so we have to test our model out of sample to see the real performance. Also we have to use rebalancing. for example if we want to test a model in one year, we can optimize the model on the past 5 years and test it on this year, but in one year markets can change a lot and estimations of model become irrelevant. so we can use rebalancing and for each month of that year, take the past year of that month as our in-sample and test the optimal weights on that month and then go to the next month and repeat this process every month. in this way we include up to date data in our model and update the weights faster and adapt to market regimes faster.\nIn this project we use monthly rebalancing with 1 year lookback window for each month.\nWe rebalance at the last available trading day of each period\n\nrebal_dates_raw = (\n    returns.groupby(pd.Grouper(freq=\"ME\"))\n           .apply(lambda x: x.index[-1])\n           .dropna()\n)\nrebal_dates = pd.DatetimeIndex(rebal_dates_raw.values)\n\n\nprint(\"Candidate rebalance dates:\", len(rebal_dates))\nprint(\"First 3:\", [d.date() for d in rebal_dates[:3]])\nprint(\"Last 3:\", [d.date() for d in rebal_dates[-3:]])\n\nCandidate rebalance dates: 121\nFirst 3: [datetime.date(2016, 1, 29), datetime.date(2016, 2, 29), datetime.date(2016, 3, 31)]\nLast 3: [datetime.date(2025, 11, 28), datetime.date(2025, 12, 31), datetime.date(2026, 1, 28)]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#liquidity-filtered-stock-selection",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#liquidity-filtered-stock-selection",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "4) Liquidity-filtered stock selection",
    "text": "4) Liquidity-filtered stock selection\nRight now our dataset contains hundreds of stocks. At each rebalance date \\(t \\in \\mathcal{T}\\), we want to include some of the stocks that have the most liquidity and certain data in that date. So we want to build a tradable combination of stocks \\({U}_t\\).\n\n4.1 Minimum history\nA ticker is included only if it has at least \\(D\\) valid daily observations before \\(t\\). The asset must exist long enough that estimates are meaningful.\nWe set \\(D\\) as 252 days or 1 year\n\n\n4.2 Average Dollar Volume (ADV)\nWe define daily dollar volume as Volume multiplied by Prices: \\[\nDV_{\\tau,i} = P_{\\tau,i} V_{\\tau,i}\n\\]\nCompute average dollar volume over a window of length \\(L\\) (using only \\(\\tau &lt; t\\)): \\[\nADV_{t,i} = \\frac{1}{L} \\sum_{\\tau=t-L}^{t-1} DV_{\\tau,i}\n\\]\nWe set \\(L\\) as 1 year to capture the stocks with most \\(ADV\\) in the past year of that month.\n\n\n4.3 Selection rule (Top-K liquidity)\nLet \\(K\\) be the target universe size (We use 100).\nWe select: \\[\n{U}_t = \\operatorname{TopK}\\big(ADV_{t,i}\\big)\n\\]\nin this way we don’t have survivorship bias and we only use big stocks of that time, not the stocks that we already know are big now but were not back then.\n\ndef select_liquid_universe(dt, close_prices, volumes, top_n, liq_lookback, min_listing_days, min_obs):\n    idx = close_prices.index\n    if dt not in idx:\n        return [], pd.Series(dtype=np.float32)\n\n    pos = idx.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n\n    need = max(min_listing_days, liq_lookback)\n    if pos &lt; need:\n        return [], pd.Series(dtype=np.float32)\n\n    cutoff_date = idx[pos - min_listing_days]\n    seasoned = (first_date.notna()) & (first_date &lt;= cutoff_date)\n    cols = close_prices.columns[seasoned.reindex(close_prices.columns).fillna(False).values]\n    \n    start = pos - liq_lookback\n    end = pos\n    c = close_prices.iloc[start:end][cols]\n    v = volumes.iloc[start:end][cols]\n    dv = c * v\n\n    obs_ok = dv.notna().sum(axis=0) &gt;= min_obs\n    pos_ok = (dv &gt; 0).sum(axis=0) &gt;= min_obs\n\n    selected = dv.columns[obs_ok & pos_ok]\n\n    adv = dv[selected].mean(axis=0, skipna=True).replace([np.inf, -np.inf], np.nan).dropna()\n    if len(adv) == 0:\n        return [], pd.Series(dtype=np.float32)\n\n    top = adv.nlargest(min(int(top_n), len(adv)))\n    return top.index.tolist(), top.astype(np.float32)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#expected-return-model-momentum-signal-for-mu_t",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#expected-return-model-momentum-signal-for-mu_t",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "5) Expected return model: momentum signal for \\(\\mu_t\\)",
    "text": "5) Expected return model: momentum signal for \\(\\mu_t\\)\nMin-Var models only try to optimize based on risk (Covariance) but Mean-Var and Max-Sharpe models need an expected-return vector \\(\\mu_t\\). if we use an average of returns in a period, it can be noisy and the model will not generalize based on that. so we need a clear and stable estimation of \\(\\mu\\) in each period and rebalance.\nWe use a simple momentum model that is:\n\neasy to compute\nconsistent across universes\nuses only past returns\n\n\n5.1 Lookback cumulative return\nWe pick a momentum lookback length \\(H\\) (like 6 months or 126 trading days) and then Define cumulative simple return for asset \\(i\\): \\[\nm_{t,i} = \\prod_{\\tau=t-H}^{t-1} (1+r_{\\tau,i}) - 1\n\\]\n(If you use log returns, you can equivalently use a sum of log returns.)\n\n\n5.2 Cross-sectional standardization (z-score)\nMomentum values are not comparable across time unless we standardize.\nWe compute a cross-sectional z-score within the selected universe \\(\\mathcal{U}_t\\):\nIf \\(\\bar{m}_t\\) is the mean of \\(m_{t,i}\\) across \\(i \\in \\mathcal{U}_t\\), and \\(s_t\\) is the cross-sectional standard deviation. Define: \\[\nz_{t,i} = \\frac{m_{t,i} - \\bar{m}_t}{s_t}\n\\]\nThis makes \\(z_{t,i}\\) dimensionless and stable across different regimes.\n\n\n5.3 Mapping the signal to expected returns\nA common simple mapping is: \\[\n\\mu_{t,i}^{(d)} = \\kappa\\, z_{t,i}\n\\]\nHere \\(\\kappa\\) is a scaling constant that controls the magnitude of expected returns.\nTwo main ways to calculate \\(\\kappa\\):\n(A) Target cross-sectional dispersion Choose a target daily standard deviation of expected returns, call it \\(\\sigma_\\mu^{(d)}\\), and set: \\[\n\\kappa = \\frac{\\sigma_\\mu^{(d)}}{\\operatorname{std}(z_t)}\n\\]\n(B) Target annual expected-return range If you want a typical annual spread of, say, \\(\\sigma_\\mu^{(ann)}\\), use: \\[\n\\sigma_\\mu^{(d)} = \\frac{\\sigma_\\mu^{(ann)}}{252}\n\\] then we apply option (A) on \\(\\sigma_\\mu^{(d)}\\) to get to \\(\\kappa\\).\nThis model is deliberately simple: it gives the optimizer a stable ranking signal without overfitting.\n\ndef momentum_score_from_returns(ret_window, mode=\"6-1\"):\n    R = ret_window.replace([np.inf, -np.inf], np.nan).dropna(how=\"any\")\n    T = len(R)\n    if T &lt; 80:\n        return R.mean().values.astype(np.float64)\n    if mode == \"12-1\":\n        lookback, skip = 252, 21\n    elif mode == \"6-1\":\n        lookback, skip = 126, 21\n    elif mode == \"3-0\":\n        lookback, skip = 63, 0\n    else:\n        raise ValueError(\"Unknown momentum mode\")\n    if T &lt; lookback + skip + 5:\n        lookback = min(lookback, max(63, T - skip - 1))\n    R_use = R.iloc[-(lookback + skip):]\n    R_mom = R_use.iloc[:-skip] if skip &gt; 0 else R_use\n    return ((1.0 + R_mom).prod(axis=0) - 1.0).values.astype(np.float64)\n\n\n\ndef winsorize_and_zscore(x, p_lo=0.05, p_hi=0.95):\n    x = np.asarray(x, dtype=np.float64)\n    lo, hi = np.quantile(x, [p_lo, p_hi])\n    x = np.clip(x, lo, hi)\n    x = x - x.mean()\n    return x / (x.std() + 1e-12)\n\n\ndef scale_mu_to_target_sharpe(mu_dir, cov_ann, target_sharpe_ann, mu_cap_ann):\n    mu = np.asarray(mu_dir, dtype=np.float64).flatten()\n    if np.all(np.abs(mu) &lt; 1e-12):\n        return np.zeros_like(mu)\n    A = cov_ann + 1e-8 * np.eye(cov_ann.shape[0])\n    try:\n        x = np.linalg.solve(A, mu)\n    except np.linalg.LinAlgError:\n        x = np.linalg.lstsq(A, mu, rcond=None)[0]\n    q = float(mu @ x)\n    if (not np.isfinite(q)) or q &lt;= 1e-18:\n        return np.zeros_like(mu)\n    s = float(target_sharpe_ann) / np.sqrt(q)\n    return np.clip(s * mu, -mu_cap_ann, mu_cap_ann)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#covariance-estimation-building-sigma_t",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#covariance-estimation-building-sigma_t",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "6) Covariance estimation: building \\(\\Sigma_t\\)",
    "text": "6) Covariance estimation: building \\(\\Sigma_t\\)\nRisk is represented by the covariance matrix \\(\\Sigma_t\\) of returns for the current set of stocks \\(\\mathcal{S}_t\\).\nIf \\(R_t \\in \\mathbb{R}^{T \\times N}\\) is the matrix of past returns (columns are assets), in the estimation window \\([t-T,\\,t)\\).\nWe set \\(\\bar{r}\\) as the sample mean vector in the window.the demeaned matrix will be: \\[\n\\tilde{R}_t = R_t - \\mathbf{1}\\bar{r}^\\top\n\\]\n\\[\n\\tilde{R}_t \\;=\\;\n\\begin{bmatrix}\nr_{t-T,1}-\\bar r_1 & r_{t-T,2}-\\bar r_2 & \\cdots & r_{t-T,N}-\\bar r_N\\\\\nr_{t-T+1,1}-\\bar r_1 & r_{t-T+1,2}-\\bar r_2 & \\cdots & r_{t-T+1,N}-\\bar r_N\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nr_{t-1,1}-\\bar r_1 & r_{t-1,2}-\\bar r_2 & \\cdots & r_{t-1,N}-\\bar r_N\n\\end{bmatrix}\n\\]\n\n6.1 Sample covariance\nThe classic estimator is: \\[\nS_t = \\frac{1}{T-1}\\tilde{R}_t^\\top \\tilde{R}_t\n\\]\nThis is unbiased under ideal assumptions, but \\(S_t\\) can be noisy when \\(T\\) is not much larger than \\(N\\).\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\operatorname{Var}(r_1) & \\operatorname{Cov}(r_1,r_2) & \\cdots & \\operatorname{Cov}(r_1,r_N)\\\\\n\\operatorname{Cov}(r_2,r_1) & \\operatorname{Var}(r_2) & \\cdots & \\operatorname{Cov}(r_2,r_N)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\operatorname{Cov}(r_N,r_1) & \\operatorname{Cov}(r_N,r_2) & \\cdots & \\operatorname{Var}(r_N)\n\\end{bmatrix}\n\\qquad\n\\operatorname{Cov}(r_i,r_j)=s_{ij}=\\frac{1}{T-1}\\sum_{k=1}^{T}\\tilde r_{k,i}\\tilde r_{k,j}\n\\]\n\n\n6.2 Diagonal covariance (no correlations)\nA “failsafe” stable model sets correlations to zero: \\[\n\\Sigma_t = \\operatorname{diag}(S_t)\n\\]\nFrom \\(S_t=[s_{ij}]\\), the diagonal-only covariance is\n\\[\n\\Sigma_{\\text{diag}} =\n\\begin{bmatrix}\n\\operatorname{Var}(r_1) & 0 & \\cdots & 0\\\\\n0 & \\operatorname{Var}(r_2) & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & \\operatorname{Var}(r_N)\n\\end{bmatrix}\n\\] This reduces estimation error but the cost is ignoring diversification effects.\n\n\n6.3 Shrinkage estimators (Ledoit–Wolf / OAS intuition)\nShrinkage stabilizes covariance by mixing the noisy sample estimate with a structured target: \\[\n\\Sigma_t = (1-\\delta)S_t + \\delta F_t\n\\]\nTypical target choices are: - scaled identity: \\(F_t = \\bar{\\sigma}^2 I\\) where \\(\\bar{\\sigma}^2\\) is average variance - diagonal target: \\(F_t = \\operatorname{diag}(S_t)\\)\nThe shrinkage intensity \\(\\delta \\in [0,1]\\) is chosen automatically by the estimator (Ledoit–Wolf or OAS).\nInterpretation: when data is noisy, we can increase \\(\\delta\\) to reduce extreme correlations.\n\n\n6.4 EWMA covariance (time-decayed risk)\nEWMA weights recent returns more, capturing volatility clustering.\nwe set \\(\\lambda \\in (0,1)\\) as the decay (example: 0.94).\nDefine demeaned return vector \\(\\tilde{r}_{t-1} = r_{t-1} - \\bar{r}\\) and update: \\[\n\\Sigma_t = \\lambda \\Sigma_{t-1} + (1-\\lambda)\\tilde{r}_{t-1}\\tilde{r}_{t-1}^\\top\n\\]\nEWMA is popular because it reacts faster to regime changes than the sample covariance.\n\newma_lambda = 0.94\njitter, psd_eps = 1e-10, 1e-10\n\ndef make_psd(sigma, eps=1e-10):\n    sigma = 0.5 * (sigma + sigma.T)\n    vals, vecs = np.linalg.eigh(sigma)\n    vals = np.maximum(vals, eps)\n    out = (vecs * vals) @ vecs.T\n    return 0.5 * (out + out.T)\n\n\ndef ewma_covariance(x, lam=0.94):\n    x = x - x.mean(axis=0, keepdims=True)\n    T, N = x.shape\n    S = np.zeros((N, N), dtype=np.float64)\n    a = 1.0 - lam\n    for t in range(T):\n        xt = x[t][:, None]\n        S = lam * S + a * (xt @ xt.T)\n    scale = 1.0 - (lam ** max(T, 1))\n    if scale &gt; 1e-12:\n        S = S / scale\n    return S\n\ndef estimate_covariance(window):\n    x = window.values.astype(np.float64)\n    nn = x.shape[1]\n\n    cov_daily = {\n        \"SampleCov\": np.cov(x, rowvar=False, ddof=1).astype(np.float64),\n        \"LedoitWolf\": LedoitWolf().fit(x).covariance_.astype(np.float64),\n        \"OAS\": OAS().fit(x).covariance_.astype(np.float64),\n        \"EWMA\": ewma_covariance(x, lam=ewma_lambda).astype(np.float64),\n    }\n\n    out = {}\n    for k, c in cov_daily.items():\n        c = 0.5 * (c + c.T)\n        c += jitter * np.eye(nn)\n        out[k] = 252.0 * make_psd(c, psd_eps)\n    return out",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#portfolio-return-and-variance",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#portfolio-return-and-variance",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "7) Portfolio return and variance",
    "text": "7) Portfolio return and variance\nBefore we optimize anything, we need to know what:\n\nexpected return vector \\(\\mu\\)\n\ncovariance matrix \\(\\Sigma\\)\n\nis and we need to understand how they translate into portfolio return and portfolio risk.\n\n\n7.1 Portfolio weights\nWe suppose the portfolio weights of all the stocks we picked are \\[\nw =\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix},\n\\qquad\n\\mathbf{1}^\\top w = 1\n\\]\n\n\\(w_i\\) is the fraction of capital invested in asset \\(i\\)\n\\(\\mathbf{1}^\\top w = 1\\) means all the investment which is 1 because we don’t use leverage or short-selling.\nfor long-only portfolios we also require \\(w \\ge 0\\)\n\n\n\n\n7.2 Portfolio return\nall assets return vector for each period is \\[\nr =\n\\begin{bmatrix}\nr_1\\\\\nr_2\\\\\n\\vdots\\\\\nr_N\n\\end{bmatrix}\n\\]\nThen the portfolio return is the weighted sum of these returns: \\[\nr_p = w^\\top r\n\\]\nor \\[\nr_p =\n\\begin{bmatrix}\nw_1 & w_2 & \\cdots & w_N\n\\end{bmatrix}\n\\begin{bmatrix}\nr_1\\\\\nr_2\\\\\n\\vdots\\\\\nr_N\n\\end{bmatrix}\n= \\sum_{i=1}^{N} w_i r_i\n\\]\n\n\n\n7.3 Expected portfolio return\nThis ia what the optimizer targets We define the expected return vector for each period: \\[\nr_p=\n\\begin{bmatrix}\nr_{p,1}\\\\\nr_{p,2}\\\\\n\\vdots\\\\\nr_{p,T}\n\\end{bmatrix}=\n\\begin{bmatrix}\nr_{1,1} & r_{1,2} & \\cdots & r_{1,N}\\\\\nr_{2,1} & r_{2,2} & \\cdots & r_{2,N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nr_{T,1} & r_{T,2} & \\cdots & r_{T,N}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix}\n\\]\nexpectation of \\(r_p\\) is: \\[\n\\mathbb{E}[r_p] = \\mathbb{E}[w^\\top r] = w^\\top \\mathbb{E}[r] = w^\\top \\mu\n\\]\nExpanded: \\[\n\\mathbb{E}[r_p] = \\sum_{i=1}^{N} w_i \\mu_i\n\\]\nThis is what we optimize when we want to maximize the expected return of our portfolio\n\n\n\n7.4 Portfolio variance (risk)\nRisk in classical mean–variance is measured by variance.\nThe covariance matrix is: \\[\n\\Sigma=\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{N1} & \\sigma_{N2} & \\cdots & \\sigma_{NN}\n\\end{bmatrix},\n\\]\nThe portfolio variance is the quadratic form: \\[\n\\operatorname{Var}(r_p) = \\operatorname{Var}(w^\\top r) = w^\\top \\Sigma w\n\\]\nOr in expanded form:\n\\[\nw^\\top \\Sigma w =\n\\begin{bmatrix}\nw_1 & w_2 & \\cdots & w_N\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{N1} & \\sigma_{N2} & \\cdots & \\sigma_{NN}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix}=\n\\sum_{i=1}^{N}\\sum_{j=1}^{N} w_i\\sigma_{ij}w_j\n\\]\n\nThe diagonal terms \\(w_i^2 \\sigma_{ii}\\) are the individual risk contributions (variances).\nThe off-diagonal terms \\(w_i w_j \\sigma_{ij}\\) capture correlations (diversification effects).\n\n\n\n\n7.5 Portfolio volatility\nOften we use volatility as standard deviation instead of variance for analyzing portfolio performance: \\[\n\\sigma_p = \\sqrt{w^\\top \\Sigma w}\n\\]\nVariance is mathematically convenient for optimization; volatility is easier to interpret.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#build-estimation-cache-at-rebalance-dates",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#build-estimation-cache-at-rebalance-dates",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "Build estimation cache at rebalance dates",
    "text": "Build estimation cache at rebalance dates\nFor each rebalance date we store: - active tickers (liquidity-selected) - return window - covariance maps - scaled annual excess returns \\(\\mu\\)\nThis speeds up the backtest.\n\ncache = {}\n\ndef rebalances_per_year(rebal_dates_index):\n    idx = pd.DatetimeIndex(rebal_dates_index)\n    if len(idx) &lt; 2:\n        return 1.0\n    per_year = pd.Series(1, index=idx).resample(\"YE\").sum()\n    return float(per_year.median())\n\n\nfor dt in rebal_dates:\n    pos = returns.index.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n    if pos &lt; 252:\n        continue\n\n\n    liquid_tickers, avg_dollar_volume = select_liquid_universe(dt,close_prices, volumes, top_n=100, liq_lookback=252,\n    min_listing_days=252, min_obs=252,)\n\n    if len(liquid_tickers) &lt; 2:\n        continue\n\n    close_for_model = close_prices[liquid_tickers].iloc[pos - 252:pos]\n    if close_for_model.shape[0] &lt; 252:\n        continue\n\n   \n    window = close_for_model.pct_change(fill_method=None).iloc[1:]\n\n    window = window.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n    if window.shape[0] &lt; 251 or window.shape[1] &lt; 2:\n        continue\n\n    active_tickers = window.columns.tolist()\n    window = window.astype(np.float32)\n\n    cov_ann_map = estimate_covariance(window)\n    cov_ann = cov_ann_map[\"LedoitWolf\"]\n\n    score = momentum_score_from_returns(window, mode=\"6-1\")\n    z = winsorize_and_zscore(score, 0.05, 0.95)\n    mu_excess_ann = scale_mu_to_target_sharpe(z, cov_ann, 0.80, 0.30)\n\n    cache[dt] = {\n        \"R\": window,\n        \"mu_excess_ann\": mu_excess_ann,\n        \"cov_ann_map\": cov_ann_map,\n        \"tickers\": active_tickers,\n        \"avg_dollar_volume\": avg_dollar_volume.reindex(active_tickers).astype(np.float32),\n    }\n\nrebal_dates = [d for d in rebal_dates if d in cache]\nif len(rebal_dates) == 0:\n    raise ValueError(\"No rebalance dates\")\n\nuniverse_size = pd.Series({dt: len(cache[dt][\"tickers\"]) for dt in rebal_dates}, name=\"UniverseSize\")\nprint(f\"Universe size across rebalances: min={universe_size.min()}, max={universe_size.max()}, mean={universe_size.mean():.1f}\")\n\nrebal_per_year = rebalances_per_year(rebal_dates)\nrf_daily = (1.0 + rf) ** (1.0 / 252.0) - 1.0\nprint(\"Rebalances per year (estimated):\", round(rebal_per_year, 2))\n\nUniverse size across rebalances: min=100, max=100, mean=100.0\nRebalances per year (estimated): 12.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#optimization-problems-the-math-behind-each-model",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#optimization-problems-the-math-behind-each-model",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "8) Optimization problems (the math behind each model)",
    "text": "8) Optimization problems (the math behind each model)\nAt each rebalance date \\(t\\), we solve for target weights \\(w_t\\) using \\(\\mu_t\\) and \\(\\Sigma_t\\) for the filtered set of stocks \\(U_t\\).\nconstraints are: \\[\n\\mathbf{1}^\\top w_t = 1, \\quad w_t \\ge 0,\\quad w_{t,i} \\le w_{\\max}\n\\]\n\n8.1 Minimum Variance (MinVar)\nMinVar ignores expected returns and finds the lowest-risk portfolio: \\[\n\\min_{w} \\; w^\\top \\Sigma_t w\n\\]\nThis is a convex quadratic program (QP) under \\(\\Sigma_t \\succeq 0\\).\n\ncvx_cache = {}\nridge_mv_gamma = 12.0\ncost_bps = 10\nsolver_order = [\"OSQP\", \"CLARABEL\", \"ECOS\", \"SCS\"]\nturnover_penalty_bps = 10.0\nlong_only, w_min, w_max = True, 0.0, 0.25\n\nridge = 1e-4\n\ndef safe_normalize_weights(w, w_min, w_max, long_only):\n    w = np.asarray(w, dtype=np.float64).flatten()\n    if long_only:\n        w = np.maximum(w, 0.0)\n    if w_min is not None:\n        w = np.maximum(w, w_min)\n    if w_max is not None:\n        w = np.minimum(w, w_max)\n    s = w.sum()\n    if (not np.isfinite(s)) or s &lt;= 0:\n        return None\n    w = w / s\n    for _ in range(2):\n        if long_only:\n            w = np.maximum(w, 0.0)\n        if w_min is not None:\n            w = np.maximum(w, w_min)\n        if w_max is not None:\n            w = np.minimum(w, w_max)\n        s = w.sum()\n        if s &lt;= 0:\n            return None\n        w = w / s\n    return w\n\ndef kappa_annual(rebals_per_year_value):\n    k = 0.0\n    k += cost_bps / 10000.0\n    k += turnover_penalty_bps / 10000.0\n    return float(rebals_per_year_value * k)\n\n\ndef solve_cvx(prob, var):\n    for solver in solver_order:\n        try:\n            if solver == \"OSQP\":\n                kwargs = {\"max_iter\": 8000}\n            elif solver in (\"ECOS\", \"SCS\"):\n                kwargs = {\"max_iters\": 10000}\n            else:\n                kwargs = {}\n            prob.solve(solver=solver, warm_start=True, **kwargs)\n\n            if var.value is not None:\n                w = np.asarray(var.value, dtype=np.float64).flatten()\n                if np.all(np.isfinite(w)):\n                    return w\n        except Exception:\n            continue\n    return None\n\ndef blend_weights(w_star, w_prev, eta):\n    eta = float(np.clip(eta, 0.0, 1.0))\n    return (1.0 - eta) * np.asarray(w_star, dtype=np.float64) + eta * np.asarray(w_prev, dtype=np.float64)\n\ndef constraints_feasible(nn, w_min, w_max, long_only):\n    w_min_eff = 0.0 if long_only else (-np.inf if w_min is None else w_min)\n    w_max_eff = np.inf if w_max is None else w_max\n    if np.isfinite(w_max_eff) and w_max_eff * nn &lt; 1.0 - 1e-9:\n        return False\n    if np.isfinite(w_min_eff) and w_min_eff * nn &gt; 1.0 + 1e-9:\n        return False\n    return True\n\n\ndef get_minvar_solver(nn, ridge, kappa):\n    key = (\"minvar\", nn, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Minimize(cp.quad_form(w, cp.psd_wrap(S)) + kappa * 0.5 * cp.norm1(w - w_prev) + 0.5 * ridge * cp.sum_squares(w))\n    problem = cp.Problem(obj, cons)\n    cvx_cache[key] = (problem, w, S, w_prev)\n    return problem, w, S, w_prev\n\ndef minvar_weights(cov_ann, w_prev, rpy):\n    nn = cov_ann.shape[0]\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    prob, w_var, S_p, wprev_p = get_minvar_solver(nn, ridge, kappa_annual(rpy))\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\n\n\n8.2 Mean–Variance Utility (MV)\nIn this model we use both return and risk and try to maximize this object with our weights: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w\n\\]\nwhere: - \\(\\mu_t^\\top w\\) is for maximizing the return - \\(w^\\top \\Sigma_t w\\) is variance. we subtract it so the trade-off between risk and return happen and try to minimize risk for the cost of reducing return - \\(\\gamma &gt; 0\\) controls risk aversion. this parameter represents our importance of risk and how much we want to minimize it in respect to return.\n\n\n8.3 Regularized Mean–Variance (Ridge MV)\nWhen weights become unstable (high turnover, concentration),we can add an \\(L_2\\) penalty: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w - \\frac{\\eta}{2}\\lVert w\\rVert_2^2\n\\]\nThis discourages extreme weight vectors and improves stability. You can interpret ridge as adding a multiple of the identity to risk: \\[\nw^\\top \\Sigma_t w + \\frac{\\eta}{\\gamma}\\lVert w\\rVert_2^2 = w^\\top\\left(\\Sigma_t + \\frac{\\eta}{\\gamma}I\\right)w\n\\]\n\n\n8.4 Turnover-aware optimization (penalize trading)\nIf \\(w_{t^-}\\) is the portfolio just before rebalancing, turnover is: \\[\n\\operatorname{TO}_t = \\sum_{i=1}^{N}\\lvert w_{t,i} - w_{t^-,i}\\rvert\n\\]\nIt shows us how much weights have changed in rebalancing. the more the weights change, the more unstable the weights get and also the transaction cost would be huge.\nA simple way to reduce churn is to penalize turnover in the objective: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w - \\kappa \\operatorname{TO}_t\n\\]\nThis is convex and works well with CVXPY.\n\nmv_lambda = 6.0\n\ndef get_mv_solver(nn, mv_lambda, ridge, kappa):\n    key = (\"mv\", nn, mv_lambda, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Maximize(mu @ w - 0.5 * mv_lambda * cp.quad_form(w, cp.psd_wrap(S))\n                      - kappa * 0.5 * cp.norm1(w - w_prev)\n                      - 0.5 * ridge * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev)\n    return prob, w, mu, S, w_prev\n\ndef get_reg_mv_solver(nn, mv_lambda, ridge, kappa, gamma_l2):\n    key = (\"ridge_mv\", nn, mv_lambda, ridge, kappa, gamma_l2, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Maximize(mu @ w - 0.5 * mv_lambda * cp.quad_form(w, cp.psd_wrap(S)) - kappa * 0.5 * cp.norm1(w - w_prev) - 0.5 * (ridge + gamma_l2) * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev)\n    return prob, w, mu, S, w_prev\n\n\n\ndef mv_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    prob, w_var, mu_p, S_p, wprev_p = get_mv_solver(nn, mv_lambda, ridge, kappa_annual(rpy))\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\ndef ridge_mv_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    g2 = ridge_mv_gamma / max(nn, 1)\n    prob, w_var, mu_p, S_p, wprev_p = get_reg_mv_solver(nn, mv_lambda, ridge, kappa_annual(rpy), g2)\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\n\n\n8.5 Max-Sharpe portfolios\nThe Sharpe ratio of a portfolio is the excess return on risk ratio of portfolio (comparing to risk free rate) and it’s a main measure for comparing investments: \\[\n\\operatorname{SR}(w) = \\frac{\\mu_t^\\top w - r_f^{(d)}}{\\sqrt{w^\\top \\Sigma_t w}}\n\\]\nDirectly maximizing this fraction is not a simple QP and can be unstable. also we use SLSQP from scipy instead of considering convex optimization\n\n\n8.6 Max Sharpe with Frontier grid\nWe pick a grid of target returns \\(m\\) values and get min variance weights for each \\(m\\) and solve:\n\\[\n\\min_{w}\\; w^\\top \\Sigma_t w\n\\]\nsubject to: \\[\n\\mu_t^\\top w \\ge m,\\quad \\mathbf{1}^\\top w = 1,\\quad w \\ge 0,\\quad w \\le w_{\\max}\n\\]\nFor each solution \\(w(m)\\) we compute its Sharpe: \\[\n\\operatorname{SR}(w(m)) = \\frac{\\mu_t^\\top w(m) - r_f^{(d)}}{\\sqrt{w(m)^\\top \\Sigma_t w(m)}}\n\\]\nAnd choose the best: \\[\nw^\\star = \\arg\\max_{m} \\operatorname{SR}(w(m))\n\\]\nThis is slow but stable and avoids fragile non-convex solvers.\n\ndef max_sharpe_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    bounds = [(0.0 if long_only else (-1.0 if w_min is None else w_min), 1.0 if w_max is None else w_max) for _ in range(nn)]\n    x0 = np.ones(nn, dtype=np.float64) / nn\n    kappa = kappa_annual(rpy)\n    mu_use = np.asarray(mu_excess_ann, dtype=np.float64)\n\n    def neg_obj(w):\n        w = np.asarray(w, dtype=np.float64)\n        if np.any(~np.isfinite(w)):\n            return 1e6\n        ret = float(mu_use @ w)\n        vol = float(np.sqrt(w @ cov_ann @ w))\n        if vol &lt; 1e-12:\n            return 1e6\n        return -(ret / vol) + kappa * 0.5 * np.sum(np.abs(w - w_prev)) + 0.5 * ridge * np.sum(w**2)\n\n    result = minimize(neg_obj, x0, method=\"SLSQP\", bounds=bounds,\n                   constraints=({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},),\n                   options={\"maxiter\": 8000})\n    if (not result.success) or np.any(~np.isfinite(result.x)):\n        return None\n    return safe_normalize_weights(result.x, w_min, w_max, long_only)\n\n\ndef sharpe_from_w(mu_excess_ann, cov_ann, w):\n    w = np.asarray(w, dtype=np.float64).flatten()\n    r = float(np.dot(mu_excess_ann, w))\n    v = float(np.sqrt(max(w @ cov_ann @ w, 1e-18)))\n    return r / v if v &gt; 1e-12 else -np.inf\n\n\n\ndef get_frontier_solver(nn, ridge, kappa):\n    key = (\"frontier\", nn, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    r_target = cp.Parameter(nonneg=False)\n\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    cons.append(mu @ w &gt;= r_target)\n\n    obj = cp.Minimize(cp.quad_form(w, cp.psd_wrap(S)) + kappa * cp.norm1(w - w_prev) + 0.5 * ridge * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev, r_target)\n    return prob, w, mu, S, w_prev, r_target\n\n\ndef greedy_max_return_weight(mu, w_max):\n    mu = np.asarray(mu, dtype=np.float64).flatten()\n    n = len(mu)\n    order = np.argsort(mu)[::-1]\n    w = np.zeros(n, dtype=np.float64)\n    cap = np.inf if w_max is None else float(w_max)\n    remaining = 1.0\n    for i in order:\n        if remaining &lt;= 1e-12:\n            break\n        add = min(cap, remaining)\n        w[i] = add\n        remaining -= add\n    if remaining &gt; 1e-6:\n        return None\n    return w\n\n\ndef max_sharpe_frontier_grid_weights(mu_excess_ann, cov_ann, w_prev, rpy, grid_n=20):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n\n    w_minv = minvar_weights(cov_ann, w_prev, rpy)\n    if w_minv is None:\n        w_minv = np.ones(nn, dtype=np.float64) / nn\n\n    w_maxr = greedy_max_return_weight(mu_excess_ann, w_max)\n    if w_maxr is None:\n        return None\n\n    r_lo = float(np.dot(mu_excess_ann, w_minv))\n    r_hi = float(np.dot(mu_excess_ann, w_maxr))\n    if not np.isfinite(r_lo) or not np.isfinite(r_hi) or r_hi &lt;= r_lo + 1e-12:\n        return None\n\n    targets = np.linspace(r_lo, r_hi, int(grid_n))\n\n    prob, w_var, mu_p, S_p, wprev_p, r_p = get_frontier_solver(nn, ridge, kappa_annual(rpy))\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n\n    best_w, best_s = None, -np.inf\n    for rt in targets:\n        r_p.value = float(rt)\n        w = solve_cvx(prob, w_var)\n        if w is None:\n            continue\n        w = safe_normalize_weights(w, w_min, w_max, long_only)\n        if w is None:\n            continue\n        s = sharpe_from_w(mu_excess_ann, cov_ann, w)\n        if s &gt; best_s:\n            best_s, best_w = s, w\n\n    return best_w",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#in-sample-efficient-frontier-at-the-last-rebalance",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#in-sample-efficient-frontier-at-the-last-rebalance",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "In sample efficient frontier at the last rebalance",
    "text": "In sample efficient frontier at the last rebalance\n\nwhat “efficient frontier” means\nconsider all feasible portfolios under constraints (long-only and weight cap):\n\\[\n\\mathcal{W}=\\left\\{w:\\; \\mathbf{1}^\\top w = 1,\\; 0 \\le w_i \\le w_{\\max}\\right\\}.\n\\]\na portfolio \\(w^*\\) is mean–variance efficient if there is no other feasible portfolio \\(w \\in \\mathcal{W}\\) such that\n\\[\n\\sigma(w)\\le \\sigma(w^*) \\text{ and } r(w)\\ge r(w^*),\n\\]\nwith at least one strict inequality.\nIt means we can’t improve return without taking more risk, and we can’t reduce risk without giving up return.\nevery point in efficient frontier is a portfolio with weights \\(w^*\\) such that we don’t have any other portfolio that has higher return with the same risk or has lower risk with the same return.\nthe efficient frontier is the set of all such non-dominated portfolios.\nto trace the frontier, we solve a family of convex quadratic programs.\n\nminimum-variance anchor (left end of the frontier)\nmaximum-return feasible anchor (right end of the feasible return range)\n\n\\[\nw^{maxr}=\\arg\\max_{w\\in\\mathcal{W}} \\; \\mu^\\top w.\n\\]\nthese define a feasible return interval:\n\\[\nr_{lo}=\\mu^\\top w^{mv},\n\\qquad\nr_{hi}=\\mu^\\top w^{maxr}.\n\\]\n\n\nfrontier sweep\nfor a grid of target returns \\(r\\) between \\(r_{lo}\\) and \\(r_{hi}\\), we solve:\n\\[\nw(r)=\\arg\\min_{w\\in\\mathcal{W}} \\; w^\\top\\Sigma w\n\\quad\\text{s.t.}\\quad\n\\mu^\\top w \\ge r.\n\\]\neach solution yields a frontier point:\n\\[\n\\hat r = \\mu^\\top w(r),\n\\qquad\n\\hat\\sigma = \\sqrt{w(r)^\\top\\Sigma w(r)}.\n\\]\nin the plot, we highlight: - MinVar: \\(w^{mv}\\) (the lowest-risk feasible portfolio) - MaxSharpe: \\(w^{ms}\\) (best risk-adjusted return under the model)\n\n\nimportant interpretation note\nthis figure is a snapshot at a single rebalance date. it is “efficient” only with respect to the estimated \\((\\mu,\\Sigma)\\) used at \\(t^*\\). in out-of-sample backtests, estimation error (especially in \\(\\mu\\)) can shift the frontier, so the frontier is best used as an interpretability and diagnostics tool rather than a guarantee of future performance.\n\nlast_dt = pd.Timestamp(rebal_dates[-1])\nst = cache[last_dt]\n\ntickers = list(st[\"tickers\"])\nnn = len(tickers)\n\nmu_raw = st[\"mu_excess_ann\"]\nif isinstance(mu_raw, pd.Series):\n    mu_excess_ann = mu_raw.reindex(tickers).to_numpy(dtype=np.float64)\nelse:\n    mu_excess_ann = np.asarray(mu_raw, dtype=np.float64).reshape(-1)\n\n\ncov_map = st[\"cov_ann_map\"]\ncov_key = \"LedoitWolf\"\n\ncov_ann = np.asarray(cov_map[cov_key], dtype=np.float64)\n\nif not constraints_feasible(nn, w_min, w_max, long_only):\n    raise ValueError(\"Weight constraints are infeasible for this universe size.\")\n\nw_prev = np.ones(nn, dtype=np.float64) / nn\nw_minv = minvar_weights(cov_ann, w_prev, rpy= 0)\nw_maxr = greedy_max_return_weight(mu_excess_ann, w_max)\nr_lo = float(np.dot(mu_excess_ann, w_minv))\nr_hi = float(np.dot(mu_excess_ann, w_maxr))\n\ngrid_n = 1000\ntargets = np.linspace(r_lo, r_hi, grid_n)\n\nprob, w_var, mu_p, S_p, wprev_p, r_p = get_frontier_solver(nn, ridge=0 , kappa=0)\nmu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\nS_p.value = np.asarray(cov_ann, dtype=np.float64)\nwprev_p.value = np.asarray(w_prev, dtype=np.float64)\n\nfrontier_rows = []\nfor rt in targets:\n    r_p.value = float(rt)\n    w_sol = solve_cvx(prob, w_var)\n    if w_sol is None:\n        continue\n    w_sol = safe_normalize_weights(w_sol, w_min, w_max, long_only)\n    if w_sol is None:\n        continue\n\n    ann_ret = float(np.dot(mu_excess_ann, w_sol))\n    ann_vol = float(np.sqrt(max(w_sol @ cov_ann @ w_sol, 1e-18)))\n    frontier_rows.append({\"ann_vol\": ann_vol, \"ann_return\": ann_ret})\n\nfrontier = pd.DataFrame(frontier_rows).dropna()\n\nfrontier = frontier.sort_values(\"ann_vol\").reset_index(drop=True)\n\nw_ms_slsqp = max_sharpe_weights(mu_excess_ann, cov_ann, w_prev, rpy=0)\n\ndef summarize_portfolio(w, name):\n    if w is None:\n        return {\"model\": name, \"ann_return\": np.nan, \"ann_vol\": np.nan, \"sharpe\": np.nan}\n    w = np.asarray(w, dtype=np.float64).reshape(-1)\n    r = float(np.dot(mu_excess_ann, w))\n    v = float(np.sqrt(max(w @ cov_ann @ w, 1e-18)))\n    s = r / v if v &gt; 1e-12 else np.nan\n    return {\"model\": name, \"ann_return\": r, \"ann_vol\": v, \"sharpe\": s}\n\nmodel_ref = pd.DataFrame([\n    summarize_portfolio(w_minv, \"MinVar\"),\n    summarize_portfolio(w_ms_slsqp, \"MaxSharpe\")\n]).set_index(\"model\")\n\n\nplt.figure(figsize=(8.6, 5.2))\nplt.plot(frontier[\"ann_vol\"], frontier[\"ann_return\"], color=colors[0], lw=2.2, label=\"Efficient Frontier\")\n\nminvar_pt = model_ref.loc[\"MinVar\", [\"ann_vol\", \"ann_return\"]]\nif np.isfinite(minvar_pt[\"ann_vol\"]) and np.isfinite(minvar_pt[\"ann_return\"]):\n    plt.scatter([minvar_pt[\"ann_vol\"]], [minvar_pt[\"ann_return\"]], marker=\"o\", s=70, color=colors[2], label=\"MinVar\")\n\nmax_name = \"MaxSharpe\"\nmax_pt = model_ref.loc[max_name, [\"ann_vol\", \"ann_return\"]]\nif np.isfinite(max_pt[\"ann_vol\"]) and np.isfinite(max_pt[\"ann_return\"]):\n    plt.scatter([max_pt[\"ann_vol\"]], [max_pt[\"ann_return\"]], marker=\"*\", s=140, color=colors[1], label=\"MaxSharpe\")\n\nplt.title(f\"Efficient Frontier - Last Rebalance: {last_dt.date()} | N={nn}\")\nplt.xlabel(\"Annualized Volatility\")\nplt.ylabel(\"Annualized Expected Excess Return\")\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"Frontier points: {len(frontier)} | Return range: [{r_lo:.3%}, {r_hi:.3%}]\")\n\n\n\n\n\n\n\n\nFrontier points: 1000 | Return range: [-1.592%, 11.299%]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#trading-transaction-costs-and-real-market-simulation",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#trading-transaction-costs-and-real-market-simulation",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "9) Trading, transaction costs, and real market simulation",
    "text": "9) Trading, transaction costs, and real market simulation\nWe talked about turnover and how to add it to our optimization problem as a penalty. At a rebalance date, trading changes weights from \\(w_{t^-}\\) to \\(w_t\\). Turnover: \\[\n\\operatorname{TO}_t = \\sum_{i=1}^{N}\\lvert w_{t,i} - w_{t^-,i}\\rvert\n\\]\nIf costs are \\(c\\) in decimal per unit turnover (for example 1 bps means \\(c=0.001\\)), then cost paid is: \\[\nC_t = c\\,\\operatorname{TO}_t\\,W_{t-1}\n\\]\nwhich \\(W_{t-1}\\) is the wealth in the last date\nNet wealth right after rebalancing becomes: \\[\nW_{t-1}^{(net)} = W_{t-1} - C_t\n\\]\nThen apply the day-\\(t\\) return: \\[\nW_t = W_{t-1}^{(net)}(1 + r_{p,t})\n\\]\nThis model is simple but captures the key reality: higher turnover causes more costs and reduces long-run performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#backtest-engine",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#backtest-engine",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "10) Backtest engine",
    "text": "10) Backtest engine\nDaily drift: \\[\n\\tilde{w}_{t,i} = \\frac{w_{t-1,i}(1+r_{t,i})}{\\sum_j w_{t-1,j}(1+r_{t,j})}\n\\]\nIn each rebalance we: - compute w_pre (drifted weights in active set of selected stocks) - compute w_tar from strategy - blend, normalize, apply costs - analyze performance",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#performance-metrics-what-we-report",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#performance-metrics-what-we-report",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "11) Performance metrics (what we report)",
    "text": "11) Performance metrics (what we report)\nAssume we have daily portfolio returns \\((r_{p,t})_{t=1}^T\\) and wealth series \\(\\{W_t\\}\\).\n\n9.1 CAGR\nif \\(T\\) is the number of trading days. The compounded annual growth rate is:\n\\(\\operatorname{CAGR} = \\left(\\frac{W_T}{W_0}\\right)^{252/T} - 1\\)\n\n\n9.2 Annualized volatility\nif \\(\\sigma_d = \\operatorname{std}(r_{p,t})\\). Then:\n\\[\n\\sigma_{ann} = \\sqrt{252}\\,\\sigma_d\n\\]\n\n\n9.3 Sharpe ratio\nWe’ve already talked about this. \\(\\bar{r}_p\\) is the mean daily portfolio return. Using daily risk-free \\(r_f^{(d)}\\):\n\\[\n\\operatorname{SR} = \\frac{\\bar{r}_p - r_f^{(d)}}{\\sigma_d}\\sqrt{252}\n\\]\n\n\n9.4 Drawdown and max drawdown\nDefine running peak (the highest point in our wealth):\n\\(M_t = \\max_{s \\le t} W_s\\)\nDrawdown (how much we go down after we hit the peak):\n\\(DD_t = 1 - \\frac{W_t}{M_t}\\)\nMax drawdown:\n\\[\n\\operatorname{MaxDD} = \\max_t DD_t\n\\]\n\n\n9.5 Turnover diagnostics\nAverage turnover per rebalance:\n\\[\\overline{\\operatorname{TO}} = \\frac{1}{|\\mathcal{T}|}\\sum_{t \\in \\mathcal{T}}\\operatorname{TO}_t\n\\]\nApproximate annual turnover if rebalances occur \\(B\\) times per year:\n\\[\\operatorname{TO}_{ann} \\approx B\\,\\overline{\\operatorname{TO}}\n\\]\n\nfixed_fee = 0.0\n\n\ndef calc_drawdown(series):\n    return series / series.cummax() - 1.0\n\ndef backtest_strategy(name, cov_key):\n    all_dates = returns.loc[rebal_dates[0]:].index\n    rebal_set = set(rebal_dates)\n\n    w = pd.Series(dtype=np.float64)\n    gross_value, net_value = 1.0, 1.0\n\n    gross_values, net_values, gross_returns = [], [], []\n    weights_rebal = {}\n    turnover_list, cost_list = [], []\n    fallback_count = 0\n\n    for dt in all_dates:\n        if dt in rebal_set:\n            st = cache[dt]\n            mu_excess_ann = st[\"mu_excess_ann\"]\n            cov_ann = st[\"cov_ann_map\"][cov_key]\n            active_tickers = st[\"tickers\"]\n            nn = len(active_tickers)\n            if nn &gt;= 2:\n\n                w_pre = w.reindex(active_tickers).fillna(0.0).astype(np.float64)\n                s = float(w_pre.sum())\n                w_pre = (w_pre / s) if s &gt; 0 else pd.Series(np.ones(nn, dtype=np.float64) / nn, index=active_tickers)\n\n                if name == \"EW\":\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                elif name.startswith(\"MinVar\"):\n                    w_tar = minvar_weights(cov_ann, w_pre.values, rebal_per_year)\n                elif name.startswith(\"MV\"):\n                    w_tar = mv_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"Ridge MV\":\n                    w_tar = ridge_mv_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"MaxSharpe\":\n                    w_tar = max_sharpe_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"MaxSharpe (FrontierGrid)\":\n                    w_tar = max_sharpe_frontier_grid_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                else:\n                    w_tar = None\n\n                if w_tar is None or np.any(~np.isfinite(w_tar)):\n                    print_warn(f\"{name} failed on {dt.date()}, fallback to EW\")\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                    fallback_count += 1\n\n                w_tar = blend_weights(w_tar, w_pre.values, float(blend.get(name, 0.0)))\n                w_tar = safe_normalize_weights(w_tar, w_min, w_max, long_only)\n                if w_tar is None:\n                    print_warn(f\"{name} infeasible on {dt.date()}, fallback to EW\")\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                    fallback_count += 1\n\n                delta = w_tar - w_pre.values\n                turnover = 0.5 * np.sum(np.abs(delta))\n                cost_value = 0.0\n                cost_rate = float((cost_bps / 10000.0) * 0.5 * np.sum(np.abs(delta)))\n                cost_value = net_value * cost_rate\n                net_value = max(net_value - cost_value, 1e-12)\n                if fixed_fee &gt; 0:\n                    fee = fixed_fee * np.count_nonzero(np.abs(delta) &gt; 1e-12)\n                    net_value = max(net_value - fee, 1e-12)\n                    cost_value += fee\n\n                turnover_list.append(turnover)\n                cost_list.append(cost_value)\n\n                weights_rebal[dt] = pd.Series(w_tar.astype(np.float32), index=active_tickers)\n                w = pd.Series(w_tar, index=active_tickers, dtype=np.float64)\n\n        if w.empty:\n            port_ret = 0.0\n            w_close = pd.Series(dtype=np.float64)\n        else:\n            r_today = returns.loc[dt].reindex(w.index).fillna(0.0).astype(np.float64)\n            port_ret = float(np.dot(w.values, r_today.values))\n\n            gross_value *= (1.0 + port_ret)\n            net_value *= (1.0 + port_ret)\n\n            grossed = w.values * (1.0 + r_today.values)\n            gs = float(grossed.sum())\n            w_close = pd.Series(grossed / gs, index=w.index, dtype=np.float64) if gs &gt; 0 and np.isfinite(gs) else pd.Series(dtype=np.float64)\n\n        gross_values.append(gross_value)\n        net_values.append(net_value)\n        gross_returns.append(port_ret)\n        w = w_close\n\n    gross_values = pd.Series(gross_values, index=all_dates, name=f\"{name}_gross\")\n    net_values = pd.Series(net_values, index=all_dates, name=f\"{name}_net\")\n    gross_returns = pd.Series(gross_returns, index=all_dates, name=f\"{name}_gross_ret\")\n    net_returns = net_values.pct_change().fillna(0.0)\n\n    wdf = pd.DataFrame.from_dict(weights_rebal, orient=\"index\")\n    if not wdf.empty:\n        wdf = wdf.fillna(0.0)\n\n    return {\n        \"gross_values\": gross_values,\n        \"net_values\": net_values,\n        \"gross_returns\": gross_returns,\n        \"net_returns\": net_returns,\n        \"weights\": wdf,\n        \"turnover\": pd.Series(turnover_list, index=wdf.index) if len(wdf) else pd.Series([], dtype=float),\n        \"costs\": pd.Series(cost_list, index=wdf.index) if len(wdf) else pd.Series([], dtype=float),\n        \"fallbacks\": fallback_count,\n    }",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#strategy-dashboards",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#strategy-dashboards",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "12) Strategy dashboards",
    "text": "12) Strategy dashboards\nWe report the performance of each model and top weights and top risk contribution (to volatility) at date \\(t\\): if portfolio variance \\(\\sigma_p^2 = w^T\\Sigma w\\), and marginal risk is \\(m = \\Sigma w\\). Contribution to variance: \\(RC^{var}_i = w_i m_i\\). Contribution to volatility: \\(RC^{vol}_i = RC^{var}_i / \\sigma_p\\).\n\ndef format_date_axis(ax):\n    ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n    ax.figure.autofmt_xdate()\n\n\ndef cache_state_on_or_before(cache_dict, dt):\n    d = pd.Timestamp(dt)\n    if d in cache_dict:\n        return cache_dict[d], d\n    keys = pd.DatetimeIndex(sorted(pd.Timestamp(k) for k in cache_dict.keys()))\n    pos = int(keys.searchsorted(d, side=\"right\")) - 1\n    if pos &lt; 0:\n        return None, None\n    use_dt = pd.Timestamp(keys[pos])\n    return cache_dict[use_dt], use_dt\n\n\ndef plot_strategy_dashboard_on_axes(axes, name, res, cov_key):\n    \"\"\"\n    Draw one strategy dashboard on a provided 2x2 axes block.\n    axes must be indexable as axes[0,0], axes[0,1], axes[1,0], axes[1,1].\n    \"\"\"\n    color = strategy_colors[name]\n\n    ax = axes[0, 0]\n    if res[\"net_values\"].empty:\n        ax.text(0.5, 0.5, \"No net values\", ha=\"center\", va=\"center\")\n        ax.set_axis_off()\n    else:\n        ax.plot(res[\"net_values\"].index, res[\"net_values\"].values, color=color)\n        ax.set_title(f\"{name} - Net Equity\")\n        ax.set_xlabel(\"Date\")\n        ax.set_ylabel(\"Growth of $1\")\n        format_date_axis(ax)\n\n    ax = axes[0, 1]\n    if res[\"net_values\"].empty:\n        ax.text(0.5, 0.5, \"No net values\", ha=\"center\", va=\"center\")\n        ax.set_axis_off()\n    else:\n        dd = calc_drawdown(res[\"net_values\"])\n        ax.plot(dd.index, dd.values, color=color)\n        ax.set_title(f\"{name} - Net Drawdown\")\n        ax.set_xlabel(\"Date\")\n        ax.set_ylabel(\"Drawdown\")\n        format_date_axis(ax)\n\n    wdf = res[\"weights\"]\n    ax_w = axes[1, 0]\n    ax_r = axes[1, 1]\n\n    if wdf.empty:\n        ax_w.text(0.5, 0.5, \"No weights\", ha=\"center\", va=\"center\")\n        ax_w.set_axis_off()\n        ax_r.text(0.5, 0.5, \"No weights\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    last_dt = pd.Timestamp(wdf.index[-1])\n    w_last = wdf.loc[last_dt].astype(float)\n    w_last = w_last[w_last &gt; 0].sort_values(ascending=False)\n\n    if w_last.empty:\n        ax_w.text(0.5, 0.5, \"No positive weights\", ha=\"center\", va=\"center\")\n        ax_w.set_axis_off()\n    else:\n        topw = w_last.head(10).sort_values()\n        ax_w.barh(topw.index, topw.values, color=color)\n        ax_w.set_title(f\"{name} - Top-10 Weights ({last_dt.date()})\")\n        ax_w.set_xlabel(\"Weight\")\n\n    st, st_dt = cache_state_on_or_before(cache, last_dt)\n    if st is None:\n        ax_r.text(0.5, 0.5, \"No cache state\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    cov_map = st.get(\"cov_ann_map\", {})\n    ck = cov_key if cov_key in cov_map else {str(k).lower(): k for k in cov_map}.get(str(cov_key).lower())\n    if ck is None or ck not in cov_map:\n        ax_r.text(0.5, 0.5, \"Missing covariance\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    tickers = [str(t) for t in st.get(\"tickers\", [])]\n    cov = np.asarray(cov_map[ck], dtype=float)\n    if len(tickers) == 0 or cov.ndim != 2 or cov.shape[0] != cov.shape[1] or cov.shape[0] != len(tickers):\n        ax_r.text(0.5, 0.5, \"Covariance mismatch\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    w_vec = wdf.loc[last_dt].reindex(tickers).fillna(0.0).to_numpy(dtype=np.float64)\n    s = float(w_vec.sum())\n    if s &lt;= 1e-12:\n        ax_r.text(0.5, 0.5, \"Zero weights\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    w_vec = w_vec / s\n    Sigma_w = cov @ w_vec\n    port_var = float(w_vec @ Sigma_w)\n    port_vol = np.sqrt(max(port_var, 1e-18))\n    rc = pd.Series((w_vec * Sigma_w) / port_vol, index=tickers).replace([np.inf, -np.inf], np.nan).dropna()\n\n    if rc.empty:\n        ax_r.text(0.5, 0.5, \"No RC data\", ha=\"center\", va=\"center\")\n        ax_r.set_axis_off()\n        return\n\n    top_rc = rc.abs().sort_values(ascending=False).head(10).index\n    plot_rc = rc.loc[top_rc].sort_values()\n    dt_lbl = st_dt.date() if st_dt is not None else last_dt.date()\n    ax_r.barh(plot_rc.index, plot_rc.values, color=color)\n    ax_r.set_title(f\"{name} - Top-10 Risk Contributions ({dt_lbl})\")\n    ax_r.set_xlabel(\"Contribution to vol\")\n\n\ndef plot_strategy_dashboard(name, res, cov_key):\n    if res[\"net_values\"].empty:\n        print_warn(f\"{name}: empty results\")\n        return\n    fig, axes = plt.subplots(2, 2, figsize=(9, 6))\n    plot_strategy_dashboard_on_axes(axes, name, res, cov_key)\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#running-strategies",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#running-strategies",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "13) Running strategies",
    "text": "13) Running strategies\n\nresults = {}\n\nstrategy_order = [\n    \"EW\",\n    \"MinVar (SampleCov)\", \"MinVar (LedoitWolf)\", \"MinVar (OAS)\", \"MinVar (EWMA)\",\n    \"MV (SampleCov)\", \"MV (LedoitWolf)\", \"MV (OAS)\", \"MV (EWMA)\",\n    \"Ridge MV\",\n    \"MaxSharpe\",\n    \"MaxSharpe (FrontierGrid)\",\n]\n\nfor name in strategy_order:\n    results[name] = backtest_strategy(name, strategy_cov_key[name])\n\nstrategy_names = list(results.keys())\nprint(\"Computed strategies:\", strategy_names)\n\n\nn = len(strategy_order)\nouter_cols = 2\nouter_rows = int(np.ceil(n / outer_cols))\n\nfig = plt.figure(figsize=(outer_cols * 10, outer_rows * 7.2))\nouter = fig.add_gridspec(outer_rows, outer_cols, hspace=0.30, wspace=0.18)\n\nfor k, name in enumerate(strategy_order):\n    r, c = divmod(k, outer_cols)\n    inner = outer[r, c].subgridspec(2, 2, hspace=0.55, wspace=0.34)\n    axes_tile = np.array([\n        [fig.add_subplot(inner[0, 0]), fig.add_subplot(inner[0, 1])],\n        [fig.add_subplot(inner[1, 0]), fig.add_subplot(inner[1, 1])],\n    ], dtype=object)\n    plot_strategy_dashboard_on_axes(axes_tile, name, results[name], strategy_cov_key[name])\n\nfor k in range(n, outer_rows * outer_cols):\n    r, c = divmod(k, outer_cols)\n    ax = fig.add_subplot(outer[r, c])\n    ax.axis(\"off\")\nplt.tight_layout(rect=[0, 0, 1, 0.985])\nplt.show()\n\nComputed strategies: ['EW', 'MinVar (SampleCov)', 'MinVar (LedoitWolf)', 'MinVar (OAS)', 'MinVar (EWMA)', 'MV (SampleCov)', 'MV (LedoitWolf)', 'MV (OAS)', 'MV (EWMA)', 'Ridge MV', 'MaxSharpe', 'MaxSharpe (FrontierGrid)']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#summary-tables-and-overall-plots-style-preserved",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#summary-tables-and-overall-plots-style-preserved",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "14) summary tables and overall plots (style preserved)",
    "text": "14) summary tables and overall plots (style preserved)\n\nTrading diagnostics (turnover, costs, concentration)\nWe compute: - average turnover per rebalance - total turnover - total costs - average HHI and effective number of holdings\n\\[\nHHI_t = \\sum_i w_{t,i}^2\n\\quad,\\quad\nN_{eff} = \\frac{1}{E[HHI]}\n\\]\n\ndef calc_drawdown(series):\n    return series / series.cummax() - 1.0\n\ndef performance_metrics(net_returns, net_values):\n    years = len(net_returns) / 252.0\n    cagr = (net_values.iloc[-1] ** (1.0 / years) - 1.0) if years &gt; 0 else 0.0\n    vol = net_returns.std() * np.sqrt(252.0)\n    excess = net_returns - rf_daily\n    sharpe = (excess.mean() / net_returns.std()) * np.sqrt(252.0) if net_returns.std() &gt; 0 else np.nan\n    dd = calc_drawdown(net_values)\n    max_dd = dd.min()\n    calmar = cagr / abs(max_dd) if max_dd &lt; 0 else np.nan\n    downside = net_returns[net_returns &lt; 0]\n    sortino = (excess.mean() / downside.std()) * np.sqrt(252.0) if downside.std() &gt; 0 else np.nan\n    return cagr, vol, sharpe, max_dd, calmar, sortino\n\nmetrics_rows = []\nfor name, res in results.items():\n    metrics_rows.append([name, *performance_metrics(res[\"net_returns\"], res[\"net_values\"])])\n\nmetrics_df = pd.DataFrame(metrics_rows, columns=[\"Strategy\", \"CAGR\", \"AnnVol\", \"Sharpe\", \"MaxDD\", \"Calmar\", \"Sortino\"]).set_index(\"Strategy\")\nprint(\"Risk/Return Summary (Net)\")\ndisplay(metrics_df)\n\ntrade_rows = []\nfor name, res in results.items():\n    turnover, costs, wdf = res[\"turnover\"], res[\"costs\"], res[\"weights\"]\n    if len(wdf) &gt; 0:\n        hhi = (wdf ** 2).sum(axis=1)\n        avg_hhi = float(hhi.mean())\n        eff_n = 1.0 / avg_hhi if avg_hhi &gt; 0 else np.nan\n    else:\n        avg_hhi, eff_n = np.nan, np.nan\n    trade_rows.append([\n        name,\n        float(turnover.mean()) if len(turnover) else 0.0,\n        float(turnover.sum()) if len(turnover) else 0.0,\n        float(costs.sum()) if len(costs) else 0.0,\n        float(costs.sum() / res[\"net_values\"].iloc[-1]) if len(costs) else 0.0,\n        avg_hhi,\n        eff_n,\n    ])\n\ntrade_df = pd.DataFrame(trade_rows, columns=[\"Strategy\", \"Avg Turnover\", \"Total Turnover\", \"Total Costs\", \"Cost % Final Value\", \"Avg HHI\", \"Effective N\"]).set_index(\"Strategy\")\nprint(\"Trading & Stability Summary\")\ndisplay(trade_df)\n\nprint(\"Fallback counts per strategy:\")\nfor name in strategy_names:\n    print(f\"{name}: {results[name]['fallbacks']}\")\n\ndef plot_equity_curves(results_dict, key, title):\n    plt.figure(figsize=(12, 6))\n    for name, res in results_dict.items():\n        s = res[key]\n        plt.plot(s.index, s.values, label=name, color=strategy_colors[name])\n    plt.title(title)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Growth of $1\")\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    format_date_axis(plt.gca())\n    plt.tight_layout()\n    plt.show()\n\nplot_equity_curves(results, \"gross_values\", \"Equity Curves (Gross)\")\nplot_equity_curves(results, \"net_values\", \"Equity Curves (Net)\")\n\nplt.figure(figsize=(12, 6))\nfor name, res in results.items():\n    dd = calc_drawdown(res[\"net_values\"])\n    plt.plot(dd.index, dd.values, label=name, color=strategy_colors[name])\nplt.title(\"Drawdowns (Net)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Drawdown\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nformat_date_axis(plt.gca())\nplt.tight_layout()\nplt.show()\n\n\n\nplt.figure(figsize=(10, 6))\nfor s in metrics_df.index:\n    x = metrics_df.loc[s, \"AnnVol\"]\n    y = metrics_df.loc[s, \"CAGR\"]\n    c = strategy_colors[s]\n    plt.scatter(x, y, color=c, s=60)\n    plt.annotate(s, (x, y), fontsize=9, alpha=0.9, color=c)\nplt.title(\"Realized Risk-Return (Net): CAGR vs Annualized Volatility\")\nplt.xlabel(\"Annualized Volatility\")\nplt.ylabel(\"CAGR\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\nsh = metrics_df[\"Sharpe\"].sort_values()\nsh_colors = [strategy_colors[s] for s in sh.index]\nsh.plot(kind=\"barh\", color=sh_colors)\nplt.title(\"Realized Sharpe (Net)\")\nplt.xlabel(\"Sharpe\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nret_mat = pd.concat({k: v[\"net_returns\"] for k, v in results.items()}, axis=1).dropna(how=\"any\")\ncorr = ret_mat.corr()\n\nplt.figure(figsize=(10, 8))\nim = plt.imshow(corr.values, aspect=\"auto\")\nplt.colorbar(im, fraction=0.046, pad=0.04)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation=90, fontsize=8)\nplt.yticks(range(len(corr.index)), corr.index, fontsize=8)\nplt.title(\"Correlation of Strategy Net Returns\")\nplt.tight_layout()\nplt.show()\n\nRisk/Return Summary (Net)\n\n\n\n\n\n\n\n\n\nCAGR\nAnnVol\nSharpe\nMaxDD\nCalmar\nSortino\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\nEW\n0.146453\n0.253005\n0.511901\n-0.460750\n0.317857\n0.672864\n\n\nMinVar (SampleCov)\n0.153991\n0.160920\n0.725942\n-0.300514\n0.512424\n0.889201\n\n\nMinVar (LedoitWolf)\n0.130693\n0.161293\n0.598409\n-0.297730\n0.438966\n0.730984\n\n\nMinVar (OAS)\n0.132298\n0.161460\n0.606626\n-0.297890\n0.444118\n0.742014\n\n\nMinVar (EWMA)\n0.166994\n0.154277\n0.823995\n-0.288007\n0.579826\n1.034746\n\n\nMV (SampleCov)\n0.170766\n0.170585\n0.779640\n-0.261325\n0.653462\n1.008728\n\n\nMV (LedoitWolf)\n0.160197\n0.172680\n0.720231\n-0.260224\n0.615611\n0.923250\n\n\nMV (OAS)\n0.168002\n0.171487\n0.762955\n-0.261363\n0.642792\n0.979051\n\n\nMV (EWMA)\n0.185129\n0.172926\n0.844145\n-0.253481\n0.730344\n1.071225\n\n\nRidge MV\n0.157800\n0.171445\n0.712393\n-0.256076\n0.616224\n0.908904\n\n\nMaxSharpe\n0.213863\n0.290424\n0.681396\n-0.458022\n0.466928\n0.877095\n\n\nMaxSharpe (FrontierGrid)\n0.271910\n0.315597\n0.799863\n-0.463704\n0.586387\n1.044933\n\n\n\n\n\n\n\nTrading & Stability Summary\n\n\n\n\n\n\n\n\n\nAvg Turnover\nTotal Turnover\nTotal Costs\nCost % Final Value\nAvg HHI\nEffective N\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\nEW\n0.048705\n5.308806\n0.010946\n0.003211\n0.010000\n100.000004\n\n\nMinVar (SampleCov)\n0.024229\n2.640967\n0.004906\n0.001357\n0.098816\n10.119779\n\n\nMinVar (LedoitWolf)\n0.030512\n3.325809\n0.006660\n0.002212\n0.060997\n16.394131\n\n\nMinVar (OAS)\n0.030349\n3.308053\n0.006694\n0.002196\n0.075948\n13.166957\n\n\nMinVar (EWMA)\n0.080659\n8.791883\n0.019071\n0.004771\n0.103168\n9.692920\n\n\nMV (SampleCov)\n0.130850\n14.262650\n0.034118\n0.008292\n0.111507\n8.968033\n\n\nMV (LedoitWolf)\n0.133851\n14.589796\n0.032932\n0.008682\n0.081683\n12.242522\n\n\nMV (OAS)\n0.131115\n14.291580\n0.033921\n0.008421\n0.090959\n10.993921\n\n\nMV (EWMA)\n0.295051\n32.160563\n0.084430\n0.018393\n0.114862\n8.706068\n\n\nRidge MV\n0.125177\n13.644327\n0.030492\n0.008189\n0.064969\n15.392010\n\n\nMaxSharpe\n0.391773\n42.703308\n0.119849\n0.021059\n0.137056\n7.296314\n\n\nMaxSharpe (FrontierGrid)\n0.336119\n36.637021\n0.129177\n0.014927\n0.151004\n6.622350\n\n\n\n\n\n\n\nFallback counts per strategy:\nEW: 0\nMinVar (SampleCov): 0\nMinVar (LedoitWolf): 0\nMinVar (OAS): 0\nMinVar (EWMA): 0\nMV (SampleCov): 0\nMV (LedoitWolf): 0\nMV (OAS): 0\nMV (EWMA): 0\nRidge MV: 0\nMaxSharpe: 0\nMaxSharpe (FrontierGrid): 0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#implementation-on-hong-kong-stock-market-with-quantfinlab",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#implementation-on-hong-kong-stock-market-with-quantfinlab",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "implementation on Hong Kong stock market with quantfinlab",
    "text": "implementation on Hong Kong stock market with quantfinlab\nthe data used in this part can be downloaded from here (Stooq HKEX daily market data)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport quantfinlab.portfolio as pf\nimport quantfinlab.plots as pl\nimport quantfinlab.risk as rk\nfrom quantfinlab import PortfolioState\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nrf_annual = 0.04\nrf_daily = (1.0 + rf_annual) ** (1.0 / 252.0) - 1.0\n\n\n\nraw = pd.read_csv(\"../data/hkex_selected_close_volume.csv\", header=[0, 1], low_memory=False)\nraw.columns = pd.MultiIndex.from_tuples([(str(a).strip(), str(b).strip()) for a, b in raw.columns])\n\ndate_cols = [c for c in raw.columns if c[0].lower() == \"date\"]\nclose_cols = [c for c in raw.columns if c[1].lower() == \"close\"]\nvolume_cols = [c for c in raw.columns if c[1].lower() == \"volume\"]\n\n\ndates = pd.to_datetime(raw[date_cols[0]], errors=\"coerce\")\n\nclose_prices = raw.loc[:, close_cols].copy()\nvolumes = raw.loc[:, volume_cols].copy()\nclose_prices.columns = [str(c[0]).strip() for c in close_cols]\nvolumes.columns = [str(c[0]).strip() for c in volume_cols]\n\nif close_prices.columns.duplicated().any():\n    close_prices = close_prices.T.groupby(level=0).last().T\nif volumes.columns.duplicated().any():\n    volumes = volumes.T.groupby(level=0).last().T\n\nclose_prices.index = dates\nvolumes.index = dates\n\nclose_prices = (\n    close_prices.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n    .astype(np.float32))\n\nvolumes = (\n    volumes.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n    .astype(np.float32))\n\nclose_prices = close_prices.where(close_prices &gt; 0)\nvolumes = volumes.where(volumes &gt;= 0)\n\nclose_prices = close_prices[~close_prices.index.isna()].sort_index()\nvolumes = volumes[~volumes.index.isna()].sort_index()\n\nif close_prices.index.has_duplicates:\n    close_prices = close_prices[~close_prices.index.duplicated(keep=\"last\")]\nif volumes.index.has_duplicates:\n    volumes = volumes[~volumes.index.duplicated(keep=\"last\")]\n\nstart = pd.Timestamp(\"2016-01-01\")\nclose_prices = close_prices.loc[close_prices.index &gt;= start]\nvolumes = volumes.loc[volumes.index &gt;= start]\n\nidx = close_prices.index.intersection(volumes.index)\ncols = close_prices.columns.intersection(volumes.columns)\nclose_prices = close_prices.loc[idx, cols]\nvolumes = volumes.loc[idx, cols]\n\nvalid_cols = close_prices.notna().any(axis=0) & volumes.notna().any(axis=0)\nclose_prices = close_prices.loc[:, valid_cols]\nvolumes = volumes.loc[:, valid_cols]\n\nprices = close_prices.copy()\n\n\nprint(\"close_prices:\", close_prices.shape, \"| volumes:\", volumes.shape)\nprint(\"Date range:\", close_prices.index.min().date(), \"to\", close_prices.index.max().date())\n\n\nreturns = pf.prices_to_returns(prices)\nrebal_dates = pf.make_rebalance_dates(returns.index, freq=\"ME\", min_history_days=252)\n\n\ncache: dict[pd.Timestamp, PortfolioState] = {}\n\n\ndef build_state(dt: pd.Timestamp) -&gt; PortfolioState | None:\n    tickers, avg_dv = pf.select_liquid_universe(dt, close_prices=prices, volumes=volumes, top_n=100,\n                                                liq_lookback=252, min_listing_days=252,min_obs=252,)\n    if dt not in prices.index:\n        return None\n\n    pos = prices.index.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n\n    if pos &lt; 252:\n        return None\n\n    close_for_model = prices.iloc[pos - 252 : pos][tickers]\n    if close_for_model.shape[0] &lt; 252:\n        return None\n\n    window = close_for_model.pct_change(fill_method=None).iloc[1:]\n    window = window.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n\n    if window.shape[0] &lt; 251 or window.shape[1] &lt; 2:\n        return None\n\n    mu_excess_ann = pf.mu_momentum(window, mode=\"6-1\", rf=rf_annual, \n                                   target_sharpe=0.80, mu_cap=0.30, winsor=0.05, \n                                   zscore=True,return_series=True,)\n\n    cov_ann_map = {\n        \"sample\": pf.cov_estimate(window, method=\"samplecov\", psd=True, ridge=1e-10),\n        \"lw\": pf.cov_estimate(window, method=\"ledoitwolf\", psd=True, ridge=1e-10),\n        \"oas\": pf.cov_estimate(window, method=\"oas\", psd=True, ridge=1e-10),\n        \"ewma\": pf.cov_estimate(window, method=\"ewma\", ewma_lambda=0.94, psd=True, ridge=1e-10)\n    }\n\n    return PortfolioState(\n        tickers=list(window.columns),\n        mu_excess_ann=mu_excess_ann.reindex(window.columns).astype(float),\n        cov_ann_map=cov_ann_map,\n        avg_dollar_volume=avg_dv.reindex(window.columns).astype(float))\n\n\nfor dt in rebal_dates:\n    st = build_state(pd.Timestamp(dt))\n    if st is not None:\n        cache[pd.Timestamp(dt)] = st\n\nrebal_dates = [pd.Timestamp(d) for d in rebal_dates if pd.Timestamp(d) in cache]\nif len(rebal_dates) == 0:\n    raise ValueError(\"No valid rebalance dates after state construction.\")\n\nprint(f\"usable rebalance dates: {len(rebal_dates)}\")\nprint(f\"avg universe size: {np.mean([len(cache[d].tickers) for d in rebal_dates]):.1f}\")\n\n\ndef ew(dt, st, w_prev):\n    return pf.weights_equal(st[\"tickers\"], w_min=0.0, w_max=0.20, long_only=True)\n\n\ndef make_minvar(cov_key: str):\n    def _fn(dt, st, w_prev):\n        return pf.weights_minvar(\n            cov_ann=st[\"cov_ann_map\"][cov_key], w_prev=w_prev,\n            w_min=0.0, w_max=0.25, long_only=True,\n            turnover_penalty_bps=10.0,solver_order=[\"osqp\", \"ecos\", \"scs\"])\n    return _fn\n\n\ndef make_mv(cov_key: str):\n    def _fn(dt, st, w_prev):\n        return pf.weights_mv(\n            mu_excess_ann=st[\"mu_excess_ann\"].values,\n            cov_ann=st[\"cov_ann_map\"][cov_key], w_prev=w_prev,\n            mv_lambda=4.0, kappa_target_annual=0.20, w_min=0.0,\n            w_max=0.25, long_only=True, turnover_penalty_bps=10.0,\n            solver_order=[\"osqp\", \"ecos\", \"scs\"])\n    return _fn\n\n\ndef ridge_mv_lw(dt, st, w_prev):\n    return pf.weights_ridge_mv(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"], w_prev=w_prev,\n        ridge=1e-4, mv_lambda=6.0, kappa_target_annual=0.30,\n        w_min=0.0, w_max=0.25, long_only=True,\n        turnover_penalty_bps=10.0,\n        solver_order=[\"osqp\", \"ecos\", \"scs\"])\n\n\ndef maxsharpe_slsqp_lw(dt, st, w_prev):\n    return pf.weights_maxsharpe_slsqp(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"], w_prev=w_prev,\n        w_min=0.0, w_max=0.25, long_only=True,\n        turnover_penalty_bps=10.0, kappa_target_annual=0.30)\n\n\n\ndef maxsharpe_frontier_lw(dt, st, w_prev):\n    return pf.weights_maxsharpe_frontier_grid(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"], w_prev=w_prev,\n        grid_n=25, w_min=0.0, w_max=0.25, long_only=True,\n        turnover_penalty_bps=10.0,solver_order=[\"osqp\", \"ecos\", \"scs\"])\n\n\nstrategy_fns = {\n    \"ew\": ew,\n    \"minvar_sample\": make_minvar(\"sample\"),\n    \"minvar_lw\": make_minvar(\"lw\"),\n    \"minvar_oas\": make_minvar(\"oas\"),\n    \"minvar_ewma\": make_minvar(\"ewma\"),\n    \"mv_sample\": make_mv(\"sample\"),\n    \"mv_lw\": make_mv(\"lw\"),\n    \"mv_oas\": make_mv(\"oas\"),\n    \"mv_ewma\": make_mv(\"ewma\"),\n    \"ridge_mv\": ridge_mv_lw,\n    \"maxsharpe_slsqp\": maxsharpe_slsqp_lw,\n    \"maxsharpe_frontier\": maxsharpe_frontier_lw,\n}\n\ncov_key_for_rc = {\n    \"ew\": \"lw\",\n    \"minvar_sample\": \"sample\",\n    \"minvar_lw\": \"lw\",\n    \"minvar_oas\": \"oas\",\n    \"minvar_ewma\": \"ewma\",\n    \"mv_sample\": \"sample\",\n    \"mv_lw\": \"lw\",\n    \"mv_oas\": \"oas\",\n    \"mv_ewma\": \"ewma\",\n    \"ridge_mv\": \"lw\",\n    \"maxsharpe_slsqp\": \"lw\",\n    \"maxsharpe_frontier\": \"lw\",\n}\n\n\nresults = {\n    name: pf.backtest(\n        returns=returns, rebal_dates=rebal_dates,\n        cache=cache, weight_fn=fn,\n        cost_bps=10.0,rf_daily=rf_daily)\n    for name, fn in strategy_fns.items()}\n\nprint(f\"computed strategies: {len(results)}\")\nprint(sorted(results.keys()))\n\n\nbest_name, sharpes = pf.best_strategy_by_sharpe(results, rf_daily=rf_daily, annualization=252.0)\n\n\ncolors = pl.make_color_map(results.keys(), pl.LAB_COLORS)\nbest_color = colors[best_name]\n\nfig, axes = plt.subplots(4, 2, figsize=(16, 26))\n\npl.plot_net_equity(axes[0, 0], results[best_name], name=best_name, color=best_color)\npl.plot_drawdown(axes[0, 1], results[best_name], name=best_name, color=best_color)\npl.plot_top_weights(axes[1, 0], results[best_name], name=best_name, color=best_color, k=10)\nvol_rc_tbl, _, _ = rk.attribution_tables(\n    {\n        best_name: {\n            \"backtest\": results[best_name],\n            \"state_cache\": cache,\n            \"cov_key\": cov_key_for_rc[best_name],\n        }\n    },\n    es_alpha=0.05,\n    top_k=10,\n)\npl.plot_top_contrib(\n    axes[1, 1],\n    vol_rc_tbl.loc[best_name],\n    title=f\"{best_name} - Top 10 Risk Contributions\",\n    k=10,\n)\nfor _p in axes[1, 1].patches:\n    _p.set_color(best_color)\naxes[1, 1].set_xlabel(\"Contribution to volatility\")\n\npl.plot_net_equity_compare(axes[2, 0], results, colors=colors, title=\"Net Equity (Comparison)\")\npl.plot_drawdown_compare(axes[2, 1], results, colors=colors, title=\"Drawdown (Comparison)\")\npl.plot_risk_return_scatter(\n    axes[3, 0],\n    results,\n    rf_daily=rf_daily,\n    annualization=252.0,\n    colors=colors,\n    title=\"Realized Risk-Return (Net)\",\n)\npl.plot_sharpe_compare(\n    axes[3, 1],\n    results,\n    rf_daily=rf_daily,\n    annualization=252.0,\n    colors=colors,\n    title=\"Realized Sharpe (Net)\",\n)\nfig.suptitle(\"Project 02 - Portfolio optimization with MeanVar, MinVar and MaxSharpe models (Hong Kong stock market Data)\", y=1.02)\nplt.tight_layout()\nplt.show()\n\nmetrics_df, trade_df = pf.summarize_results(results, rf_daily=rf_daily, annualization=252.0)\n\nprint(\"Risk/Return Summary (Net)\")\ndisplay(metrics_df)\n\nprint(\"Trading & Stability Summary\")\ndisplay(trade_df)\n\nprint(f\"best sharpe: {best_name} | sharpe: {sharpes[best_name]:.4f}\")\n\nclose_prices: (2478, 290) | volumes: (2478, 290)\nDate range: 2016-01-04 to 2026-01-28\nusable rebalance dates: 109\navg universe size: 100.0\ncomputed strategies: 12\n['ew', 'maxsharpe_frontier', 'maxsharpe_slsqp', 'minvar_ewma', 'minvar_lw', 'minvar_oas', 'minvar_sample', 'mv_ewma', 'mv_lw', 'mv_oas', 'mv_sample', 'ridge_mv']\n\n\n\n\n\n\n\n\n\nRisk/Return Summary (Net)\n\n\n\n\n\n\n\n\n\nCAGR\nAnnVol\nSharpe\nMaxDD\nCalmar\nSortino\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\new\n0.076280\n0.213572\n0.269004\n-0.400074\n0.190664\n0.366343\n\n\nmaxsharpe_frontier\n0.173174\n0.253011\n0.602840\n-0.396645\n0.436597\n0.763618\n\n\nmaxsharpe_slsqp\n0.137410\n0.243979\n0.490898\n-0.399129\n0.344275\n0.657423\n\n\nminvar_ewma\n0.080919\n0.132465\n0.356138\n-0.270858\n0.298753\n0.445874\n\n\nminvar_lw\n0.067458\n0.126572\n0.267248\n-0.320084\n0.210751\n0.343290\n\n\nminvar_oas\n0.068393\n0.126131\n0.274732\n-0.320297\n0.213530\n0.352852\n\n\nminvar_sample\n0.070466\n0.125965\n0.290240\n-0.315555\n0.223307\n0.373151\n\n\nmv_ewma\n0.047504\n0.155153\n0.125856\n-0.432365\n0.109869\n0.166299\n\n\nmv_lw\n0.059472\n0.174422\n0.195321\n-0.350070\n0.169886\n0.265576\n\n\nmv_oas\n0.059888\n0.172727\n0.197770\n-0.349493\n0.171358\n0.268584\n\n\nmv_sample\n0.055860\n0.169960\n0.175636\n-0.351519\n0.158910\n0.238167\n\n\nridge_mv\n0.058862\n0.176880\n0.191773\n-0.343360\n0.171428\n0.261115\n\n\n\n\n\n\n\nTrading & Stability Summary\n\n\n\n\n\n\n\n\n\nAvg Turnover\nTotal Turnover\nTotal Costs\nCost % Final Value\nAvg HHI\nEffective N\nFallbacks\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\n\new\n0.042396\n4.621129\n0.006054\n0.003173\n0.010000\n100.000000\n0\n\n\nmaxsharpe_frontier\n0.461996\n50.357586\n0.127701\n0.031370\n0.151092\n6.618471\n0\n\n\nmaxsharpe_slsqp\n0.068841\n7.503668\n0.011999\n0.003870\n0.130483\n7.663833\n0\n\n\nminvar_ewma\n0.441603\n48.134699\n0.067150\n0.033886\n0.137801\n7.256842\n0\n\n\nminvar_lw\n0.106463\n11.604440\n0.014779\n0.008326\n0.094543\n10.577203\n0\n\n\nminvar_oas\n0.108628\n11.840467\n0.015143\n0.008466\n0.108075\n9.252864\n0\n\n\nminvar_sample\n0.115967\n12.640403\n0.016355\n0.008989\n0.130010\n7.691693\n0\n\n\nmv_ewma\n0.020636\n2.249277\n0.002477\n0.001648\n0.056364\n17.741750\n0\n\n\nmv_lw\n0.005375\n0.585831\n0.000751\n0.000452\n0.031996\n31.254348\n0\n\n\nmv_oas\n0.005633\n0.613968\n0.000785\n0.000471\n0.033847\n29.544465\n0\n\n\nmv_sample\n0.005587\n0.608993\n0.000757\n0.000469\n0.035177\n28.427916\n0\n\n\nridge_mv\n0.003899\n0.425024\n0.000549\n0.000332\n0.030416\n32.877851\n0\n\n\n\n\n\n\n\nbest sharpe: maxsharpe_frontier | sharpe: 0.6028",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html",
    "title": "3. risk analysis and CAPM",
    "section": "",
    "text": "sign conventions\nIn this notebook we build a risk-focused report for a small set of “objects” (can be assets, strategies, or portfolios). the goal is not just to compute numbers, but to make the math and interpretation transparent so we can understand what each metric means and how it is calculated and how we can interpret from them.\nin this notebook we produce\nmany risk measures are easier to read as positive “loss magnitudes”. for example, if the 5% quantile of returns is negative, we report:\nso bigger vaR/ES means worse tail risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#sign-conventions",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#sign-conventions",
    "title": "3. risk analysis and CAPM",
    "section": "",
    "text": "\\(\\text{vaR}_{5\\%} = -q_{0.05}(r)\\) (positive number)\n\\(\\text{ES}_{5\\%} = -\\mathbb{e}[r \\mid r \\le q_{0.05}(r)]\\) (positive number)\n\n\n\nImports and plotting style\n\nimport warnings\nfrom pathlib import Path as path\nfrom datetime import datetime\nfrom statistics import NormalDist as normaldist\nimport textwrap\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_pdf import PdfPages as pdfpages\nfrom cycler import cycler\nfrom scipy.stats import chi2\nfrom sklearn.linear_model import LinearRegression\nfrom IPython.display import display\n\nimport sys\nsys.path.append(str(path(\"..\").resolve()))\nimport quantfinlab.portfolio as pf\n\nwarnings.filterwarnings(\"ignore\")\n\n\npalette = [\n    \"#069af3\", \"#fe420f\", \"#00008b\", \"#800080\",\n    \"#008080\", \"#7bc8f6\", \"#0072b2\", \"#04d8b2\",\n    \"#cc79a7\", \"#ff8072\", \"#9614fa\", \"#dc143c\",\n]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=palette)\nplt.rcParams.update({\n    \"figure.dpi\": 200,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 11,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\nann = 252\nrf_annual = 0.04\nrf_daily = (1 + rf_annual) ** (1 / ann) - 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#data-and-returns",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#data-and-returns",
    "title": "3. risk analysis and CAPM",
    "section": "1) data and returns",
    "text": "1) data and returns\n\nwhat we need as input\nthe report needs aligned daily returns for each object:\n\na date index \\(t = 1, 2, \\dots, T\\)\nfor each object \\(j\\), a series \\(\\{r_{j,t}\\}\\)\n\nif you start from prices \\(p_t\\), we use simple returns:\n\\[\nr_t = \\frac{p_t}{p_{t-1}} - 1\n\\]\nin this notebook we keep everything in simple returns because: - the nav compounding is literally \\(\\prod (1+r)\\), - most risk metrics (vaR/ES on daily returns) are commonly shown in simple-return units.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#load-data-and-compute-returns",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#load-data-and-compute-returns",
    "title": "3. risk analysis and CAPM",
    "section": "Load data and compute returns",
    "text": "Load data and compute returns\nthe data used in this project can be downloaded from here (Stooq US (nasdaq) daily market data)\n\ndf = pd.read_parquet(\"../data/nasdaq_all_close_volume.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndcol = \"date\"\ndf = df.dropna(subset=[dcol]).sort_values(dcol)\n\nclose_map, vol_map = {}, {}\nfor c in df.columns:\n    c_str = str(c)\n    if c_str.lower() == dcol.lower() or \"__\" not in c_str:\n        continue\n    t, f = c_str.rsplit(\"__\", 1)\n    f = f.lower()\n    if f == \"close\":\n        close_map[t] = c\n    elif f == \"volume\":\n        vol_map[t] = c\n\ntickers_all = sorted(set(close_map).intersection(vol_map))\n\nclose_prices = df[[close_map[t] for t in tickers_all]].copy()\nvolumes = df[[vol_map[t] for t in tickers_all]].copy()\nclose_prices.columns = tickers_all\nvolumes.columns = tickers_all\nclose_prices.index = pd.to_datetime(df[dcol].values)\nvolumes.index = pd.to_datetime(df[dcol].values)\n\nclose_prices = close_prices.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\nvolumes = volumes.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n\nstart = pd.Timestamp(\"2016-01-01\")\nclose_prices = close_prices.loc[close_prices.index &gt;= start]\nvolumes = volumes.loc[volumes.index &gt;= start]\n\nidx = close_prices.index.intersection(volumes.index)\ncols = close_prices.columns.intersection(volumes.columns)\nclose_prices = close_prices.loc[idx, cols]\nvolumes = volumes.loc[idx, cols]\n\nreturns = pf.prices_to_returns(close_prices)\n\n\nfirst_date = pd.concat([close_prices.apply(pd.Series.first_valid_index),\n                         volumes.apply(pd.Series.first_valid_index)],axis=1,).max(axis=1)\n\n\nspy = pd.read_csv(\"../data/spy_yfinance.csv\")\nspy[\"date\"] = pd.to_datetime(spy[\"Date\"], errors=\"coerce\") if \"date\" in [str(c).lower() for c in spy.columns] else pd.to_datetime(spy[\"Date\"], errors=\"coerce\")\ndcol_spy = \"date\" if \"date\" in spy.columns else \"Date\"\nspy = spy.dropna(subset=[dcol_spy]).sort_values(dcol_spy).set_index(dcol_spy)\n\nif \"Adj Close\" in spy.columns:\n    spy_px = pd.to_numeric(spy[\"Adj Close\"], errors=\"coerce\")\nelse:\n    raise ValueError(\"spy_yfinance.csv missing adj close column\")\n\nspy_px = spy_px.loc[spy_px.index &gt;= start]\nmarket_ret = spy_px.pct_change().replace([np.inf, -np.inf], np.nan)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#rebalancing-universe-selection-and-strategies",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#rebalancing-universe-selection-and-strategies",
    "title": "3. risk analysis and CAPM",
    "section": "2) rebalancing, universe selection, and strategies",
    "text": "2) rebalancing, universe selection, and strategies\nIn the last project we imported the same data from nasdaq and filtered the most liquid stocks for each month from 2016 to 2026 and implemented MeanVariance, MinVariance and MaxSharpe models with monthly rebalancing and backtested and compared them under real market conditions.\nfor better understanding please read the last project (2. Portfolio Optimization with Mean–Variance Models)\n\nrebalancing logic (no look-ahead)\na clean backtest timeline is:\n\nat the start of day \\(t\\), if \\(t\\) is a rebalance date, compute target weights \\(w_t\\) using information up to \\(t-1\\)\n\napply transaction costs/turnover if needed\n\nhold weights through the day and apply realized return \\(r_{p,t+1}\\) next\n\ntransaction cost proxy (simple linear model):\n\\[\n\\text{tc}_t = c \\sum_i |w_{i,t} - w_{i,t-1}|\n\\]\nwhere \\(c\\) is a per-unit turnover cost.\n\n\nstrategies used here (high level)\nIn this notebook we don’t repeat the code in the last notebook and we use quantfinlab library for creating the needed strategies. for the first implementation we just use two of the best strategies from the last notebook. MV_ewma and MaxSharpe_frontier. we also use two of the stocks in our dataset for showing different results in different types of objects. We use NVIDIA and Apple. and for CAPM analysis we use SPY as our benchmmark.\nif you later add more objects, the report works as long as: - each object is a daily return series aligned to the same date index - object names are consistent (for labeling)\n\nrebal_dates = pf.make_rebalance_dates(returns.index, freq=\"ME\", min_history_days=252)\n\ncache = {}\n\nfor dt in rebal_dates:\n    tickers, adv = pf.select_liquid_universe(dt, close_prices=close_prices,\n                                             volumes=volumes, top_n=100,\n                                             liq_lookback=252, min_listing_days=252,\n                                             min_obs=252, first_date=first_date,)\n    if len(tickers) &lt; 2:\n        continue\n\n    pos = returns.index.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n    if pos &lt; 252:\n        continue\n\n    window = (returns[tickers].iloc[pos - 252:pos]\n        .dropna(axis=0, how=\"any\"))\n    if window.shape[0] &lt; 170 or window.shape[1] &lt; 2:\n        continue\n\n    tickers = window.columns.tolist()\n    cov_ewma = pf.cov_estimate(window, method=\"ewma\", annualization=ann)\n    cov_lw = pf.cov_estimate(window, method=\"ledoitwolf\", annualization=ann)\n\n    mu = pf.mu_momentum(window, mode=\"6-1\", rf=rf_annual,\n                        cov_for_scaling=cov_lw, target_sharpe=0.80,\n                        mu_cap=0.30)\n\n    cache[dt] = {\n        \"tickers\": tickers,\n        \"mu_excess_ann\": np.asarray(mu, dtype=float),\n        \"cov_ann_map\": {\"ewma\": cov_ewma, \"ledoitwolf\": cov_lw},\n        \"window\": window,\n    }\n\nrebal_dates = pd.DatetimeIndex([d for d in rebal_dates if d in cache])\n\ndef mv_weight_fn(dt, state, w_prev):\n    return pf.weights_mv(mu_excess_ann=state[\"mu_excess_ann\"],\n                         cov_ann=state[\"cov_ann_map\"][\"ewma\"],\n                         w_prev=w_prev, w_max=0.25, long_only=True,\n                         turnover_penalty_bps=10, ridge=1e-8)\n\ndef maxsharpe_weight_fn(dt, state, w_prev):\n    return pf.weights_maxsharpe_frontier_grid(\n        mu_excess_ann=state[\"mu_excess_ann\"],\n        cov_ann=state[\"cov_ann_map\"][\"ledoitwolf\"],\n        w_prev=w_prev,grid_n=25,\n        w_max=0.25, long_only=True,\n        turnover_penalty_bps=10, ridge=1e-8,)\n\n\nres_mv = pf.backtest(returns, rebal_dates, cache, \n                     mv_weight_fn, cost_bps=10,\n                     fallback=\"equal\", w_max=0.25,\n                     long_only=True, rf_daily=rf_daily)\n\nres_mx = pf.backtest(returns, rebal_dates,\n                     cache, maxsharpe_weight_fn,\n                     cost_bps=10, fallback=\"equal\", \n                     w_max=0.25, long_only=True, rf_daily=rf_daily)\n\n\nbase_idx = returns.index.intersection(res_mv.net_returns.index).intersection(res_mx.net_returns.index)\n\n\nnvda_ret = returns[\"NVDA\"].reindex(base_idx).fillna(0.0) \naapl_ret = returns[\"AAPL\"].reindex(base_idx).fillna(0.0)\n\nmv_ret = res_mv.net_returns.reindex(base_idx).fillna(0.0)\nmx_ret = res_mx.net_returns.reindex(base_idx).fillna(0.0)\n\nmarket_ret = market_ret.reindex(base_idx).fillna(0.0)\n\nobj = {\n    \"nvda\": nvda_ret,\n    \"aapl\": aapl_ret,\n    \"mv_ewma\": mv_ret,\n    \"maxsharpe_frontier\": mx_ret,\n}\n\nobj_colors = {\n    \"nvda\": palette[0],\n    \"aapl\": palette[1],\n    \"mv_ewma\": palette[2],\n    \"maxsharpe_frontier\": palette[3],\n}\n\nprint(\"analysis objects:\", list(obj.keys()))\nprint(\"date range:\", base_idx.min().date(), \"to\", base_idx.max().date(), \", n:\", len(base_idx))\n\nanalysis objects: ['nvda', 'aapl', 'mv_ewma', 'maxsharpe_frontier']\ndate range: 2017-01-31 to 2026-01-28 , n: 2261",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#core-risk-metrics",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#core-risk-metrics",
    "title": "3. risk analysis and CAPM",
    "section": "3) core risk metrics",
    "text": "3) core risk metrics\nIn this section we build tables that summarize each object using only its own return series (We had this in the project 2 too).\n\n3.1 nav (cumulative wealth)\nGiven daily returns \\(r_t\\), if returns are computed simple, we define nav as:\n\\[\n\\text{nav}_t = \\prod_{u \\le t} (1 + r_u)\n\\]\nin code, this is a cumulative product of \\((1+r)\\).\n\n\n3.2 annualized return\nfrom nav, the total growth factor is \\(\\text{nav}_T\\). to annualize over \\(T\\) trading days (We assume 252 trading days in one year):\n\\[\nr^{ann} = \\text{nav}_T^{252/T} - 1\n\\]\nthis assumes the sample growth rate continues at the same pace (a standard convention).\n\n\n3.3 annualized volatility\ndaily volatility is the sample standard deviation:\n\\[\n\\hat\\sigma = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^T (r_t - \\bar r)^2}\n\\]\nannualized volatility is:\n\\[\n\\hat\\sigma^{ann} = \\hat\\sigma\\sqrt{252}\n\\]\n\n\n3.4 sharpe ratio\nFor sharpe we use excess returns (the returns compared to risk free rate returns) and Volatility for comparing excess-return risk-adjusted performance :\n\\[\n\\text{sharpe} = \\frac{\\bar r^{ex}}{\\hat\\sigma}\\sqrt{252}\n\\quad\\text{where}\\quad\n\\bar r^{ex} = \\frac{1}{T}\\sum_{t=1}^T (r_t - r_{f,t})\n\\]\nWith sharpe we can tell how many units of excess return we gain per unit of volatility.\n\n\n3.5 sortino ratio (downside-focused)\nFor sortino we replace total volatility with downside deviation. We define downside returns relative to a target \\(\\tau\\) (often \\(0\\) or \\(r_f\\)). So we set returns higher than risk free rate as 0 to analyze the lowest returns:\n\\[\nd_t = \\min(0, r_t - \\tau)\n\\]\ndownside deviation:\n\\[\n\\sigma_d = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^T d_t^2}\n\\]\nsortino:\n\\[\n\\text{sortino} = \\frac{\\bar r - \\tau}{\\sigma_d}\\sqrt{252}\n\\]\n\ndef nav_series(r):\n    r = pd.Series(r).fillna(0.0)\n    return (1 + r).cumprod()\n\ndef sortino(r):\n    x = pd.Series(r).dropna()\n    ex = x - rf_daily\n    dn = np.minimum(ex, 0)\n    den = np.sqrt((dn ** 2).mean())\n    return float((ex.mean() / den) * np.sqrt(ann)) if den &gt; 1e-12 else np.nan\n\nperf_rows = []\n\nfor name, r in obj.items():\n    x = pd.Series(r).dropna()\n\n    nav = nav_series(x)\n    ann_return = float(nav.iloc[-1] ** (ann / len(x)) - 1) if len(x) else np.nan\n\n    daily_mean = float(x.mean())\n    daily_vol = float(x.std(ddof=1))\n    ann_vol = daily_vol * np.sqrt(ann) if daily_vol &gt; 1e-12 else np.nan\n\n    sharpe = ((daily_mean - rf_daily) / daily_vol * np.sqrt(ann)) if daily_vol &gt; 1e-12 else np.nan\n    sortino_ratio = sortino(x)\n\n    perf_rows.append({\n        \"object\": name,\n        \"ann_return\": ann_return,\n        \"ann_vol\": ann_vol,\n        \"sharpe\": float(sharpe) if np.isfinite(sharpe) else np.nan,\n        \"sortino\": float(sortino_ratio) if np.isfinite(sortino_ratio) else np.nan,\n    })\n\nperf_tbl = pd.DataFrame(perf_rows).set_index(\"object\").sort_index()\n\n\ndisplay(perf_tbl.round(4))\n\n\n\n\n\n\n\n\nann_return\nann_vol\nsharpe\nsortino\n\n\nobject\n\n\n\n\n\n\n\n\naapl\n0.2798\n0.2967\n0.8478\n1.2478\n\n\nmaxsharpe_frontier\n0.2084\n0.2951\n0.6565\n0.9390\n\n\nmv_ewma\n0.1688\n0.1717\n0.7661\n1.0921\n\n\nnvda\n0.6074\n0.5016\n1.1189\n1.6765\n\n\n\n\n\n\n\nAs we can see nvidia has so much more annual returns than other objects but the annual volatility is insanely more. even with more volatility, Apple and Nvidia still have more sharpe and sortino ratio, but if we care about risk, it’s even obvious from the first metric (annual vol) that our diversification reduced the volatility succesfully. And in 2016 we wouldn’t know in 2026 nvidia would grow this much and give this much return. and even if we knew we probably wouldn’t trust this much volatility and hold it until now. the best thing we could’ve done was make a portfolio of the top 100 stocks in that time and update it each month. we now get to other metrics for comparing the risk of these 4 objects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#rolling-volatility",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#rolling-volatility",
    "title": "3. risk analysis and CAPM",
    "section": "4) rolling volatility",
    "text": "4) rolling volatility\nvolatility is not constant. a single full-sample \\(\\sigma\\) hides regime changes. with rolling volatility we can analyze the volatility overtime and see different volatility of an asset in different times.\n\nrolling statistics\na rolling volatility over window length \\(w\\) (for example 60 days) is:\n\\[\n\\hat\\sigma_{t,w} = \\sqrt{\\frac{1}{w-1}\\sum_{u=t-w+1}^t (r_u - \\bar r_{t,w})^2}\n\\]\nand annualized rolling vol is:\n\\[\n\\hat\\sigma_{t,w}^{ann} = \\hat\\sigma_{t,w}\\sqrt{252}\n\\]\nwe plot multiple windows (e.g., 20/60/252 days) because: - short windows react quickly and are more noisy (good for risk control) - long windows are smoother (good for long-horizon intuition)\n\nwindows = [20, 60, 252]\n\nfig, axes = plt.subplots(2, 2, figsize=(11, 7), sharex=True, sharey=True)\naxes = axes.ravel()\n\nfor i, (name, r) in enumerate(obj.items()):\n    ax = axes[i]\n    x = pd.Series(r).dropna()\n    for w in windows:\n        rv = x.rolling(w).std(ddof=1) * np.sqrt(ann)\n        ax.plot(rv.index, rv.values, lw=1.5, label=f\"{w}d\")\n\n    ax.set_title(f\"rolling vol — {name}\")\n    ax.set_ylabel(\"ann vol\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs we can see all of our objects have been more volatile in 2020 due to Covid crash. but the difference is that our two stocks have experienced more volatility in crash times and our diversified portfolios were able to manage the volatility in those times better and had lower effect from crashes. And as we can see Nvidia has the most noise and movement and high volatility overtime and our MeanVariance model with ewma covariance managed to control volatility overtime better than all the other objects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#distribution-shape-and-tail-diagnostics",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#distribution-shape-and-tail-diagnostics",
    "title": "3. risk analysis and CAPM",
    "section": "4) distribution shape and tail diagnostics",
    "text": "4) distribution shape and tail diagnostics\nperformance ratios (sharpe/sortino) do not tell you what the return distribution looks like. in the metrics that we analyzed we only work with variance and mean. not the real shape and behavior of distribution. two strategies can have the same sharpe but very different crash behavior.\n\n4.1 skewness\nskewness measures asymmetry. using centered moments:\n\\[\n\\text{skew} = \\frac{\\mathbb{e}[(r-\\mu)^3]}{\\sigma^3}\n\\]\n\nnegative skew can mean occasional large negative days (crash)\npositive skew can mean occasional large positive days (lottery-like)\n\n\n\nexcess kurtosis\nkurtosis measures tail heaviness relative to normal:\n\\[\n\\text{kurt} = \\frac{\\mathbb{e}[(r-\\mu)^4]}{\\sigma^4} - 3\n\\]\nthe “\\(-3\\)” makes normal distribution kurtosis equal to \\(0\\) (“excess kurtosis”).\n\n\ntail ratio (quantile-based)\na simple, robust tail comparison is:\n\\[\n\\text{tail ratio} = \\left|\\frac{q_{0.95}}{q_{0.05}}\\right|\n\\]\nwhere \\(q_p\\) is the \\(p\\)-quantile of daily returns. if the left tail is much larger in magnitude than the right tail, the ratio drops. If we have bigger left tail than right tail, it means we have more extreme negative returns than extreme possitive which can be a sign of risk.\n\n\nworst-day averages\nanother tail measure that can be used is:\n\nworst 1-day return: \\(\\min_t r_t\\)\naverage of worst 5 days: mean of the 5 smallest returns\naverage of worst 10 days: mean of the 10 smallest returns\n\nthese are easy for users to understand what does a bad week look like without introducing a full scenario model.\nin below we get to more advanced models for these types of risk with VaR.\n\nshape_rows = []\n\nfor name, r in obj.items():\n    x = pd.Series(r).dropna()\n\n    q05 = float(x.quantile(0.05))\n    q95 = float(x.quantile(0.95))\n    tail_ratio = float(abs(q95 / q05)) if abs(q05) &gt; 1e-12 else np.nan\n\n    worst_1d = float(x.min()) if len(x) else np.nan\n    worst_5d_avg = float(x.nsmallest(5).mean()) if len(x) &gt;= 5 else np.nan\n    worst_10d_avg = float(x.nsmallest(10).mean()) if len(x) &gt;= 10 else np.nan\n\n    shape_rows.append({\n        \"object\": name,\n        \"skew\": float(x.skew()) if len(x) else np.nan,\n        \"excess_kurtosis\": float(x.kurt()) if len(x) else np.nan,  \n        # pandas returns excess kurtosis so we don't have to subtract 3\n        \"tail_ratio_95_05\": tail_ratio,\n        \"worst_1d\": worst_1d,\n        \"worst_5d_avg\": worst_5d_avg,\n        \"worst_10d_avg\": worst_10d_avg,\n    })\n\nshape_tbl = pd.DataFrame(shape_rows).set_index(\"object\").sort_index()\ndisplay(shape_tbl.round(4))\n\n\n\n\n\n\n\n\nskew\nexcess_kurtosis\ntail_ratio_95_05\nworst_1d\nworst_5d_avg\nworst_10d_avg\n\n\nobject\n\n\n\n\n\n\n\n\n\n\naapl\n0.1655\n6.8540\n0.9931\n-0.1286\n-0.0999\n-0.0850\n\n\nmaxsharpe_frontier\n-0.1096\n6.9413\n0.9987\n-0.1208\n-0.1070\n-0.0907\n\n\nmv_ewma\n-0.4052\n12.5224\n1.0800\n-0.1050\n-0.0767\n-0.0602\n\n\nnvda\n0.1793\n5.2051\n1.0930\n-0.1875\n-0.1605\n-0.1310\n\n\n\n\n\n\n\nLooks like MV_ewma doesn’t have the best shape if we use skew and kurt, but this is because it’s distribution is closer to 0 and every little extreme loss can drive the skew and kurt to bad situation. based on kurtosis, all of the objects are fat tailed but the amount of losses that we take from tail is different for each object. When it comes to see how much loss we take in worst days we again see that MV_ewma has lower loss and nvidia again has the most loss. and from these measure our maxsharpe and apple are very close to eachother.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#cumulative-performance-and-drawdown",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#cumulative-performance-and-drawdown",
    "title": "3. risk analysis and CAPM",
    "section": "5) cumulative performance and drawdown",
    "text": "5) cumulative performance and drawdown\n\n5.1 drawdown\ndrawdown measures how far we get below the previous peak and how much time does it take to get back to peak.\nwe define peak nav as:\n\\[\n\\text{peak}_t = \\max_{u \\le t} \\text{nav}_u\n\\]\nthen drawdown is:\n\\[\n\\text{dd}_t = \\frac{\\text{nav}_t}{\\text{peak}_t} - 1\n\\]\nso drawdown is \\(0\\) at peaks and negative otherwise.\ndrawdown analysis answers: - how deep are losses during stress? - how long does it take to recover? - are drawdowns frequent but shallow, or rare but huge?\n\ndef dd_series(r):\n    nav = nav_series(r)\n    return nav / nav.cummax() - 1.0\n\nfig, ax = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n\nfor name, r in obj.items():\n    nav = nav_series(r)\n    ax[0].plot(nav.index, nav.values, lw=2.0, color=obj_colors[name], label=name)\n\nax[0].set_title(\"cumulative nav\")\nax[0].set_ylabel(\"nav\")\nax[0].legend(ncol=4)\n\nfor name, r in obj.items():\n    dd = dd_series(r)\n    ax[1].plot(dd.index, dd.values, lw=1.6, color=obj_colors[name], label=name)\n\nax[1].axhline(0.0, color=\"#444\", lw=1)\nax[1].set_title(\"drawdown\")\nax[1].set_ylabel(\"drawdown\")\nax[1].set_xlabel(\"date\")\nax[1].legend(ncol=4)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2 drawdown episode\nthe drawdown time-series is great visually, but a user also needs events. we want to know exactly what were the worst drawdowns, when did they start, and how long did they last\nan episode starts when drawdown becomes negative and ends when nav reaches the last peak (drawdown returns to \\(0\\)).\nfor each episode \\(k\\) we record: - start date \\(t_k^{start}\\) - end date \\(t_k^{end}\\) - depth: \\(\\min_{t \\in [t_k^{start}, t_k^{end}]} \\text{dd}_t\\) - duration: number of trading days in the episode\n\ndef drawdown_episodes(r):\n    dd = dd_series(r)\n    in_dd = False\n    start_i = None\n    out = []\n    for i, v in enumerate(dd.values):\n        if v &lt; 0 and not in_dd:\n            in_dd = True\n            start_i = i\n        if v == 0 and in_dd:\n            end_i = i\n            seg = dd.iloc[start_i:end_i]\n            out.append((seg.index[0], seg.index[-1], float(seg.min()), int(len(seg))))\n            in_dd = False\n    if in_dd:\n        seg = dd.iloc[start_i:]\n        out.append((seg.index[0], seg.index[-1], float(seg.min()), int(len(seg))))\n    return pd.DataFrame(out, columns=[\"start\", \"end\", \"depth\", \"duration\"])\n\n\ndef avg_recovery_time(r):\n    nav = nav_series(r)\n    peak = nav.cummax()\n    dd = nav / peak - 1\n    rec_times = []\n    in_dd = False\n    t0 = None\n    for i, v in enumerate(dd.values):\n        if v &lt; 0 and not in_dd:\n            in_dd = True\n            t0 = i\n        if v == 0 and in_dd:\n            rec_times.append(i - t0)\n            in_dd = False\n    return float(np.mean(rec_times)) if len(rec_times) else np.nan\n\ndd_rows = []\n\nfor name, r in obj.items():\n    x = pd.Series(r).dropna()\n    dd = dd_series(x)\n\n    ep = drawdown_episodes(x)\n    longest_dd_days = int(ep[\"duration\"].max()) if len(ep) else 0\n\n    dd_rows.append({\n        \"object\": name,\n        \"max_dd\": float(dd.min()) if len(dd) else np.nan,\n        \"longest_dd_days\": longest_dd_days,\n        \"avg_recovery_days\": avg_recovery_time(x),\n    })\n\ndd_summary_tbl = pd.DataFrame(dd_rows).set_index(\"object\").sort_index()\ndisplay(dd_summary_tbl.round(4))\n\n\n\n\n\n\n\n\nmax_dd\nlongest_dd_days\navg_recovery_days\n\n\nobject\n\n\n\n\n\n\n\naapl\n-0.3852\n354\n21.2234\n\n\nmaxsharpe_frontier\n-0.4772\n1238\n30.5217\n\n\nmv_ewma\n-0.2954\n679\n16.8803\n\n\nnvda\n-0.6634\n373\n18.4019\n\n\n\n\n\n\n\n\nepisodes_rows = []\n\nfor name, r in obj.items():\n    ep = drawdown_episodes(r).sort_values(\"depth\")\n    ep = ep.head(2).copy()\n    ep.insert(0, \"object\", name)\n    episodes_rows.append(ep)\n\nepisodes_tbl = pd.concat(episodes_rows, axis=0).reset_index(drop=True)\ndisplay(episodes_tbl)\n\n\n\n\n\n\n\n\nobject\nstart\nend\ndepth\nduration\n\n\n\n\n0\nnvda\n2021-11-30\n2023-05-24\n-0.663351\n373\n\n\n1\nnvda\n2018-10-02\n2020-02-13\n-0.560400\n344\n\n\n2\naapl\n2018-10-04\n2019-10-09\n-0.385177\n255\n\n\n3\naapl\n2024-12-27\n2025-10-17\n-0.333607\n202\n\n\n4\nmv_ewma\n2020-02-21\n2020-05-13\n-0.295405\n58\n\n\n5\nmv_ewma\n2021-03-23\n2023-11-30\n-0.251735\n679\n\n\n6\nmaxsharpe_frontier\n2021-02-16\n2026-01-20\n-0.477211\n1238\n\n\n7\nmaxsharpe_frontier\n2020-02-20\n2020-05-08\n-0.282402\n56\n\n\n\n\n\n\n\nFrom the plot, we have the most drawdown in Nvidia. it means in around 2022 we reached our peak in nvidia and then it came down around 66% in 2022 before going up and reaching it’s last peak again. this is the part that makes keeping nvidia stocks hard. you would have earned a lot of money from Nvidia Only if you kept it through the 50-60% loss in 2019 and 2022. Apple seems to have better performance than maxsharpe in drawdown. and MV_ewma has the most stable performance and the least max drawdown. below we get to more details about drawdown.\nlooks like Maxsharpe diversification isn’t as good for reducing drawdown and negative effects of crashes like covid, but in some times like 2019 that both stocks have huge drawdown that can be a market effect, it doesn’t have that much drawdown.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#value-at-risk-var-and-expected-shortfall-es-or-cvar",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#value-at-risk-var-and-expected-shortfall-es-or-cvar",
    "title": "3. risk analysis and CAPM",
    "section": "6) Value-at-risk (VaR) and expected shortfall (ES or CVaR)",
    "text": "6) Value-at-risk (VaR) and expected shortfall (ES or CVaR)\nWe analyzed tail and distribution of objects for risk and average worst days to answer On bad days, how large can losses get and how severe are losses once we enter the tail\nIf daily simple returns be \\(r_t\\) (\\(r_t=-0.02\\) means a \\(-2\\%\\) return in one day).\nWe choose a tail probability \\(\\alpha\\) (common one is \\(\\alpha=0.05\\) for the worst 5% of days).\n\n6.1 left-tail quantile\nWe define the left-tail quantile \\(q_\\alpha\\) of the return distribution as the threshold such that only an \\(\\alpha\\) fraction of observations fall below it:\n\\[\nP(r \\le q_\\alpha(r)) = \\alpha.\n\\]\nBecause this is the left tail, \\(q_\\alpha(r)\\) is typically negative (a loss).\nRisk reports often present tail risk as a positive loss magnitude for readability.\n\n\n6.2 value-at-risk (VaR)\nVaR is a threshold loss:\nUsing the quantile definition:\n\\[\n\\text{VaR}_\\alpha = -q_\\alpha(r).\n\\]\nFor example if \\(\\alpha=0.05\\) - If \\(\\text{VaR}_{0.05}=2.1\\%\\), then on 95% of days the loss is no worse than 2.1%. - On the worst 5% of days, losses are worse than 2.1%. - This means we should except this asset to have more than 2.1% loss in the worst 5% of days. It’s rare but it happens and it’s importnant to know how much loss we except as a measure of risk.\nThe problem is that VaR tells you where the tail begins, but not how large losses are inside the tail.\n\n\n\n6.3 expected shortfall (ES) / conditional VaR\nExpected Shortfall (ES) measures tail severity by averaging losses beyond VaR:\n\\[\n\\text{ES}_\\alpha = -E\\!\\left[r \\mid r \\le q_\\alpha(r)\\right].\n\\]\nIf \\(\\alpha=0.05\\): - If \\(\\text{ES}_{0.05}=3.4\\%\\), then among the worst 5% of days, the average loss is 3.4%.\n\nVaR is a cutoff (one quantile). It tells us what the best return in the worst 5% losses are.\nES is a severity measure. it tells us the average loss we should except in the worst 5% of losses.\n\nES is always bigger (or maybe equal) than VaR. If ES is much larger than VaR, the distribution has a heavier left tail (more extreme losses after crossing the threshold).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#estimation-methods-used-in-this-report",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#estimation-methods-used-in-this-report",
    "title": "3. risk analysis and CAPM",
    "section": "6.4 estimation methods used in this report",
    "text": "6.4 estimation methods used in this report\nWe report a comparison table for \\(\\alpha=0.05\\) using two other estimators: 1) Cornish–Fisher (CF) adjusted quantiles 2) Filtered historical simulation (FHS) with EWMA volatility\nA single VaR/ES estimate can be fragile, so comparing multiple approaches helps users see a plausible range.\n\n6.4.1 cornish–fisher (CF): non-normal quantile correction using skewness and kurtosis\nCF starts from the normal quantile and adjusts it to reflect skewness and fat tails.\n\n\n(a) standardize returns\nFrom a sample window, estimate: - \\(\\mu\\) (sample mean) and \\(\\sigma\\) (sample standard deviation) - standardized values \\(x_t = (r_t-\\mu)/\\sigma\\)\nCompute standardized skewness \\(S\\) and excess kurtosis \\(K\\):\n\\[\nS = E[X^3] \\approx \\frac{1}{T}\\sum_{t=1}^T x_t^3,\n\\qquad\nK = E[X^4]-3 \\approx \\frac{1}{T}\\sum_{t=1}^T x_t^4 - 3.\n\\]\n\n\n(b) adjust the normal quantile\nWe set \\(z = z_\\alpha = \\Phi^{-1}(\\alpha)\\) as the standard normal \\(\\alpha\\)-quantile.\nA commonly used CF expansion is:\n\\[\nz_{\\text{CF}} =\nz\n+\\frac{1}{6}(z^2-1)S\n+\\frac{1}{24}(z^3-3z)K\n-\\frac{1}{36}(2z^3-5z)S^2.\n\\]\nThen the CF return quantile is:\n\\[\nq_\\alpha^{\\text{CF}}(r) = \\mu + \\sigma z_{\\text{CF}}.\n\\]\nSo the CF VaR is:\n\\[\n\\text{VaR}_\\alpha^{\\text{CF}} = -\\left(\\mu + \\sigma z_{\\text{CF}}\\right).\n\\]\n\n\n(c) Cornish Fisher ES\nCF primarily provides a corrected quantile (VaR).\nA common practical ES approximation is to compute ES empirically using the CF cutoff:\nWe first compute \\(q_\\alpha^{\\text{CF}}(r)\\)\nand then average sample returns below that cutoff:\n\\[\n\\text{ES}_\\alpha^{\\text{CF}}\n\\approx\n-\\frac{1}{|\\mathcal{T}_\\alpha^{\\text{CF}}|}\\sum_{t\\in \\mathcal{T}_\\alpha^{\\text{CF}}} r_t,\n\\qquad\n\\mathcal{T}_\\alpha^{\\text{CF}}=\\{t:r_t\\le q_\\alpha^{\\text{CF}}(r)\\}.\n\\]\nThis model incorporates skewness/kurtosis (non-normality), but approximation can be unstable if skew/kurt estimates are noisy or tails are extreme\nnegative skew (\\(S&lt;0\\)) usually worsens left-tail quantiles positive excess kurtosis (\\(K&gt;0\\)) deepens tail risk vs normal\n\n\n\n6.4.2 historical simulation (HS)\nHS is the most direct approach: it treats the observed return window as the empirical distribution.\nThe HS quantile is the empirical quantile\n\\[\nq_\\alpha^{\\text{HS}}(r) = \\text{EmpQuantile}_\\alpha(\\{r_t\\}_{t=1}^T),\n\\]\nso\n\\[\n\\text{VaR}_\\alpha^{\\text{HS}} = -q_\\alpha^{\\text{HS}}(r).\n\\]\nAnd HS ES is:\n\\[\n\\text{ES}_\\alpha^{\\text{HS}}\n= -E[r \\mid r \\le q_\\alpha^{\\text{HS}}(r)]\n\\approx\n-\\frac{1}{|\\mathcal{T}_\\alpha|}\\sum_{t\\in \\mathcal{T}_\\alpha} r_t.\n\\]\nThis moodel can be sensitive to the chosen window, and to regime changes\n\n\n\n6.4.3 filtered historical simulation (FHS): volatility-adjusted tail estimation\nHS assumes the return distribution is stable across time.\nIn reality, returns show volatility clustering (calm vs turbulent periods).\nWith FHS we address this by filtering out time-varying volatility before sampling the tail.\n\n\nEWMA volatility filter\nThere are many ways for filtering. In this project we use this approach:\nWe estimate conditional variance using EWMA:\n\\[\n\\sigma_t^2 = \\lambda \\sigma_{t-1}^2 + (1-\\lambda)r_{t-1}^2,\n\\]\nwhere \\(\\lambda\\in(0,1)\\) is the decay parameter (we set as \\(\\lambda\\approx 0.94\\)).\n\n\nFHS expected shortfall\n\\[\n\\text{ES}_{\\alpha,t+1}^{\\text{FHS}} =\n-\\left(\\mu_{t+1} + \\sigma_{t+1} \\, E[\\varepsilon \\mid \\varepsilon \\le q_\\alpha(\\varepsilon)]\\right).\n\\]\nEmpirically:\n\\[\nE[\\varepsilon \\mid \\varepsilon \\le q_\\alpha(\\varepsilon)]\n\\approx\n\\frac{1}{|\\mathcal{T}_\\alpha^\\varepsilon|}\n\\sum_{t\\in \\mathcal{T}_\\alpha^\\varepsilon}\\varepsilon_t.\n\\]\nSo:\n\\[\n\\text{ES}_{\\alpha,t+1}^{\\text{FHS}}\n\\approx\n-\\left(\\mu_{t+1} + \\sigma_{t+1}\n\\frac{1}{|\\mathcal{T}_\\alpha^\\varepsilon|}\n\\sum_{t\\in \\mathcal{T}_alpha^\\varepsilon}\\varepsilon_t\\right).\n\\]\nThis model adapts to volatility regimes and has better behavior when today’s volatility differs from the historical average of volatility, but it depends what model we use and can be different\n\ndef hist_var_es(r, alpha=0.05):\n    x = pd.Series(r).dropna()\n    q = x.quantile(alpha)\n    es = x[x &lt;= q].mean()\n    return -float(q), -float(es)\n\ndef cf_var_es(r, alpha=0.05, n_sim=70000, seed=7):\n    x = pd.Series(r).dropna()\n    mu = float(x.mean())\n    sd = float(x.std(ddof=1))\n    if sd &lt;= 1e-12:\n        return np.nan, np.nan\n    s = float(x.skew())\n    k = float(x.kurt())\n    z = normaldist().inv_cdf(alpha)\n    zc = z + (z**2 - 1)*s/6 + (z**3 - 3*z)*k/24 - (2*z**3 - 5*z)*(s**2)/36\n    q = mu + sd * zc\n\n    rng = np.random.default_rng(seed)\n    zs = rng.standard_normal(n_sim)\n    za = zs + (zs**2 - 1)*s/6 + (zs**3 - 3*zs)*k/24 - (2*zs**3 - 5*zs)*(s**2)/36\n    rs = mu + sd * za\n    es = rs[rs &lt;= q].mean()\n    return -float(q), -float(es)\n\ndef fhs_var_es(r, alpha=0.05, lam=0.94):\n    x = pd.Series(r).dropna().astype(float)\n    mu = float(x.mean())\n    e = x - mu\n\n    sig = np.zeros(len(e), dtype=float)\n    sig[0] = max(float(e.std(ddof=1)), 1e-6)\n    for t in range(1, len(e)):\n        sig[t] = np.sqrt(lam * sig[t - 1]**2 + (1 - lam) * e.iloc[t - 1]**2)\n\n    z = e.to_numpy() / np.where(sig &gt; 1e-12, sig, np.nan)\n    z = z[np.isfinite(z)]\n    qz = np.quantile(z, alpha)\n    ez = z[z &lt;= qz].mean()\n\n    sn = sig[-1]\n    return float(-(mu + sn * qz)), float(-(mu + sn * ez))\n\n\nvar_rows = []\nfor name, r in obj.items():\n    x = pd.Series(r).dropna()\n    hv, he = hist_var_es(x, 0.05)\n    cv, ce = cf_var_es(x, 0.05)\n    fv, fe = fhs_var_es(x, 0.05)\n    var_rows.append({\n        \"object\": name,\n        \"hist_var5\": hv,\n        \"hist_es5\": he,\n        \"cf_var5\": cv,\n        \"cf_es5\": ce,\n        \"fhs_var5\": fv,\n        \"fhs_es5\": fe,\n    })\n\nvar_tbl = pd.DataFrame(var_rows).set_index(\"object\").sort_index()\ndisplay(var_tbl.round(4))\n\n\n\n\n\n\n\n\nhist_var5\nhist_es5\ncf_var5\ncf_es5\nfhs_var5\nfhs_es5\n\n\nobject\n\n\n\n\n\n\n\n\n\n\naapl\n0.0287\n0.0425\n0.0261\n0.0547\n0.0205\n0.0299\n\n\nmaxsharpe_frontier\n0.0283\n0.0447\n0.0276\n0.0578\n0.0455\n0.0679\n\n\nmv_ewma\n0.0149\n0.0245\n0.0156\n0.0439\n0.0136\n0.0207\n\n\nnvda\n0.0466\n0.0691\n0.0446\n0.0839\n0.0286\n0.0409\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6), sharex=True)\naxes = axes.ravel()\n\nfor i, (name, r) in enumerate(obj.items()):\n    ax = axes[i]\n    x = pd.Series(r).dropna()\n\n    hv, he = hist_var_es(x, 0.05)\n    ax.hist(x.values, bins=60, density=True, alpha=0.75)\n    ax.axvline(-hv, lw=2.0, ls=\"--\", color=obj_colors[name], label=\"vaR 5% (hist)\")\n    ax.axvline(-he, lw=2.0, ls=\":\", color=obj_colors[name], label=\"es 5% (hist)\")\n    ax.set_title(f\"return distribution — {name}\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see from the table that models give us different results. If we only cosider Historic VaR, We might think that Apple is more risky than MaxSharpe, but other models show otherwise. again we see that the loss of Nvidia is the most in most of the models. and MV_ewma has controlled the risk the best way.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#var-backtesting-model-risk",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#var-backtesting-model-risk",
    "title": "3. risk analysis and CAPM",
    "section": "7) VaR backtesting (Model risk)",
    "text": "7) VaR backtesting (Model risk)\nA VaR model makes a testable promise. For a 5% VaR, Losses should exceed the VaR threshold about 5% of the time.\nBacktesting checks whether that promise holds in realized data.\n\n7.1 breach indicator (what counts as a VaR failure)\nA VaR breach occurs when the realized return is worse than the VaR threshold:\n\\[\nr_t &lt; -\\text{VaR}_{\\alpha,t}.\n\\]\nWe define the breach indicator:\n\\[\nb_t = \\mathbb{1}\\!\\left[r_t &lt; -\\text{VaR}_{\\alpha,t}\\right],\n\\]\nwhere \\(b_t=1\\) means a breach happened, and \\(b_t=0\\) otherwise.\nWe summarize breaches using: - breach count: \\(x = \\sum_{t=1}^n b_t\\) - breach rate: \\(\\hat p = x/n\\) - longest breach streak: \\(\\max\\) number of consecutive \\(b_t=1\\) (a simple clustering diagnostic)\nIf the VaR model is correct, we expect \\(\\hat p \\approx \\alpha\\) over a long sample. If breaches happen in streaks, the model may be underreacting to volatility regime changes (clustering risk).\n\n\n\n7.2 Kupiec test: unconditional coverage (frequency)\nThe Kupiec (POF) test checks whether breaches occur with the correct long-run frequency.\nIf we have: - \\(n\\) = number of test days - \\(x\\) = number of breaches - \\(\\hat p = x/n\\) = observed breach rate - \\(p = \\alpha\\) = model-implied breach probability (5%)\nKupiec’s likelihood ratio statistic is:\nThe log-likelihood under the null (correct coverage) is:\n\\[\n\\ell_0 = (n-x)\\log(1-p) + x\\log(p).\n\\]\nThe log-likelihood under the alternative (best-fitting rate) is:\n\\[\n\\ell_1 = (n-x)\\log(1-\\hat p) + x\\log(\\hat p).\n\\]\nKupiec’s likelihood-ratio statistic is:\n\\[\n\\text{LR}_{uc} = -2(\\ell_0-\\ell_1),\n\\qquad\n\\text{LR}_{uc}\\sim \\chi^2(1)\\ \\text{under }H_0.\n\\]\nit means: - large \\(\\text{LR}_{uc}\\) (small p-value) means the breach frequency is wrong: - too many breaches means VaR is too small (underestimates risk) - too few breaches means VaR is too conservative\n\n\n\n7.3 Christoffersen test: independence / clustering\nCorrect frequency alone is not enough, breaches should also be independent over time.\nIf breaches have clustering, the VaR model may fail during volatility spikes or regime shifts. se we test independence\nThe Christoffersen independence test treats the breach sequence \\(b_t\\) as a two-state process (0 = no breach, 1 = breach) and checks whether transitions depend on the previous day.\nCount transitions: - \\(n_{00}\\): number of times \\(b_{t-1}=0 \\to b_t=0\\) - \\(n_{01}\\): number of times \\(b_{t-1}=0 \\to b_t=1\\) - \\(n_{10}\\): number of times \\(b_{t-1}=1 \\to b_t=0\\) - \\(n_{11}\\): number of times \\(b_{t-1}=1 \\to b_t=1\\)\nEstimate transition probabilities:\n\\[\n\\pi_{01} = \\frac{n_{01}}{n_{00}+n_{01}}, \\qquad\n\\pi_{11} = \\frac{n_{11}}{n_{10}+n_{11}}.\n\\]\nIf breaches are independent, the probability of a breach tomorrow does not depend on whether there was a breach today, so:\n\\[\nH_0: \\pi_{01} = \\pi_{11}.\n\\]\nLog-likelihood under independence: \\[\n\\ell_0 = (n_{00}+n_{10})\\log(1-\\pi) + (n_{01}+n_{11})\\log(\\pi).\n\\]\nLog-likelihood under dependence (two transition probabilities): \\[\n\\ell_1 =\nn_{00}\\log(1-\\pi_{01}) + n_{01}\\log(\\pi_{01})\n+\nn_{10}\\log(1-\\pi_{11}) + n_{11}\\log(\\pi_{11}).\n\\]\nThe test statistic is: \\[\n\\text{LR}_{ind} = -2(\\ell_0-\\ell_1),\n\\qquad\n\\text{LR}_{ind}\\sim \\chi^2(1)\\ \\text{under }H_0.\n\\]\nWe define likelihoods: - under independence (Bernoulli with constant probability \\(\\hat p\\)):\n\\[\nL_0 =\n(1-\\hat p)^{n_{00}+n_{10}}\n\\hat p^{n_{01}+n_{11}}\n\\]\n\nunder first-order dependence (different transition probabilities):\n\n\\[\nL_1 =\n(1-\\pi_{01})^{n_{00}}\n\\pi_{01}^{n_{01}}\n(1-\\pi_{11})^{n_{10}}\n\\pi_{11}^{n_{11}}.\n\\]\nChristoffersen’s likelihood ratio statistic:\n\\[\n\\text{LR}_{ind} = -2\\ln\\left(\\frac{L_0}{L_1}\\right)\n\\qquad\n\\text{LR}_{ind}\\sim \\chi^2(1) \\text{ under } H_0.\n\\]\nsmall p-value means breaches are clustered (depend on previous breach status) clustering is a common sign that the VaR model is not adapting fast enough to changing volatility\nWe report p-values for:\n\nKupiec (coverage): is breach frequency close to \\(\\alpha\\)\nChristoffersen (independence): are breaches unclustered\n\nA well-specified VaR model typically has: - coverage p-value not too small (frequency is plausible) - independence p-value not too small (no strong clustering)\nSmall p-values suggest misspecification: wrong level of risk, volatility dynamics not captured, or regime changes.\n\nalpha = 0.05\nlookback = 252\nbt_methods = [\"hist\", \"cf\", \"fhs\"]\n\n\ndef chi2_sf(x, df):\n    return float(chi2.sf(x, df))\n\n\ndef rolling_var_quantile(r, alpha=0.05, lookback=252, method=\"hist\", cf_n_sim=15000, cf_seed=7, fhs_lambda=0.94):\n    x = pd.Series(r).dropna().astype(float)\n    if len(x) &lt; lookback + 1:\n        return pd.Series(dtype=float)\n\n    m = str(method).strip().lower()\n    q = pd.Series(np.nan, index=x.index, dtype=float)\n    for i in range(lookback, len(x)):\n        w = x.iloc[i - lookback:i]\n        if m == \"hist\":\n            v, _ = hist_var_es(w, alpha=alpha)\n        elif m == \"cf\":\n            v, _ = cf_var_es(w, alpha=alpha, n_sim=cf_n_sim, seed=cf_seed)\n        elif m == \"fhs\":\n            v, _ = fhs_var_es(w, alpha=alpha, lam=fhs_lambda)\n        else:\n            raise ValueError(\"method must be one of {'hist', 'cf', 'fhs'}\")\n        q.iloc[i] = -float(v) if np.isfinite(v) else np.nan\n    return q\n\n\ndef longest_true_streak(mask):\n    m = np.asarray(mask, dtype=bool)\n    best = 0\n    cur = 0\n    for v in m:\n        if v:\n            cur += 1\n            best = max(best, cur)\n        else:\n            cur = 0\n    return int(best)\n\n\ndef kupiec_test(breach, alpha=0.05):\n    b = np.asarray(breach, dtype=bool)\n    n = int(b.size)\n    x = int(b.sum())\n    if n &lt;= 0:\n        return np.nan, np.nan\n\n    p = float(alpha)\n    eps = 1e-12\n    ph = x / n\n    ph = min(max(ph, eps), 1.0 - eps)\n\n    ll0 = (n - x) * np.log1p(-p) + x * np.log(p)\n    ll1 = (n - x) * np.log1p(-ph) + x * np.log(ph)\n\n    lr = float(-2.0 * (ll0 - ll1))\n    pv = chi2_sf(lr, df=1)\n    return lr, pv\n\n\ndef christoffersen_independence(breach):\n    b = np.asarray(breach, dtype=int)\n    if b.size &lt; 3:\n        return np.nan, np.nan\n\n    b0 = b[:-1]\n    b1 = b[1:]\n\n    n00 = int(((b0 == 0) & (b1 == 0)).sum())\n    n01 = int(((b0 == 0) & (b1 == 1)).sum())\n    n10 = int(((b0 == 1) & (b1 == 0)).sum())\n    n11 = int(((b0 == 1) & (b1 == 1)).sum())\n\n    eps = 1e-12\n    pi01 = n01 / (n00 + n01 + eps)\n    pi11 = n11 / (n10 + n11 + eps)\n    pi = (n01 + n11) / (n00 + n01 + n10 + n11 + eps)\n\n    pi01 = min(max(pi01, eps), 1.0 - eps)\n    pi11 = min(max(pi11, eps), 1.0 - eps)\n    pi = min(max(pi, eps), 1.0 - eps)\n\n    ll0 = (n00 + n10) * np.log1p(-pi) + (n01 + n11) * np.log(pi)\n    ll1 = (\n        n00 * np.log1p(-pi01) + n01 * np.log(pi01)\n        + n10 * np.log1p(-pi11) + n11 * np.log(pi11)\n    )\n\n    lr = float(-2.0 * (ll0 - ll1))\n    pv = chi2_sf(lr, df=1)\n    return lr, pv\n\n\ndef quantile_loss(ret, q, alpha=0.05):\n    z = pd.concat([pd.Series(ret).rename(\"ret\"), pd.Series(q).rename(\"q\")], axis=1).dropna()\n    if len(z) == 0:\n        return np.nan\n    e = z[\"ret\"] - z[\"q\"]\n    loss = e * (alpha - (e &lt; 0).astype(float))\n    return float(loss.mean())\n\n\ndef breach_stats(r, alpha=0.05, lookback=252, method=\"hist\"):\n    x = pd.Series(r).dropna().astype(float)\n    q = rolling_var_quantile(x, alpha=alpha, lookback=lookback, method=method)\n    z = pd.concat([x.rename(\"ret\"), q.rename(\"var_q\")], axis=1).dropna()\n\n\n    br = z[\"ret\"] &lt; z[\"var_q\"]\n    lr_uc, pv_uc = kupiec_test(br, alpha=alpha)\n    lr_ind, pv_ind = christoffersen_independence(br)\n\n    idx = np.flatnonzero(br.to_numpy())\n    gaps = np.diff(idx) if idx.size &gt;= 2 else np.array([])\n\n    rate = float(br.mean())\n    return {\n        \"series\": z,\n        \"breach\": br,\n        \"count\": int(br.sum()),\n        \"rate\": rate,\n        \"coverage_error\": float(rate - alpha),\n        \"abs_coverage_error\": float(abs(rate - alpha)),\n        \"longest_streak\": longest_true_streak(br.to_numpy()),\n        \"avg_gap\": float(np.mean(gaps)) if gaps.size else np.nan,\n        \"med_gap\": float(np.median(gaps)) if gaps.size else np.nan,\n        \"kupiec_lr\": lr_uc,\n        \"kupiec_p\": pv_uc,\n        \"christ_lr\": lr_ind,\n        \"christ_p\": pv_ind,\n        \"quantile_loss\": quantile_loss(z[\"ret\"], z[\"var_q\"], alpha=alpha),\n    }\n\n\nrows = []\nstats_map = {}\nfor name, r in obj.items():\n    stats_map[name] = {}\n    for m in bt_methods:\n        st = breach_stats(r, alpha=alpha, lookback=lookback, method=m)\n        stats_map[name][m] = st\n        rows.append({\n            \"object\": name,\n            \"method\": m,\n            \"breach_count\": st[\"count\"],\n            \"breach_rate\": st[\"rate\"],\n            \"coverage_error\": st[\"coverage_error\"],\n            \"abs_coverage_error\": st[\"abs_coverage_error\"],\n            \"longest_breach_streak\": st[\"longest_streak\"],\n            \"avg_gap_days\": st[\"avg_gap\"],\n            \"kupiec_p\": st[\"kupiec_p\"],\n            \"christoffersen_p\": st[\"christ_p\"],\n            \"quantile_loss\": st[\"quantile_loss\"],\n        })\n\nvar_bt_tbl = pd.DataFrame(rows).set_index([\"object\", \"method\"]).sort_index()\nvar_bt_tbl[\"accuracy_rank\"] = np.nan\nvar_bt_tbl[\"accuracy_score\"] = np.nan\nvar_bt_tbl[\"is_best\"] = False\n\nfor name, g in var_bt_tbl.groupby(level=0, sort=False):\n    abs_cov = g[\"abs_coverage_error\"]\n    qloss = g[\"quantile_loss\"]\n    kup = g[\"kupiec_p\"].fillna(-np.inf)\n    chrp = g[\"christoffersen_p\"].fillna(-np.inf)\n\n    r_abs = abs_cov.rank(ascending=True, method=\"min\", na_option=\"bottom\")\n    r_ql = qloss.rank(ascending=True, method=\"min\", na_option=\"bottom\")\n    r_k = kup.rank(ascending=False, method=\"min\")\n    r_c = chrp.rank(ascending=False, method=\"min\")\n\n    rank_sum = (r_abs + r_ql + r_k + r_c).astype(float)\n    acc_rank = rank_sum.rank(ascending=True, method=\"min\")\n    acc_score = 1.0 / (1.0 + rank_sum)\n\n    var_bt_tbl.loc[g.index, \"accuracy_rank\"] = acc_rank.to_numpy(dtype=float)\n    var_bt_tbl.loc[g.index, \"accuracy_score\"] = acc_score.to_numpy(dtype=float)\n\n    best_idx = pd.DataFrame(\n        {\n            \"rank_sum\": rank_sum,\n            \"abs_cov\": abs_cov,\n            \"qloss\": qloss,\n            \"kupiec\": kup,\n            \"christ\": chrp,\n            \"method_name\": [idx[1] for idx in g.index],\n        },\n        index=g.index,\n    ).sort_values(\n        by=[\"rank_sum\", \"abs_cov\", \"qloss\", \"kupiec\", \"christ\", \"method_name\"],\n        ascending=[True, True, True, False, False, True],\n    ).index[0]\n    var_bt_tbl.loc[best_idx, \"is_best\"] = True\n\n\ndisplay(var_bt_tbl.round(4))\n\nbest_method_map = {obj_name: method for obj_name, method in var_bt_tbl[var_bt_tbl[\"is_best\"]].index}\nbreach_map = {}\nfor name, r in obj.items():\n    m = best_method_map.get(name, \"hist\")\n    st = stats_map[name][m]\n    st[\"method\"] = m\n    breach_map[name] = st\n\nvar_bt_tbl_pdf = var_bt_tbl.rename(columns={\n    \"breach_count\": \"breaches\",\n    \"breach_rate\": \"rate\",\n    \"longest_breach_streak\": \"max_streak\",\n    \"avg_gap_days\": \"avg_gap_d\",\n    \"kupiec_p\": \"kupiec_p\",\n    \"christoffersen_p\": \"christoffersen_p\",\n})\n\n\n\n\n\n\n\n\n\nbreach_count\nbreach_rate\ncoverage_error\nabs_coverage_error\nlongest_breach_streak\navg_gap_days\nkupiec_p\nchristoffersen_p\nquantile_loss\naccuracy_rank\naccuracy_score\nis_best\n\n\nobject\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naapl\ncf\n135\n0.0672\n0.0172\n0.0172\n4\n14.9254\n0.0008\n0.0002\n0.0023\n2.0\n0.0909\nFalse\n\n\nfhs\n105\n0.0523\n0.0023\n0.0023\n3\n19.2308\n0.6437\n0.0011\n0.0022\n1.0\n0.2000\nTrue\n\n\nhist\n124\n0.0617\n0.0117\n0.0117\n4\n16.2602\n0.0198\n0.0002\n0.0023\n2.0\n0.0909\nFalse\n\n\nmaxsharpe_frontier\ncf\n119\n0.0592\n0.0092\n0.0092\n3\n16.7712\n0.0647\n0.0017\n0.0023\n3.0\n0.0909\nFalse\n\n\nfhs\n107\n0.0533\n0.0033\n0.0033\n2\n18.6415\n0.5068\n0.0016\n0.0021\n1.0\n0.1667\nTrue\n\n\nhist\n117\n0.0582\n0.0082\n0.0082\n3\n17.0603\n0.0983\n0.0004\n0.0023\n2.0\n0.1000\nFalse\n\n\nmv_ewma\ncf\n103\n0.0513\n0.0013\n0.0013\n2\n19.1961\n0.7949\n0.0023\n0.0014\n1.0\n0.1429\nTrue\n\n\nfhs\n112\n0.0557\n0.0057\n0.0057\n2\n17.6396\n0.2453\n0.0001\n0.0013\n3.0\n0.0909\nFalse\n\n\nhist\n109\n0.0543\n0.0043\n0.0043\n3\n17.4537\n0.3877\n0.0002\n0.0013\n2.0\n0.1111\nFalse\n\n\nnvda\ncf\n170\n0.0846\n0.0346\n0.0346\n3\n11.8343\n0.0000\n0.0009\n0.0039\n3.0\n0.0769\nFalse\n\n\nfhs\n116\n0.0577\n0.0077\n0.0077\n2\n17.3913\n0.1199\n0.2037\n0.0035\n1.0\n0.2000\nTrue\n\n\nhist\n126\n0.0627\n0.0127\n0.0127\n3\n15.9920\n0.0117\n0.0344\n0.0037\n2.0\n0.1111\nFalse\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6), sharex=True, sharey=False)\naxes = axes.ravel()\n\nfor i, name in enumerate(obj.keys()):\n    ax = axes[i]\n    z = breach_map[name][\"series\"]\n    br = breach_map[name][\"breach\"]\n    m = breach_map[name].get(\"method\", \"hist\")\n\n    ax.plot(z.index, z[\"ret\"].values, lw=0.9, alpha=0.85, label=\"return\")\n    ax.plot(z.index, z[\"var_q\"].values, lw=2.0, label=f\"rolling vaR q(5%) [{m}]\")\n    ax.scatter(z.index[br], z.loc[br, \"ret\"].values, s=12, marker=\"x\", label=\"breach\")\n\n    ax.set_title(f\"rolling vaR + breaches (best model) - {name}\")\n    ax.set_ylabel(\"daily return\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can see both CF and FHS models have performed better and more accurate than the Historic VaR estimating. and for 3 of our objects FHS has performed better and for MV_ewma, CF ended up with more accuracy",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#historical-stress-windows-scenario-slices",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#historical-stress-windows-scenario-slices",
    "title": "3. risk analysis and CAPM",
    "section": "8) historical stress windows (scenario slices)",
    "text": "8) historical stress windows (scenario slices)\ninstead of hypothetical shocks, we can look at real historical periods like covid crash and see how did each object behave during that exact window, what was the max drawdown inside the window and how much loss we would take in that period if we held these objects.\nfor this project we use three periods: - the 4th quarter of 2018 - the Covid crash in the first months of 2020 - inflation of 2022\n\nstress_windows = {\n    \"2018_q4\": (\"2018-10-01\", \"2018-12-31\"),\n    \"2020_covid\": (\"2020-02-20\", \"2020-04-30\"),\n    \"2022_inflation\": (\"2022-01-03\", \"2022-10-31\"),\n}\n\nstress_rows = []\nfor wname, (s, e) in stress_windows.items():\n    s = pd.Timestamp(s)\n    e = pd.Timestamp(e)\n    for name, r in obj.items():\n        x = pd.Series(r).loc[(pd.Series(r).index &gt;= s) & (pd.Series(r).index &lt;= e)].dropna()\n        if len(x) == 0:\n            continue\n        nav = nav_series(x)\n        dd = nav / nav.cummax() - 1\n        worst_week = x.resample(\"W-FRI\").sum().min() if len(x) &gt; 5 else np.nan\n        stress_rows.append({\n            \"window\": wname,\n            \"object\": name,\n            \"cum_return\": float(nav.iloc[-1] - 1),\n            \"max_dd\": float(dd.min()),\n            \"worst_day\": float(x.min()),\n            \"worst_week\": float(worst_week) if np.isfinite(worst_week) else np.nan,\n        })\n\nstress_tbl = pd.DataFrame(stress_rows).sort_values([\"window\", \"object\"]).reset_index(drop=True)\n\n\ndisplay(stress_tbl.round(4))\n\n\n\n\n\n\n\n\nwindow\nobject\ncum_return\nmax_dd\nworst_day\nworst_week\n\n\n\n\n0\n2018_q4\naapl\n-0.2988\n-0.3651\n-0.0663\n-0.1140\n\n\n1\n2018_q4\nmaxsharpe_frontier\n-0.1036\n-0.1442\n-0.0375\n-0.0506\n\n\n2\n2018_q4\nmv_ewma\n-0.1150\n-0.1495\n-0.0296\n-0.0535\n\n\n3\n2018_q4\nnvda\n-0.5245\n-0.5604\n-0.1875\n-0.1988\n\n\n4\n2020_covid\naapl\n-0.0921\n-0.2995\n-0.1286\n-0.1803\n\n\n5\n2020_covid\nmaxsharpe_frontier\n-0.0848\n-0.2770\n-0.1208\n-0.1448\n\n\n6\n2020_covid\nmv_ewma\n-0.0461\n-0.2954\n-0.1050\n-0.1666\n\n\n7\n2020_covid\nnvda\n-0.0707\n-0.3634\n-0.1846\n-0.1286\n\n\n8\n2022_inflation\naapl\n-0.1329\n-0.2834\n-0.0587\n-0.0830\n\n\n9\n2022_inflation\nmaxsharpe_frontier\n-0.1842\n-0.2827\n-0.0649\n-0.0999\n\n\n10\n2022_inflation\nmv_ewma\n-0.0138\n-0.1623\n-0.0396\n-0.0566\n\n\n11\n2022_inflation\nnvda\n-0.5408\n-0.6270\n-0.0947\n-0.1709\n\n\n\n\n\n\n\n\nimport matplotlib.dates as mdates\n\nfig, axes = plt.subplots(1, len(stress_windows), figsize=(12, 3.5), sharey=True)\n\nif len(stress_windows) == 1:\n    axes = [axes]\n\nfor ax, (wname, (s, e)) in zip(axes, stress_windows.items()):\n    s = pd.Timestamp(s)\n    e = pd.Timestamp(e)\n    mid = s + (e - s) / 2\n\n    for name, r in obj.items():\n        x = pd.Series(r).loc[(pd.Series(r).index &gt;= s) & (pd.Series(r).index &lt;= e)].dropna()\n        if len(x) == 0:\n            continue\n        nav = nav_series(x)\n        nav = nav / nav.iloc[0]\n        ax.plot(nav.index, nav.values, lw=1.8, color=obj_colors[name], label=name)\n\n    ax.set_title(wname)\n    ax.set_xlabel(\"date\")\n    ax.grid(True, alpha=0.2)\n\n    # Force exactly 3 x-ticks to avoid overlap: start, mid, end.\n    ticks = [s, mid, e]\n    ax.set_xticks(ticks)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n    ax.tick_params(axis=\"x\", labelrotation=0, labelsize=8)\n\naxes[0].set_ylabel(\"nav (rebased to 1)\")\naxes[0].legend(ncol=1, fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhen it comes to these periods, companies get effected directly and these events cause more drops and negative returns in stock prices and diversification seems to be the safer choice. this is one of the rare measures that we can see MaxSharpe performed better than apple.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#capm-market-factor-decomposition",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#capm-market-factor-decomposition",
    "title": "3. risk analysis and CAPM",
    "section": "9) CAPM / market factor decomposition",
    "text": "9) CAPM / market factor decomposition\ncapm explains returns using a single market factor. here, we use spy data that we imported as our market proxy (S&P 500 ETF).\nWith this model we assume that any return more than risk free rate of our objects can be explained by market returns and market risk effects on these returns.\nFrom this relationship we can interpret how much each object’s risk is dependent to market risk and we can see if each object has any returns more than market returns that can’t be explained with market risk.\n\n9.1 capm model (daily)\nusing excess returns:\n\\[\nr^{ex}_{j,t} = \\alpha_j + \\beta_j\\, r^{ex}_{m,t} + \\varepsilon_{j,t}\n\\]\nwhere: - \\(r^{ex}_{j,t} = r_{j,t} - r_{f,t}\\) (excess return of the object) - \\(r^{ex}_{m,t} = r_{m,t} - r_{f,t}\\) (market = spy) - \\(\\alpha_j\\) is “alpha” (average unexplained excess return) - \\(\\beta_j\\) is market sensitivity - \\(\\varepsilon\\) is the residual\n\n\nestimation (ols via sklearn)\nOLS (ordinary least squares) is a model for minimizing the square of residuals between the prediction and the real data for fitting a linear regression on our data.\n\\[\n\\min_{\\alpha,\\beta} \\sum_t \\left(r^{ex}_{j,t} - \\alpha - \\beta r^{ex}_{m,t}\\right)^2\n\\]\nthe fitted values are:\n\\[\n\\hat r^{ex}_{j,t} = \\hat\\alpha + \\hat\\beta r^{ex}_{m,t}\n\\]\nand \\(r^2\\) is:\n\\[\nr^2 = 1 - \\frac{\\sum_t (r^{ex}_{j,t} - \\hat r^{ex}_{j,t})^2}{\\sum_t (r^{ex}_{j,t} - \\bar r^{ex}_j)^2}\n\\]\n\n\n9.2 Active risk vs benchmark (tracking error and information ratio)\nWe define active return as performance relative to the benchmark:\n\\[\na_t = r_t - m_t\n\\]\nThen Tracking error (annualized):\n\\[\nTE = \\sqrt{ann}\\;\\sigma(a_t)\n\\]\nInformation ratio (annualized): \\[\nIR = \\sqrt{ann}\\;\\frac{E[a_t]}{\\sigma(a_t)}\n\\]\n\nhigh TE: we deviate a lot from the benchmark (high active risk)\nhigh IR: we’re rewarded well per unit of active risk\n\n\n\n9.3 Up capture and down capture\nThese measure how the object behaves when the market is up vs down.\nLet: - \\(\\mathcal{U}=\\{t: m_t&gt;0\\}\\) (up-market days) - \\(\\mathcal{D}=\\{t: m_t&lt;0\\}\\) (down-market days)\nThen:\n\\[\n\\text{UpCapture}=\\frac{E[r_t\\mid t\\in\\mathcal{U}]}{E[m_t\\mid t\\in\\mathcal{U}]},\n\\qquad\n\\text{DownCapture}=\\frac{E[r_t\\mid t\\in\\mathcal{D}]}{E[m_t\\mid t\\in\\mathcal{D}]}\n\\]\n\nUpCapture &gt; 1: stronger upside participation than the benchmark\nDownCapture &gt; 1: worse downside participation than the benchmark\n&lt; 1 means more calm moves than the benchmark in that regime\n\n\n\n9.4 Systematic variance share\nwe want to see how much of an object’s risk is market risk. A simple CAPM-based approximation of the market-driven share of variance:\n\\[\n\\text{SystematicVarShare} \\approx \\frac{\\beta^2\\,\\text{Var}(m^{ex})}{\\text{Var}(r^{ex})}\n\\]\n\nnear 1: most risk comes from the market\nnear 0: most risk is idiosyncratic (strategy/asset-specific)\n\n\n\nthe regression scatter plot\n\neach dot is a day (\\(x\\) = market excess return, \\(y\\) = object excess return)\nthe fitted line slope is \\(\\beta\\)\nthe intercept is \\(\\alpha\\)\n\n\ncapm_rows = []\nroll_store = {}\n\nm_ex = pd.Series(market_ret, index=base_idx) - rf_daily\n\ndef capm_ols(y, x):\n    xy = pd.concat([pd.Series(y), pd.Series(x)], axis=1).dropna()\n    yv = xy.iloc[:, 0].to_numpy(float)\n    xv = xy.iloc[:, 1].to_numpy(float)\n\n    xmat = np.column_stack([np.ones(len(xv)), xv])\n    coef = np.linalg.lstsq(xmat, yv, rcond=None)[0]\n\n    alpha = float(coef[0])\n    beta = float(coef[1])\n    yhat = xmat @ coef\n\n    ssr = float(((yv - yhat) ** 2).sum())\n    sst = float(((yv - yv.mean()) ** 2).sum())\n    r2 = 1 - ssr / sst if sst &gt; 1e-12 else np.nan\n\n    return alpha, beta, r2\n\ndef rolling_beta_corr(r, m, w):\n    x = pd.concat([pd.Series(r), pd.Series(m)], axis=1).dropna()\n    rp = x.iloc[:, 0]\n    rm = x.iloc[:, 1]\n    beta = rp.rolling(w).cov(rm) / rm.rolling(w).var()\n    corr = rp.rolling(w).corr(rm)\n    beta.name = f\"beta_{w}\"\n    corr.name = f\"corr_{w}\"\n    return beta, corr\n\n\nfor name, r in obj.items():\n    y_ex = pd.Series(r, index=base_idx) - rf_daily\n    xy_ex = pd.concat([m_ex, y_ex], axis=1).dropna()\n    if len(xy_ex) &gt;= 30:\n        x_reg = xy_ex.iloc[:, 0].to_numpy(float).reshape(-1, 1)\n        y_reg = xy_ex.iloc[:, 1].to_numpy(float)\n        reg = LinearRegression().fit(x_reg, y_reg)\n        alpha = float(reg.intercept_)\n        beta = float(reg.coef_[0])\n        r2 = float(reg.score(x_reg, y_reg))\n    else:\n        alpha, beta, r2 = np.nan, np.nan, np.nan\n\n\n    alpha_ann = (1 + alpha) ** ann - 1 if alpha &gt; -0.999 else np.nan\n\n    active = (pd.Series(r, index=base_idx) - pd.Series(market_ret, index=base_idx)).dropna()\n    te = float(active.std(ddof=1) * np.sqrt(ann))\n    ir = float(active.mean() / active.std(ddof=1) * np.sqrt(ann))\n\n    m = pd.Series(market_ret, index=base_idx).dropna()\n    y = pd.Series(r, index=base_idx).dropna()\n    xy = pd.concat([y, m], axis=1).dropna()\n    y_aligned = xy.iloc[:, 0].to_numpy(float)\n    m_aligned = xy.iloc[:, 1].to_numpy(float)\n\n    up_m = m_aligned &gt; 0\n    dn_m = m_aligned &lt; 0\n    up_cap = (np.mean(y_aligned[up_m]) / np.mean(m_aligned[up_m]))\n    dn_cap = (np.mean(y_aligned[dn_m]) / np.mean(m_aligned[dn_m]))\n\n    var_m = float(np.var(m_ex.dropna().to_numpy(float), ddof=1))\n    var_y = float(np.var(y_ex.dropna().to_numpy(float), ddof=1))\n    sys_share = (beta ** 2) * var_m / var_y\n\n    capm_rows.append({\n        \"object\": name,\n        \"alpha_daily\": alpha,\n        \"alpha_ann\": alpha_ann,\n        \"beta\": beta,\n        \"r2\": r2,\n        \"tracking_error\": te,\n        \"information_ratio\": ir,\n        \"up_capture\": up_cap,\n        \"down_capture\": dn_cap,\n        \"systematic_var_share\": sys_share,\n    })\n\n    b126, c126 = rolling_beta_corr(y_ex, m_ex, 126)\n    b252, c252 = rolling_beta_corr(y_ex, m_ex, 252)\n    roll_store[name] = {\"beta_126\": b126, \"corr_126\": c126, \"beta_252\": b252, \"corr_252\": c252}\n\ncapm_tbl = pd.DataFrame(capm_rows).set_index(\"object\").sort_index()\n\n\ndisplay(capm_tbl.round(4))\n\n\n\n\n\n\n\n\nalpha_daily\nalpha_ann\nbeta\nr2\ntracking_error\ninformation_ratio\nup_capture\ndown_capture\nsystematic_var_share\n\n\nobject\n\n\n\n\n\n\n\n\n\n\n\n\n\naapl\n0.0004\n0.1117\n1.2284\n0.5866\n0.1954\n0.6806\n1.2905\n1.1891\n0.5866\n\n\nmaxsharpe_frontier\n0.0003\n0.0829\n0.9622\n0.3637\n0.2355\n0.3193\n1.0845\n1.0116\n0.3637\n\n\nmv_ewma\n0.0002\n0.0458\n0.7316\n0.6213\n0.1167\n0.1114\n0.7570\n0.6969\n0.6213\n\n\nnvda\n0.0014\n0.4093\n1.8388\n0.4598\n0.4000\n1.1068\n2.1876\n1.8877\n0.4598\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(11, 7), sharex=True, sharey=True)\naxes = axes.ravel()\n\nfor i, name in enumerate(obj.keys()):\n    ax = axes[i]\n    y_ex = (pd.Series(obj[name], index=base_idx) - rf_daily).dropna()\n    m_ex = (pd.Series(market_ret, index=base_idx) - rf_daily).dropna()\n    xy = pd.concat([m_ex, y_ex], axis=1).dropna()\n    x = xy.iloc[:, 0].to_numpy(float)\n    y = xy.iloc[:, 1].to_numpy(float)\n\n    alpha = capm_tbl.loc[name, \"alpha_daily\"]\n    beta = capm_tbl.loc[name, \"beta\"]\n    r2 = capm_tbl.loc[name, \"r2\"]\n\n    ax.scatter(x, y, s=10, alpha=0.15, color=palette[5])\n    xs = np.linspace(np.percentile(x, 1), np.percentile(x, 99), 200)\n    ys = alpha + beta * xs\n    ax.plot(xs, ys, lw=2.2, color=obj_colors[name])\n\n    ax.axhline(0.0, color=\"#444\", lw=1)\n    ax.axvline(0.0, color=\"#444\", lw=1)\n    ax.set_title(f\"capm fit — {name}\")\n    ax.set_xlabel(\"market excess return\")\n    ax.set_ylabel(\"asset excess return\")\n    ax.text(\n        0.02, 0.98,\n        f\"alpha(d): {alpha:.4f}\\n\"\n        f\"beta: {beta:.3f}\\n\"\n        f\"r2: {r2:.3f}\",\n        transform=ax.transAxes,\n        va=\"top\",\n        fontsize=9,\n        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.75),\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(11, 6), sharex=True)\n\nfor name in obj.keys():\n    axes[0].plot(roll_store[name][\"beta_126\"], lw=1.2, color=obj_colors[name], label=f\"{name} 126d\")\n    axes[0].plot(roll_store[name][\"beta_252\"], lw=2.0, color=obj_colors[name], alpha=0.6, ls=\"--\", label=f\"{name} 252d\")\n\naxes[0].axhline(1.0, color=\"#444\", lw=1, ls=\"--\")\naxes[0].set_title(\"rolling beta to market (spy)\")\naxes[0].set_ylabel(\"beta\")\naxes[0].legend(ncol=2)\n\nfor name in obj.keys():\n    axes[1].plot(roll_store[name][\"corr_126\"], lw=1.2, color=obj_colors[name], label=f\"{name} 126d\")\n    axes[1].plot(roll_store[name][\"corr_252\"], lw=2.0, color=obj_colors[name], alpha=0.6, ls=\"--\", label=f\"{name} 252d\")\n\naxes[1].axhline(0.0, color=\"#444\", lw=1)\naxes[1].set_title(\"rolling correlation to market (spy)\")\naxes[1].set_ylabel(\"corr\")\naxes[1].set_xlabel(\"date\")\naxes[1].legend(ncol=2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNow we can see the difference between our models and our picked stocks. both of our models have beta under 1 and both the stocks have over 1 beta. this means that the risk of these stocks are more dependent on market risk than the diversified portfolios. all the 4 objects have positive alpha which means they all have more returns that can’t be explained by market return and can be interpreted as outperformance of these objects relative to S&P 500. correlation of all the objects to SPY are high through time and MaxSharpe has some times that has higher than 1 beta, but MV_ewma has less than 1 beta almost all the time and is the only object with less than one up and down capture, but this object moves with market a lot due to correlation and R2 and var share.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#risk-attribution",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#risk-attribution",
    "title": "3. risk analysis and CAPM",
    "section": "10) risk attribution",
    "text": "10) risk attribution\nWe explained this in last notebook. now we repeat it and add another part. for our portfolios we are looking for the assets that have the most share of portfolios risk.\n\n10.1 volatility attribution (covariance-based)\nIf portfolio weights are \\(w\\) and covariance matrix is \\(\\Sigma\\). portfolio volatility is:\n\\[\n\\sigma_p = \\sqrt{w^\\top \\Sigma w}\n\\]\nthe marginal contribution to risk (mrc) of asset \\(i\\) is:\n\\[\n\\text{mrc}_i = \\frac{(\\Sigma w)_i}{\\sigma_p}\n\\]\nthe component contribution is:\n\\[\n\\text{rc}_i = w_i\\,\\text{mrc}_i\n\\]\nand the contributions sum to total volatility:\n\\[\n\\sum_i \\text{rc}_i = \\sigma_p\n\\]\nthis tells us which names drive most of the volatility.\n\n\n10.2 es attribution\nfor expected shortfall at level \\(\\alpha\\), identify the set of tail days:\n\\[\n\\mathcal{t}_\\alpha = \\{t : r_{p,t} \\le q_\\alpha(r_p)\\}\n\\]\na simple scenario-based contribution approximation is:\n\\[\n\\text{es contrib}_i \\approx -\\frac{1}{|\\mathcal{t}_\\alpha|}\\sum_{t \\in \\mathcal{t}_\\alpha} w_i r_{i,t}\n\\]\nso the largest contributors are the positions that lose the most on the worst portfolio days.\n\nport_info = {\n    \"mv_ewma\": (res_mv, \"ewma\"),\n    \"maxsharpe_frontier\": (res_mx, \"ledoitwolf\")}\n\nvol_contrib = {}\nes_contrib = {}\noverlap_rows = []\n\nfor pname, (res, cov_key) in port_info.items():\n    dt = res.weights.index[-1]\n    st = cache[dt]\n\n    tickers = st[\"tickers\"]\n    w = res.weights.loc[dt].reindex(tickers).fillna(0.0).to_numpy(float)\n    w = w / w.sum()\n\n    cov = st[\"cov_ann_map\"][cov_key]\n    port_vol = np.sqrt(float(w @ cov @ w))\n\n    m = cov @ w\n    rc = pd.Series(w * m / port_vol, index=tickers).sort_values(ascending=False)\n    vol_contrib[pname] = rc\n\n    x = st[\"window\"][tickers].to_numpy(float)\n    rp = x @ w\n    q = np.quantile(rp, 0.05)\n    mask = rp &lt;= q\n    esc = pd.Series(-(x[mask] * w).mean(axis=0), index=tickers).sort_values(ascending=False)\n    es_contrib[pname] = esc\n\n    ov = len(set(rc.head(10).index).intersection(set(esc.head(10).index)))\n    overlap_rows.append({\"portfolio\": pname, \"top10_overlap_count\": ov})\n\noverlap_tbl = pd.DataFrame(overlap_rows).set_index(\"portfolio\")\n\n\ndisplay(overlap_tbl)\n\n\n\n\n\n\n\n\ntop10_overlap_count\n\n\nportfolio\n\n\n\n\n\nmv_ewma\n9\n\n\nmaxsharpe_frontier\n9\n\n\n\n\n\n\n\nfor both strategies, 9 of top 10 contributions seem to be the same in Volatility and ES contributions. but the order might be different.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n\nfor ax, pname in zip(axes, port_info.keys()):\n    top = vol_contrib[pname].head(10).sort_values()\n    ax.barh(top.index, top.values)\n    ax.set_title(f\"top 10 volatility contributors — {pname}\")\n    ax.set_xlabel(\"component contribution (ann vol)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 3.5))\n\nfor ax, pname in zip(axes, port_info.keys()):\n    top = es_contrib[pname].head(10).sort_values()\n    ax.barh(top.index, top.values)\n    ax.set_title(f\"top 10 es contributors — {pname}\")\n    ax.set_xlabel(\"scenario-based contribution (daily loss)\")\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#correlation-and-diversification-object-to-object",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#correlation-and-diversification-object-to-object",
    "title": "3. risk analysis and CAPM",
    "section": "13) correlation and diversification (object-to-object)",
    "text": "13) correlation and diversification (object-to-object)\ncorrelation is a core ingredient of diversification. for two return series \\(x_t\\) and \\(y_t\\):\n\\[\n\\rho_{x,y} = \\frac{\\text{cov}(x,y)}{\\sigma_x\\sigma_y}\n\\]\nwe compute the correlation matrix across objects and visualize it as a heatmap.\n\n\\(\\rho \\approx 1\\): objects move together (low diversification benefit)\n\\(\\rho \\approx 0\\): movements are mostly independent\n\\(\\rho &lt; 0\\): objects move in opposite direction, and we can hedge the other in some regimes (so rare in stock market)\n\n\ncorr = pd.DataFrame({k: pd.Series(v) for k, v in obj.items()}).dropna().corr()\n\nfig, ax = plt.subplots(1, 1, figsize=(6.5, 5.5))\nim = ax.imshow(corr.values, vmin=-1, vmax=1, cmap= \"Spectral\")\nax.set_xticks(range(len(corr.columns)))\nax.set_yticks(range(len(corr.index)))\nax.set_xticklabels(corr.columns, rotation=45, ha=\"right\")\nax.set_yticklabels(corr.index)\nax.set_title(\"correlation matrix (daily returns)\")\n\n\nfor i in range(corr.shape[0]):\n    for j in range(corr.shape[1]):\n        ax.text(j, i, f\"{corr.values[i, j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n\nfig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#implementation-with-quantfinlab",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#implementation-with-quantfinlab",
    "title": "3. risk analysis and CAPM",
    "section": "Implementation with Quantfinlab",
    "text": "Implementation with Quantfinlab\nWe can implement this whole project in two ways with our library:\nin this notebook we show both implementation on US market and Hong Kong market\nnow we compare all of our portfolios in the last project based on these risk measures.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#using-mannual-functions-for-risk-measures-and-plots-us-data",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#using-mannual-functions-for-risk-measures-and-plots-us-data",
    "title": "3. risk analysis and CAPM",
    "section": "1) Using mannual functions for risk measures and plots (US data)",
    "text": "1) Using mannual functions for risk measures and plots (US data)\n\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport quantfinlab.portfolio as pf\nimport quantfinlab.risk as rk\nimport quantfinlab.plots as pl\n\nwarnings.filterwarnings(\"ignore\")\n\nrf_annual = 0.04\nrf_daily = (1.0 + rf_annual) ** (1.0 / 252.0) - 1.0\n\ndf = pd.read_parquet(\"../data/nasdaq_all_close_volume.parquet\")\ndf[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"date\"]).sort_values(\"date\")\n\nclose_map, vol_map = {}, {}\nfor c in df.columns:\n    c_str = str(c)\n    if c_str.lower() == \"date\" or \"__\" not in c_str:\n        continue\n    t, f = c_str.rsplit(\"__\", 1)\n    fl = f.lower()\n    if fl == \"close\":\n        close_map[t] = c\n    elif fl == \"volume\":\n        vol_map[t] = c\n\ntickers_all = sorted(set(close_map).intersection(vol_map))\nif len(tickers_all) &lt; 2:\n    raise ValueError(\"Not enough ticker__close/ticker__volume pairs in NASDAQ parquet.\")\n\nclose_prices = df[[close_map[t] for t in tickers_all]].copy()\nvolumes = df[[vol_map[t] for t in tickers_all]].copy()\nclose_prices.columns = tickers_all\nvolumes.columns = tickers_all\nclose_prices.index = pd.to_datetime(df[\"date\"].values)\nvolumes.index = pd.to_datetime(df[\"date\"].values)\n\nstack = pf.build_all_portfolio_strategies(\n    close_prices,\n    volumes,\n    start=\"2016-01-01\",\n    rf_annual=rf_annual,\n    rf_daily=rf_daily,\n)\n\nresults = dict(stack.results)\ncache = dict(stack.cache)\ncov_key_for_rc = dict(stack.cov_key_for_rc)\n\n\ncommon_idx = None\nfor res in results.values():\n    idx_res = pd.DatetimeIndex(res.net_returns.index)\n    common_idx = idx_res if common_idx is None else common_idx.intersection(idx_res)\nif common_idx is None or len(common_idx) == 0:\n    raise ValueError(\"No overlapping index across strategy returns.\")\n\nobjects = {name: res.net_returns.reindex(common_idx).fillna(0.0) for name, res in results.items()}\nobj_colors = pl.make_color_map(objects.keys(), pl.LAB_COLORS)\n\n\nspy = pd.read_csv(\"../data/spy_yfinance.csv\")\nspy[\"date\"] = pd.to_datetime(spy[\"Date\"], errors=\"coerce\")\nspy = spy.dropna(subset=[\"date\"]).sort_values(\"date\").set_index(\"date\")\nif \"Adj Close\" in spy.columns:\n    spy_px = pd.to_numeric(spy[\"Adj Close\"], errors=\"coerce\")\nelif \"Close\" in spy.columns:\n    spy_px = pd.to_numeric(spy[\"Close\"], errors=\"coerce\")\nelse:\n    raise ValueError(\"spy_yfinance.csv missing close/adj close column\")\nmarket_ret = spy_px.pct_change(fill_method=None).reindex(common_idx).fillna(0.0)\n\nportfolios = {\n    name: {\n        \"backtest\": results[name],\n        \"state_cache\": cache,\n        \"cov_key\": cov_key_for_rc[name],\n    }\n    for name in results.keys()\n}\n\n\nperf_tbl = rk.performance_table(objects, rf_daily=rf_daily, annualization=252)\nshape_tbl = rk.tail_shape_table(objects)\ndd_summary_tbl = rk.drawdown_summary_table(objects)\ndd_episodes_tbl = rk.drawdown_episodes_table(objects, top_n=1)\nvar_es_tbl = rk.var_es_table(objects, alpha=0.05, methods=[\"hist\", \"cf\", \"fhs\"])\nvar_bt_methods = [\"hist\", \"cf\", \"fhs\"]\nvar_bt_tbl = rk.var_backtest_table(objects, alpha=0.05, methods=var_bt_methods, lookback=252)\n\nstress_windows = {\n    \"2018_q4\": (\"2018-10-01\", \"2018-12-31\"),\n    \"2020_covid\": (\"2020-02-20\", \"2020-04-30\"),\n    \"2022_inflation\": (\"2022-01-03\", \"2022-10-31\"),\n}\nstress_tbl = rk.stress_table(objects, windows=stress_windows, worst_only=True)\nstress_tbl_full = rk.stress_table(objects, windows=stress_windows, worst_only=False)\n\ncapm_tbl, capm_roll = rk.capm_table(objects, market_ret=market_ret, rf_daily=rf_daily, rolling=[126, 252])\ncorr = rk.corr_matrix(objects)\nvol_rc_tbl, es_rc_tbl, overlap_tbl = rk.attribution_tables(portfolios, es_alpha=0.05, top_k=10)\n\ndisplay(perf_tbl.round(4))\ndisplay(shape_tbl.round(4))\ndisplay(dd_summary_tbl.round(4))\ndisplay(var_es_tbl.round(4))\ndisplay(var_bt_tbl.round(4))\ndisplay(capm_tbl.round(4))\n\nfig, ax = plt.subplots(2, 1, figsize=(11, 7), sharex=True)\npl.plot_nav_compare(ax[0], objects, colors=obj_colors, title=\"Cumulative NAV (all portfolio strategies)\")\npl.plot_drawdown_compare_objects(ax[1], objects, colors=obj_colors, title=\"Drawdown (all portfolio strategies)\")\nplt.tight_layout()\nplt.show()\n\nfig, axes = pl.auto_grid(len(objects), ncols=3, figsize=(13, 8), sharex=True, sharey=True)\nfor a, (name, r) in zip(axes, objects.items()):\n    pl.plot_rolling_vol(a, r, windows=[20, 60, 252], name=name)\npl.turn_off_unused_axes(axes, used=len(objects))\nplt.tight_layout()\nplt.show()\n\nfig, axes = pl.auto_grid(len(objects), ncols=3, figsize=(13, 8), sharex=True, sharey=False)\nfor a, (name, r) in zip(axes, objects.items()):\n    pl.plot_var_backtest(a, r, alpha=0.05, lookback=252, method=\"best\", methods=var_bt_methods, name=name)\npl.turn_off_unused_axes(axes, used=len(objects))\nplt.tight_layout()\nplt.show()\n\nfig, axes = pl.auto_grid(stress_tbl_full.index.nunique(), ncols=3, figsize=(12, 4), sharey=True)\nfor a, w in zip(axes, stress_tbl_full.index.unique()):\n    pl.plot_stress_bar(a, stress_tbl_full, window=w)\npl.turn_off_unused_axes(axes, used=stress_tbl_full.index.nunique())\nplt.tight_layout()\nplt.show()\n\nfig, axes = pl.auto_grid(len(objects), ncols=3, figsize=(13, 8), sharex=True, sharey=True)\nfor a, (name, r) in zip(axes, objects.items()):\n    pl.plot_capm_scatter(a, r, market_ret, rf_daily=rf_daily, name=name, color=obj_colors[name])\npl.turn_off_unused_axes(axes, used=len(objects))\nplt.tight_layout()\nplt.show()\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\npl.plot_rolling_beta_compare(ax[0], capm_roll, window=126)\npl.plot_rolling_beta_compare(ax[1], capm_roll, window=252)\nplt.tight_layout()\nplt.show()\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\npl.plot_corr_heatmap(ax, corr)\nplt.tight_layout()\nplt.show()\n\nnames = list(portfolios.keys())\nfig, axes = plt.subplots(2, len(names), figsize=(3.8 * len(names), 7))\nif len(names) == 1:\n    axes = axes.reshape(2, 1)\nfor j, pname in enumerate(names):\n    pl.plot_top_contrib(axes[0, j], vol_rc_tbl.loc[pname], title=f\"{pname} - top vol rc\", k=10)\n    pl.plot_top_contrib(axes[1, j], es_rc_tbl.loc[pname], title=f\"{pname} - top es rc\", k=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nann_return\nann_vol\nsharpe\nsortino\n\n\nobject\n\n\n\n\n\n\n\n\new\n0.1464\n0.2530\n0.5119\n0.7195\n\n\nmaxsharpe_frontier\n0.2092\n0.2949\n0.6590\n0.9431\n\n\nmaxsharpe_slsqp\n0.2024\n0.3161\n0.6175\n0.8760\n\n\nminvar_ewma\n0.1332\n0.1477\n0.6552\n0.9331\n\n\nminvar_lw\n0.1329\n0.1535\n0.6344\n0.9059\n\n\nminvar_oas\n0.1423\n0.1530\n0.6903\n0.9910\n\n\nminvar_sample\n0.1520\n0.1512\n0.7524\n1.0898\n\n\nmv_ewma\n0.1708\n0.1830\n0.7393\n1.0615\n\n\nmv_lw\n0.1462\n0.2001\n0.5864\n0.8235\n\n\nmv_oas\n0.1413\n0.1971\n0.5703\n0.8035\n\n\nmv_sample\n0.1484\n0.1935\n0.6096\n0.8608\n\n\nridge_mv\n0.1382\n0.1980\n0.5553\n0.7769\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskew\nexcess_kurtosis\ntail_ratio_95_05\nworst_1d\nworst_5d_avg\nworst_10d_avg\n\n\nobject\n\n\n\n\n\n\n\n\n\n\new\n-0.1442\n6.5229\n0.8626\n-0.1239\n-0.0852\n-0.0729\n\n\nmaxsharpe_frontier\n-0.1042\n6.9172\n1.0044\n-0.1203\n-0.1067\n-0.0904\n\n\nmaxsharpe_slsqp\n-0.1347\n5.6136\n0.9442\n-0.1403\n-0.1056\n-0.0903\n\n\nminvar_ewma\n-0.2237\n15.7958\n1.0361\n-0.0879\n-0.0686\n-0.0549\n\n\nminvar_lw\n-0.2048\n20.0543\n1.0072\n-0.1031\n-0.0717\n-0.0569\n\n\nminvar_oas\n-0.1550\n20.2887\n1.0248\n-0.1021\n-0.0717\n-0.0568\n\n\nminvar_sample\n-0.0546\n19.5492\n1.0468\n-0.0962\n-0.0708\n-0.0561\n\n\nmv_ewma\n-0.1893\n11.7377\n1.0380\n-0.1067\n-0.0762\n-0.0613\n\n\nmv_lw\n-0.2516\n11.2377\n0.9091\n-0.1194\n-0.0776\n-0.0639\n\n\nmv_oas\n-0.1621\n11.4696\n0.9020\n-0.1151\n-0.0757\n-0.0629\n\n\nmv_sample\n-0.2380\n10.9302\n0.9371\n-0.1144\n-0.0742\n-0.0625\n\n\nridge_mv\n-0.2658\n11.2937\n0.8839\n-0.1177\n-0.0776\n-0.0626\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax_dd\nlongest_dd_days\navg_recovery_days\nulcer_index\n\n\nobject\n\n\n\n\n\n\n\n\new\n-0.4608\n771\n18.1759\n0.1689\n\n\nmaxsharpe_frontier\n-0.4764\n1238\n30.5072\n0.2400\n\n\nmaxsharpe_slsqp\n-0.5730\n1153\n28.5616\n0.2750\n\n\nminvar_ewma\n-0.2339\n613\n15.0547\n0.0518\n\n\nminvar_lw\n-0.2791\n312\n14.6493\n0.0495\n\n\nminvar_oas\n-0.2787\n288\n13.5208\n0.0456\n\n\nminvar_sample\n-0.2797\n304\n12.0562\n0.0459\n\n\nmv_ewma\n-0.3087\n701\n15.1034\n0.0822\n\n\nmv_lw\n-0.3128\n695\n16.5981\n0.1080\n\n\nmv_oas\n-0.3063\n620\n16.5981\n0.0994\n\n\nmv_sample\n-0.3098\n609\n17.9910\n0.0937\n\n\nridge_mv\n-0.3116\n621\n16.8019\n0.1024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist_var5\nhist_es5\ncf_var5\ncf_es5\nfhs_var5\nfhs_es5\n\n\nobject\n\n\n\n\n\n\n\n\n\n\new\n0.0267\n0.0377\n0.0241\n0.0490\n0.0197\n0.0281\n\n\nmaxsharpe_frontier\n0.0281\n0.0446\n0.0276\n0.0577\n0.0453\n0.0679\n\n\nmaxsharpe_slsqp\n0.0317\n0.0480\n0.0303\n0.0583\n0.0389\n0.0537\n\n\nminvar_ewma\n0.0126\n0.0213\n0.0124\n0.0417\n0.0068\n0.0109\n\n\nminvar_lw\n0.0129\n0.0218\n0.0120\n0.0494\n0.0083\n0.0123\n\n\nminvar_oas\n0.0127\n0.0215\n0.0117\n0.0493\n0.0088\n0.0128\n\n\nminvar_sample\n0.0124\n0.0210\n0.0114\n0.0471\n0.0088\n0.0132\n\n\nmv_ewma\n0.0161\n0.0262\n0.0162\n0.0444\n0.0133\n0.0200\n\n\nmv_lw\n0.0192\n0.0306\n0.0181\n0.0481\n0.0102\n0.0147\n\n\nmv_oas\n0.0191\n0.0300\n0.0175\n0.0474\n0.0099\n0.0144\n\n\nmv_sample\n0.0182\n0.0289\n0.0175\n0.0459\n0.0099\n0.0143\n\n\nridge_mv\n0.0190\n0.0305\n0.0180\n0.0478\n0.0097\n0.0142\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreach_count\nbreach_rate\ncoverage_error\nabs_coverage_error\nlongest_breach_streak\navg_gap_days\nkupiec_p\nchristoffersen_p\nquantile_loss\naccuracy_rank\naccuracy_score\nis_best\n\n\nobject\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\new\ncf\n133\n0.0662\n0.0162\n0.0162\n3\n15.1515\n0.0015\n0.0030\n0.0020\n3.0\n0.0769\nFalse\n\n\nfhs\n118\n0.0587\n0.0087\n0.0087\n3\n17.0940\n0.0801\n0.0268\n0.0018\n1.0\n0.2000\nTrue\n\n\nhist\n128\n0.0637\n0.0137\n0.0137\n3\n15.4409\n0.0067\n0.0033\n0.0020\n2.0\n0.1111\nFalse\n\n\nmaxsharpe_frontier\ncf\n120\n0.0597\n0.0097\n0.0097\n3\n16.6303\n0.0519\n0.0021\n0.0023\n3.0\n0.0909\nFalse\n\n\nfhs\n106\n0.0528\n0.0028\n0.0028\n2\n18.8190\n0.5732\n0.0013\n0.0021\n1.0\n0.1667\nTrue\n\n\nhist\n116\n0.0577\n0.0077\n0.0077\n3\n17.2087\n0.1199\n0.0003\n0.0023\n2.0\n0.1000\nFalse\n\n\nmaxsharpe_slsqp\ncf\n138\n0.0687\n0.0187\n0.0187\n3\n14.5547\n0.0003\n0.0029\n0.0024\n2.0\n0.0909\nFalse\n\n\nfhs\n113\n0.0562\n0.0062\n0.0062\n3\n17.5179\n0.2074\n0.0048\n0.0023\n1.0\n0.2000\nTrue\n\n\nhist\n129\n0.0642\n0.0142\n0.0142\n3\n15.5781\n0.0050\n0.0014\n0.0024\n2.0\n0.0909\nFalse\n\n\nminvar_ewma\ncf\n109\n0.0543\n0.0043\n0.0043\n3\n17.8889\n0.3877\n0.0002\n0.0012\n2.0\n0.1250\nFalse\n\n\nfhs\n114\n0.0567\n0.0067\n0.0067\n3\n17.6991\n0.1741\n0.0006\n0.0011\n1.0\n0.1429\nTrue\n\n\nhist\n118\n0.0587\n0.0087\n0.0087\n3\n16.5128\n0.0801\n0.0000\n0.0012\n3.0\n0.0833\nFalse\n\n\nminvar_lw\ncf\n100\n0.0498\n-0.0002\n0.0002\n2\n19.6465\n0.9632\n0.0043\n0.0012\n2.0\n0.1250\nFalse\n\n\nfhs\n111\n0.0553\n0.0053\n0.0053\n2\n18.0727\n0.2879\n0.0583\n0.0011\n1.0\n0.1429\nTrue\n\n\nhist\n114\n0.0567\n0.0067\n0.0067\n4\n17.2124\n0.1741\n0.0000\n0.0012\n3.0\n0.0833\nFalse\n\n\nminvar_oas\ncf\n101\n0.0503\n0.0003\n0.0003\n2\n19.4500\n0.9551\n0.0016\n0.0012\n2.0\n0.1250\nFalse\n\n\nfhs\n112\n0.0557\n0.0057\n0.0057\n2\n17.9099\n0.2453\n0.0282\n0.0011\n1.0\n0.1429\nTrue\n\n\nhist\n116\n0.0577\n0.0077\n0.0077\n3\n16.9130\n0.1199\n0.0001\n0.0012\n3.0\n0.0833\nFalse\n\n\nminvar_sample\ncf\n104\n0.0518\n0.0018\n0.0018\n3\n18.8835\n0.7178\n0.0002\n0.0011\n2.0\n0.1250\nFalse\n\n\nfhs\n108\n0.0538\n0.0038\n0.0038\n2\n18.5794\n0.4449\n0.0159\n0.0011\n1.0\n0.1429\nTrue\n\n\nhist\n111\n0.0553\n0.0053\n0.0053\n4\n17.6818\n0.2879\n0.0001\n0.0011\n3.0\n0.0833\nFalse\n\n\nmv_ewma\ncf\n107\n0.0533\n0.0033\n0.0033\n2\n18.2642\n0.5068\n0.0001\n0.0014\n1.0\n0.1429\nTrue\n\n\nfhs\n110\n0.0548\n0.0048\n0.0048\n2\n17.9817\n0.3353\n0.0028\n0.0013\n1.0\n0.1429\nFalse\n\n\nhist\n112\n0.0557\n0.0057\n0.0057\n3\n16.9820\n0.2453\n0.0000\n0.0014\n3.0\n0.0769\nFalse\n\n\nmv_lw\ncf\n115\n0.0572\n0.0072\n0.0072\n3\n17.5439\n0.1450\n0.0067\n0.0016\n3.0\n0.1000\nFalse\n\n\nfhs\n110\n0.0548\n0.0048\n0.0048\n2\n18.3486\n0.3353\n0.0009\n0.0015\n2.0\n0.1111\nFalse\n\n\nhist\n105\n0.0523\n0.0023\n0.0023\n3\n17.4038\n0.6437\n0.0011\n0.0016\n1.0\n0.1250\nTrue\n\n\nmv_oas\ncf\n119\n0.0592\n0.0092\n0.0092\n3\n16.9492\n0.0647\n0.0048\n0.0016\n3.0\n0.1000\nFalse\n\n\nfhs\n112\n0.0557\n0.0057\n0.0057\n3\n18.0180\n0.2453\n0.0004\n0.0015\n2.0\n0.1111\nFalse\n\n\nhist\n103\n0.0513\n0.0013\n0.0013\n3\n17.7451\n0.7949\n0.0023\n0.0016\n1.0\n0.1250\nTrue\n\n\nmv_sample\ncf\n122\n0.0607\n0.0107\n0.0107\n4\n16.2975\n0.0326\n0.0010\n0.0015\n3.0\n0.0833\nFalse\n\n\nfhs\n110\n0.0548\n0.0048\n0.0048\n3\n18.0917\n0.3353\n0.0028\n0.0014\n1.0\n0.2000\nTrue\n\n\nhist\n112\n0.0557\n0.0057\n0.0057\n4\n16.9820\n0.2453\n0.0013\n0.0016\n2.0\n0.1000\nFalse\n\n\nridge_mv\ncf\n119\n0.0592\n0.0092\n0.0092\n3\n16.9492\n0.0647\n0.0048\n0.0016\n3.0\n0.0909\nFalse\n\n\nfhs\n110\n0.0548\n0.0048\n0.0048\n2\n18.3486\n0.3353\n0.0080\n0.0015\n1.0\n0.1429\nTrue\n\n\nhist\n106\n0.0528\n0.0028\n0.0028\n3\n17.2381\n0.5732\n0.0041\n0.0016\n2.0\n0.1111\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalpha_daily\nalpha_ann\nbeta\nr2\ntracking_error\ninformation_ratio\nup_capture\ndown_capture\nsystematic_var_share\n\n\nobject\n\n\n\n\n\n\n\n\n\n\n\n\n\new\n-0.0001\n-0.0156\n1.2248\n0.8019\n0.1200\n0.0913\n1.2875\n1.3259\n0.8019\n\n\nmaxsharpe_frontier\n0.0003\n0.0839\n0.9599\n0.3625\n0.2356\n0.3218\n1.0839\n1.0101\n0.3625\n\n\nmaxsharpe_slsqp\n0.0002\n0.0632\n1.1294\n0.4368\n0.2384\n0.3215\n1.1774\n1.1185\n0.4368\n\n\nminvar_ewma\n0.0001\n0.0251\n0.6072\n0.5787\n0.1203\n-0.1812\n0.5900\n0.5404\n0.5787\n\n\nminvar_lw\n0.0001\n0.0186\n0.6660\n0.6442\n0.1105\n-0.1917\n0.6303\n0.5873\n0.6442\n\n\nminvar_oas\n0.0001\n0.0285\n0.6532\n0.6241\n0.1136\n-0.1142\n0.6181\n0.5636\n0.6241\n\n\nminvar_sample\n0.0002\n0.0398\n0.6304\n0.5949\n0.1180\n-0.0406\n0.5989\n0.5314\n0.5949\n\n\nmv_ewma\n0.0002\n0.0444\n0.7751\n0.6137\n0.1211\n0.1384\n0.8011\n0.7445\n0.6137\n\n\nmv_lw\n0.0000\n0.0074\n0.9279\n0.7357\n0.1038\n-0.0115\n0.9232\n0.9096\n0.7357\n\n\nmv_oas\n0.0000\n0.0038\n0.9168\n0.7400\n0.1017\n-0.0602\n0.9104\n0.9002\n0.7400\n\n\nmv_sample\n0.0001\n0.0142\n0.8754\n0.7007\n0.1083\n-0.0058\n0.8727\n0.8493\n0.7007\n\n\nridge_mv\n-0.0000\n-0.0020\n0.9442\n0.7785\n0.0937\n-0.0921\n0.9392\n0.9369\n0.7785",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  },
  {
    "objectID": "notebooks/03_risk_report_engine_and_CAPM.html#using-risk-report-function-for-faster-results-hkex-data",
    "href": "notebooks/03_risk_report_engine_and_CAPM.html#using-risk-report-function-for-faster-results-hkex-data",
    "title": "3. risk analysis and CAPM",
    "section": "2) Using Risk report function for faster results (HKEX data)",
    "text": "2) Using Risk report function for faster results (HKEX data)\nthe data used in this part can be downloaded from here (Stooq HKEX daily market data)\nfor CAPM benchmark we use Equal Weights portfolio here\n\nimport warnings\n\nimport pandas as pd\n\nimport quantfinlab.portfolio as pf\nimport quantfinlab.risk as rk\n\nwarnings.filterwarnings(\"ignore\")\n\nrf_annual = 0.04\nrf_daily = (1.0 + rf_annual) ** (1.0 / 252.0) - 1.0\n\n\nraw = pd.read_csv(\"../data/hkex_selected_close_volume.csv\", header=[0, 1], low_memory=False)\nraw.columns = pd.MultiIndex.from_tuples([(str(a).strip(), str(b).strip()) for a, b in raw.columns])\n\ndate_cols = [c for c in raw.columns if c[0].lower() == \"date\"]\nclose_cols = [c for c in raw.columns if c[1].lower() == \"close\"]\nvolume_cols = [c for c in raw.columns if c[1].lower() == \"volume\"]\n\n\ndates = pd.to_datetime(raw[date_cols[0]], errors=\"coerce\")\n\nclose_prices_hk = raw.loc[:, close_cols].copy()\nvolumes_hk = raw.loc[:, volume_cols].copy()\nclose_prices_hk.columns = [str(c[0]).strip() for c in close_cols]\nvolumes_hk.columns = [str(c[0]).strip() for c in volume_cols]\n\nif close_prices_hk.columns.duplicated().any():\n    close_prices_hk = close_prices_hk.T.groupby(level=0).last().T\nif volumes_hk.columns.duplicated().any():\n    volumes_hk = volumes_hk.T.groupby(level=0).last().T\n\nclose_prices_hk.index = dates\nvolumes_hk.index = dates\nstack_hk = pf.build_all_portfolio_strategies(\n    close_prices_hk,\n    volumes_hk,\n    start=\"2016-01-01\",\n    rf_annual=rf_annual,\n    rf_daily=rf_daily,\n)\n\nresults_hk = dict(stack_hk.results)\ncache_hk = dict(stack_hk.cache)\ncov_key_for_rc_hk = dict(stack_hk.cov_key_for_rc)\n\ncommon_idx = None\nfor res in results_hk.values():\n    idx_res = pd.DatetimeIndex(res.net_returns.index)\n    common_idx = idx_res if common_idx is None else common_idx.intersection(idx_res)\nif common_idx is None or len(common_idx) == 0:\n    raise ValueError(\"No overlapping index across HKEX strategy returns.\")\n\nobjects_hk = {name: res.net_returns.reindex(common_idx).fillna(0.0) for name, res in results_hk.items()}\nmarket_proxy = objects_hk[\"ew\"] if \"ew\" in objects_hk else next(iter(objects_hk.values()))\n\nportfolios_hk = {\n    name: {\n        \"backtest\": results_hk[name],\n        \"state_cache\": cache_hk,\n        \"cov_key\": cov_key_for_rc_hk[name],\n    }\n    for name in results_hk.keys()\n}\n\nreport_hk = rk.risk_report(\n    objects=objects_hk,\n    market_ret=market_proxy,\n    rf_daily=rf_daily,\n    portfolios=portfolios_hk,\n    include={\n        \"performance_tables\": True,\n        \"shape_tables\": True,\n        \"drawdowns\": True,\n        \"drawdown_episodes\": True,\n        \"var_es\": True,\n        \"var_backtest\": True,\n        \"stress\": True,\n        \"capm\": True,\n        \"rolling_beta\": True,\n        \"correlation\": True,\n        \"attribution\": True,\n        \"exec_bullets\": True,\n    },\n    var_settings={\"alpha\": 0.05, \"methods\": [\"hist\", \"cf\", \"fhs\"], \"lookback\": 252},\n    backtest_settings={\"alpha\": 0.05, \"methods\": [\"hist\", \"cf\", \"fhs\"], \"lookback\": 252, \"plot_method\": \"best\"},\n    rolling_settings={\"vol_windows\": [20, 60, 252], \"beta_windows\": [126, 252]},\n    stress_settings={\n        \"windows\": {\n            \"2018_q4\": (\"2018-10-01\", \"2018-12-31\"),\n            \"2020_covid\": (\"2020-02-20\", \"2020-04-30\"),\n            \"2022_inflation\": (\"2022-01-03\", \"2022-10-31\"),\n        },\n        \"worst_only\": True,\n        \"worst_by\": \"cum_return\",\n    },\n    attribution_settings={\"es_alpha\": 0.05, \"top_k\": 5},\n    layout={\"ncols\": 4, \"sharex\": True, \"sharey\": True},\n    output={\n        \"round_tables\": 4,\n        \"print_exec_bullets\": True,\n        \"display_tables\": True,\n        \"hide_table_keys\": [\n            \"drawdown_episodes\",\n            \"stress\",\n            \"corr\",\n            \"attribution_overlap\",\n            \"attribution_vol\",\n            \"attribution_es\",\n        ],\n        \"show_figures\": True,\n    },\n)\n\nprint(\"HKEX strategies in report:\", sorted(objects_hk.keys()))\nprint(\"HKEX report tables:\", sorted(report_hk.tables.keys()))\n\n\n\n\n\n\n\n\nann_return\nann_vol\nsharpe\nsortino\n\n\nobject\n\n\n\n\n\n\n\n\new\n0.0766\n0.2136\n0.2690\n0.3744\n\n\nmaxsharpe_frontier\n0.1726\n0.2530\n0.6028\n0.8270\n\n\nmaxsharpe_slsqp\n0.1334\n0.2558\n0.4649\n0.6455\n\n\nminvar_ewma\n0.0807\n0.1325\n0.3561\n0.4822\n\n\nminvar_lw\n0.0672\n0.1266\n0.2672\n0.3616\n\n\nminvar_oas\n0.0681\n0.1261\n0.2747\n0.3720\n\n\nminvar_sample\n0.0702\n0.1260\n0.2902\n0.3936\n\n\nmv_ewma\n0.0478\n0.1552\n0.1262\n0.1704\n\n\nmv_lw\n0.0597\n0.1744\n0.1952\n0.2679\n\n\nmv_oas\n0.0602\n0.1727\n0.1978\n0.2712\n\n\nmv_sample\n0.0560\n0.1700\n0.1753\n0.2401\n\n\nridge_mv\n0.0592\n0.1769\n0.1919\n0.2638\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskew\nexcess_kurtosis\ntail_ratio_95_05\nworst_1d\nworst_5d_avg\nworst_10d_avg\n\n\nobject\n\n\n\n\n\n\n\n\n\n\new\n-0.4398\n7.1888\n0.9910\n-0.1245\n-0.0801\n-0.0647\n\n\nmaxsharpe_frontier\n-1.2429\n15.7970\n1.0581\n-0.2106\n-0.1013\n-0.0803\n\n\nmaxsharpe_slsqp\n-0.6185\n5.5660\n1.0212\n-0.1575\n-0.0868\n-0.0738\n\n\nminvar_ewma\n-0.9958\n9.1113\n1.1044\n-0.0780\n-0.0601\n-0.0487\n\n\nminvar_lw\n-0.8338\n6.6360\n1.0077\n-0.0685\n-0.0532\n-0.0438\n\n\nminvar_oas\n-0.8208\n6.5202\n1.0117\n-0.0667\n-0.0527\n-0.0436\n\n\nminvar_sample\n-0.8168\n6.5715\n1.0203\n-0.0659\n-0.0524\n-0.0436\n\n\nmv_ewma\n-0.6173\n4.4727\n0.9296\n-0.0804\n-0.0533\n-0.0472\n\n\nmv_lw\n-0.4660\n4.4792\n0.9760\n-0.0862\n-0.0625\n-0.0530\n\n\nmv_oas\n-0.4705\n4.3553\n0.9708\n-0.0840\n-0.0618\n-0.0525\n\n\nmv_sample\n-0.4746\n4.2779\n0.9699\n-0.0810\n-0.0610\n-0.0520\n\n\nridge_mv\n-0.4230\n4.3373\n0.9688\n-0.0825\n-0.0627\n-0.0535\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax_dd\nlongest_dd_days\navg_recovery_days\nulcer_index\n\n\nobject\n\n\n\n\n\n\n\n\new\n-0.4001\n992\n45.5870\n0.1557\n\n\nmaxsharpe_frontier\n-0.3966\n1027\n30.5294\n0.1539\n\n\nmaxsharpe_slsqp\n-0.3886\n833\n30.0455\n0.1761\n\n\nminvar_ewma\n-0.2709\n1058\n23.6512\n0.0865\n\n\nminvar_lw\n-0.3201\n1376\n31.2121\n0.1212\n\n\nminvar_oas\n-0.3203\n1369\n30.7761\n0.1200\n\n\nminvar_sample\n-0.3156\n1341\n26.9605\n0.1147\n\n\nmv_ewma\n-0.4324\n1824\n64.3333\n0.2044\n\n\nmv_lw\n-0.3501\n1006\n50.2143\n0.1433\n\n\nmv_oas\n-0.3496\n989\n50.1905\n0.1429\n\n\nmv_sample\n-0.3517\n1009\n51.4878\n0.1437\n\n\nridge_mv\n-0.3434\n1021\n51.5122\n0.1364\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist_var5\nhist_es5\ncf_var5\ncf_es5\nfhs_var5\nfhs_es5\n\n\nobject\n\n\n\n\n\n\n\n\n\n\new\n0.0203\n0.0299\n0.0214\n0.0445\n0.0123\n0.0179\n\n\nmaxsharpe_frontier\n0.0241\n0.0374\n0.0255\n0.0747\n0.0263\n0.0384\n\n\nmaxsharpe_slsqp\n0.0248\n0.0368\n0.0268\n0.0501\n0.0199\n0.0279\n\n\nminvar_ewma\n0.0117\n0.0194\n0.0141\n0.0308\n0.0083\n0.0123\n\n\nminvar_lw\n0.0121\n0.0186\n0.0135\n0.0264\n0.0084\n0.0127\n\n\nminvar_oas\n0.0119\n0.0186\n0.0135\n0.0262\n0.0078\n0.0120\n\n\nminvar_sample\n0.0117\n0.0185\n0.0134\n0.0262\n0.0078\n0.0117\n\n\nmv_ewma\n0.0157\n0.0234\n0.0166\n0.0289\n0.0089\n0.0129\n\n\nmv_lw\n0.0170\n0.0252\n0.0182\n0.0320\n0.0100\n0.0147\n\n\nmv_oas\n0.0170\n0.0250\n0.0181\n0.0315\n0.0101\n0.0147\n\n\nmv_sample\n0.0167\n0.0246\n0.0178\n0.0309\n0.0102\n0.0148\n\n\nridge_mv\n0.0173\n0.0256\n0.0184\n0.0321\n0.0101\n0.0147\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbreach_count\nbreach_rate\ncoverage_error\nabs_coverage_error\nlongest_breach_streak\navg_gap_days\nkupiec_p\nchristoffersen_p\nquantile_loss\naccuracy_rank\naccuracy_score\nis_best\n\n\nobject\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\new\ncf\n91\n0.0464\n-0.0036\n0.0036\n2\n21.3000\n0.4537\n0.0295\n0.0016\n2.0\n0.1000\nFalse\n\n\nfhs\n98\n0.0499\n-0.0001\n0.0001\n3\n19.9278\n0.9876\n0.0769\n0.0016\n1.0\n0.2000\nTrue\n\n\nhist\n109\n0.0555\n0.0055\n0.0055\n2\n17.9074\n0.2692\n0.0205\n0.0016\n3.0\n0.0833\nFalse\n\n\nmaxsharpe_frontier\ncf\n106\n0.0540\n0.0040\n0.0040\n3\n18.2476\n0.4220\n0.3404\n0.0019\n2.0\n0.1250\nFalse\n\n\nfhs\n107\n0.0545\n0.0045\n0.0045\n3\n18.0755\n0.3660\n0.3667\n0.0019\n1.0\n0.1429\nTrue\n\n\nhist\n121\n0.0616\n0.0116\n0.0116\n2\n15.9667\n0.0222\n0.3102\n0.0019\n3.0\n0.0833\nFalse\n\n\nmaxsharpe_slsqp\ncf\n94\n0.0479\n-0.0021\n0.0021\n2\n20.3441\n0.6652\n0.1150\n0.0019\n1.0\n0.1111\nTrue\n\n\nfhs\n109\n0.0555\n0.0055\n0.0055\n2\n17.8981\n0.2692\n0.4228\n0.0019\n1.0\n0.1111\nFalse\n\n\nhist\n104\n0.0530\n0.0030\n0.0030\n3\n18.7670\n0.5483\n0.1468\n0.0019\n1.0\n0.1111\nFalse\n\n\nminvar_ewma\ncf\n90\n0.0458\n-0.0042\n0.0042\n3\n21.7303\n0.3923\n0.0025\n0.0011\n1.0\n0.1429\nTrue\n\n\nfhs\n112\n0.0571\n0.0071\n0.0071\n4\n17.4234\n0.1603\n0.0005\n0.0010\n2.0\n0.1000\nFalse\n\n\nhist\n108\n0.0550\n0.0050\n0.0050\n3\n18.0748\n0.3151\n0.0001\n0.0011\n2.0\n0.1000\nFalse\n\n\nminvar_lw\ncf\n96\n0.0489\n-0.0011\n0.0011\n3\n20.3579\n0.8232\n0.0570\n0.0010\n1.0\n0.1429\nTrue\n\n\nfhs\n112\n0.0571\n0.0071\n0.0071\n3\n17.4234\n0.1603\n0.0313\n0.0010\n3.0\n0.0909\nFalse\n\n\nhist\n111\n0.0565\n0.0065\n0.0065\n3\n17.5818\n0.1920\n0.0106\n0.0010\n2.0\n0.1111\nFalse\n\n\nminvar_oas\ncf\n94\n0.0479\n-0.0021\n0.0021\n3\n20.7312\n0.6652\n0.0442\n0.0010\n1.0\n0.1250\nTrue\n\n\nfhs\n111\n0.0565\n0.0065\n0.0065\n3\n17.5818\n0.1920\n0.0638\n0.0010\n3.0\n0.1000\nFalse\n\n\nhist\n103\n0.0525\n0.0025\n0.0025\n3\n18.9608\n0.6182\n0.0221\n0.0010\n2.0\n0.1111\nFalse\n\n\nminvar_sample\ncf\n94\n0.0479\n-0.0021\n0.0021\n3\n20.7312\n0.6652\n0.0442\n0.0010\n3.0\n0.0909\nFalse\n\n\nfhs\n113\n0.0576\n0.0076\n0.0076\n3\n17.2679\n0.1328\n0.1662\n0.0010\n2.0\n0.1000\nFalse\n\n\nhist\n99\n0.0504\n0.0004\n0.0004\n3\n19.7347\n0.9300\n0.0815\n0.0010\n1.0\n0.1667\nTrue\n\n\nmv_ewma\ncf\n92\n0.0469\n-0.0031\n0.0031\n2\n21.1868\n0.5200\n0.0338\n0.0012\n3.0\n0.0769\nFalse\n\n\nfhs\n101\n0.0515\n0.0015\n0.0015\n2\n19.3300\n0.7689\n0.1076\n0.0012\n2.0\n0.1250\nFalse\n\n\nhist\n98\n0.0499\n-0.0001\n0.0001\n2\n19.9278\n0.9876\n0.1721\n0.0012\n1.0\n0.1667\nTrue\n\n\nmv_lw\ncf\n88\n0.0448\n-0.0052\n0.0052\n2\n22.1494\n0.2850\n0.1430\n0.0013\n3.0\n0.0833\nFalse\n\n\nfhs\n102\n0.0520\n0.0020\n0.0020\n2\n19.1386\n0.6919\n0.0522\n0.0013\n1.0\n0.1429\nTrue\n\n\nhist\n108\n0.0550\n0.0050\n0.0050\n2\n18.0654\n0.3151\n0.3942\n0.0013\n2.0\n0.1250\nFalse\n\n\nmv_oas\ncf\n88\n0.0448\n-0.0052\n0.0052\n2\n22.1494\n0.2850\n0.1430\n0.0013\n3.0\n0.0833\nFalse\n\n\nfhs\n102\n0.0520\n0.0020\n0.0020\n2\n19.1386\n0.6919\n0.1197\n0.0013\n1.0\n0.1429\nTrue\n\n\nhist\n107\n0.0545\n0.0045\n0.0045\n2\n18.2358\n0.3660\n0.3667\n0.0013\n2.0\n0.1250\nFalse\n\n\nmv_sample\ncf\n92\n0.0469\n-0.0031\n0.0031\n2\n21.1868\n0.5200\n0.0874\n0.0013\n2.0\n0.1000\nFalse\n\n\nfhs\n104\n0.0530\n0.0030\n0.0030\n2\n18.7670\n0.5483\n0.0667\n0.0013\n1.0\n0.1429\nTrue\n\n\nhist\n105\n0.0535\n0.0035\n0.0035\n2\n18.5865\n0.4828\n0.3152\n0.0013\n2.0\n0.1000\nFalse\n\n\nridge_mv\ncf\n91\n0.0464\n-0.0036\n0.0036\n2\n21.4111\n0.4537\n0.0825\n0.0014\n2.0\n0.1000\nFalse\n\n\nfhs\n104\n0.0530\n0.0030\n0.0030\n2\n18.7670\n0.5483\n0.0667\n0.0013\n1.0\n0.1429\nTrue\n\n\nhist\n107\n0.0545\n0.0045\n0.0045\n2\n18.2358\n0.3660\n0.3667\n0.0014\n2.0\n0.1000\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalpha_daily\nalpha_ann\nbeta\nr2\ntracking_error\ninformation_ratio\nup_capture\ndown_capture\nsystematic_var_share\n\n\nobject\n\n\n\n\n\n\n\n\n\n\n\n\n\new\n0.0000\n0.0000\n1.0000\n1.0000\nNaN\nNaN\n1.0000\n1.0000\n1.0000\n\n\nmaxsharpe_frontier\n0.0004\n0.1107\n0.8266\n0.4869\n0.1850\n0.5140\n0.9263\n0.8392\n0.4869\n\n\nmaxsharpe_slsqp\n0.0003\n0.0661\n0.9564\n0.6377\n0.1542\n0.3985\n1.0391\n0.9900\n0.6377\n\n\nminvar_ewma\n0.0001\n0.0209\n0.4606\n0.5514\n0.1454\n-0.0707\n0.4837\n0.4499\n0.5514\n\n\nminvar_lw\n0.0000\n0.0072\n0.4642\n0.6134\n0.1389\n-0.1701\n0.4800\n0.4573\n0.6134\n\n\nminvar_oas\n0.0000\n0.0086\n0.4545\n0.5922\n0.1416\n-0.1610\n0.4709\n0.4467\n0.5922\n\n\nminvar_sample\n0.0000\n0.0112\n0.4432\n0.5648\n0.1451\n-0.1440\n0.4590\n0.4322\n0.5648\n\n\nmv_ewma\n-0.0001\n-0.0163\n0.6262\n0.7430\n0.1121\n-0.3379\n0.6507\n0.6542\n0.7430\n\n\nmv_lw\n-0.0000\n-0.0092\n0.7542\n0.8529\n0.0850\n-0.2752\n0.7767\n0.7783\n0.8529\n\n\nmv_oas\n-0.0000\n-0.0085\n0.7439\n0.8459\n0.0871\n-0.2674\n0.7677\n0.7684\n0.8459\n\n\nmv_sample\n-0.0000\n-0.0121\n0.7312\n0.8439\n0.0884\n-0.3129\n0.7542\n0.7575\n0.8439\n\n\nridge_mv\n-0.0000\n-0.0104\n0.7729\n0.8708\n0.0800\n-0.2939\n0.7918\n0.7947\n0.8708\n\n\n\n\n\n\n\n- maxsharpe_frontier has the highest realized Sharpe ratio.\n- Least severe maximum drawdown: minvar_ewma (-27.09%).\n- Lower historical ES tail risk: minvar_sample (1.85%).\n- Highest market beta: ew (1.00); lowest: minvar_sample (0.44).\n- Potential VaR model instability (p&lt;0.05): minvar_ewma, minvar_oas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHKEX strategies in report: ['ew', 'maxsharpe_frontier', 'maxsharpe_slsqp', 'minvar_ewma', 'minvar_lw', 'minvar_oas', 'minvar_sample', 'mv_ewma', 'mv_lw', 'mv_oas', 'mv_sample', 'ridge_mv']\nHKEX report tables: ['attribution_es', 'attribution_overlap', 'attribution_vol', 'capm', 'corr', 'drawdown_episodes', 'drawdown_summary', 'performance', 'shape', 'stress', 'var_backtest', 'var_es']",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3. risk analysis and CAPM</span>"
    ]
  }
]