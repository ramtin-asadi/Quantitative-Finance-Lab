[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "What this is\nQuantitative-Finance-Lab is a project series where each topic is developed end-to-end:\nThe goal is to build a portfolio that is both research-grade (well explained) and engineering-grade (reusable, testable, maintainable).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#what-this-is",
    "href": "index.html#what-this-is",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "Clear narrative: intuition → math → implementation\nReproducible experiments: consistent metrics, plots, and comparisons\nLibrary-first engineering: reusable components are extracted into quantfinlab",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#start-here",
    "href": "index.html#start-here",
    "title": "Quantitative Finance Lab",
    "section": "Start here",
    "text": "Start here\n\nRead the Overview for conventions, reproducibility notes, and how projects are structured.\nThen open a project notebook and follow it top-to-bottom.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Quantitative Finance Lab",
    "section": "Projects",
    "text": "Projects\n\n\n\n01 — Yield Curve, Bond Pricing & Risk\n\nYield curve construction (discount factors / zero rates)\nBond pricing and key risk measures\nPractical fixed-income utilities you can reuse elsewhere\n\nOpen Project 01\n\n\n\n02 — Portfolio Optimization (Mean–Variance)\n\nMean–variance optimization pipeline\nCovariance estimators and model comparisons\nBacktest-style evaluation and performance diagnostics\n\nOpen Project 02",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#the-quantfinlab-library",
    "href": "index.html#the-quantfinlab-library",
    "title": "Quantitative Finance Lab",
    "section": "The quantfinlab library",
    "text": "The quantfinlab library\nAs the series grows, reusable code is consolidated into quantfinlab:\n\nshared fixed-income helpers\nportfolio utilities\nplotting/diagnostic helpers\ncommon conventions (metrics, configs, I/O)\n\nThe intent is simple: notebooks explain and experiment; the library implements.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Quantitative Finance Lab",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nThis repo follows a reproducibility-first workflow (formatting/linting/testing/CI).\nLarge datasets are typically not committed. If a notebook expects data files, place them under a local data/ directory (gitignored) and follow any project-specific notes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Quantitative Finance Lab",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis repository is for research and educational purposes. Nothing here is investment advice, and results may vary by dataset, assumptions, costs, constraints, and implementation details.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Purpose\nThis lab is designed to be a long-running series of quantitative finance projects, where each project is:\nThe long-term output is a portfolio of work that demonstrates both research depth and software discipline.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#purpose",
    "href": "overview.html#purpose",
    "title": "Overview",
    "section": "",
    "text": "Readable (clear narrative, equations, and interpretation)\n\nReproducible (same inputs → same results, consistent metrics)\n\nReusable (core logic extracted into a Python package)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#how-projects-are-structured",
    "href": "overview.html#how-projects-are-structured",
    "title": "Overview",
    "section": "How projects are structured",
    "text": "How projects are structured\nEach project is written as a Quarto notebook and typically follows:\n\nMotivation & problem statement\nMathematical formulation\nImplementation details\nExperiments and comparisons\nDiagnostics + failure modes\nSummary of results & takeaways\n\nAs patterns emerge, reusable components are moved into the quantfinlab package so future projects can build on stable foundations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#repository-layout-conceptual",
    "href": "overview.html#repository-layout-conceptual",
    "title": "Overview",
    "section": "Repository layout (conceptual)",
    "text": "Repository layout (conceptual)\n\nnotebooks/\nProject notebooks (Quarto). These are the public-facing research artifacts.\nquantfinlab/\nReusable Python library modules shared across projects.\ndocs/\nRendered website output (GitHub Pages).\n.github/workflows/, pyproject.toml, .pre-commit-config.yaml\nCode quality + reproducibility tooling (lint/format/tests/CI).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#the-quantfinlab-philosophy",
    "href": "overview.html#the-quantfinlab-philosophy",
    "title": "Overview",
    "section": "The quantfinlab philosophy",
    "text": "The quantfinlab philosophy\nThe design rule is:\n\nNotebooks are clients. The library is the product.\n\nIf code is used in more than one place—or is critical enough to deserve tests—it should live in quantfinlab.\nExamples of what belongs in the library: - curve construction helpers, bond pricing primitives, risk measures - portfolio constraints, optimization wrappers, objective/penalty building blocks - standardized performance metrics and plotting utilities - data validation, common transformations, and safe defaults",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#data-reproducibility",
    "href": "overview.html#data-reproducibility",
    "title": "Overview",
    "section": "Data & reproducibility",
    "text": "Data & reproducibility\nMany finance datasets are large or licensed, so the repository generally avoids committing raw data.\nRecommended approach - Keep datasets in a local data/ directory (gitignored). - Use clear filenames and a short data note inside each notebook: - required file(s) - expected columns - frequency and timezone assumptions - missing data handling expectations\nStrong reproducibility standard - A notebook should either: - run end-to-end from a clean clone once data is present, or - fail with a clear, friendly message explaining what’s missing and where to put it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#evaluation-conventions-recommended",
    "href": "overview.html#evaluation-conventions-recommended",
    "title": "Overview",
    "section": "Evaluation conventions (recommended)",
    "text": "Evaluation conventions (recommended)\nTo keep comparisons consistent across projects, the lab aims to standardize:\n\nReturn definition (close-to-close vs open-to-open, etc.)\nRebalance convention (timing, lookahead prevention)\nTransaction costs model (turnover-based, spread-based, fees)\nMetrics\nAt minimum: CAGR, volatility, Sharpe, max drawdown, turnover, hit rate\nWhen relevant: tail metrics (VaR/CVaR), exposure diagnostics, constraint activity\n\nIf a metric can be misleading, the notebook should highlight it and include at least one complementary diagnostic plot/table.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#quality-checklist-used-in-this-lab",
    "href": "overview.html#quality-checklist-used-in-this-lab",
    "title": "Overview",
    "section": "Quality checklist used in this lab",
    "text": "Quality checklist used in this lab\nA project is considered “complete” when it includes:\n\nclear assumptions and constraints\n\nsanity checks and edge-case discussion\n\ncomparison baselines (simple but strong references)\n\nreproducible plots/tables (consistent formatting and labels)\n\nreusable implementation extracted to quantfinlab where appropriate\n\nminimal tests for core functions when feasible",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#roadmap-high-level",
    "href": "overview.html#roadmap-high-level",
    "title": "Overview",
    "section": "Roadmap (high level)",
    "text": "Roadmap (high level)\n\nExpand the library into clearer submodules (fixed income, portfolio, risk, datasets)\nAdd test coverage for high-value finance primitives\nAdd standardized experiment configs + results tables at the end of each project\nAdd small sample datasets for “out-of-the-box” runs when possible\nAdd additional projects in areas like:\n\nrisk attribution and factor models\ntransaction cost modeling and turnover control\nrobust optimization and regularization\nstress testing and scenario analysis\nyield curve strategies and bond portfolio PnL attribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "overview.html#disclaimer",
    "href": "overview.html#disclaimer",
    "title": "Overview",
    "section": "Disclaimer",
    "text": "Disclaimer\nThis content is for education and research. It is not investment advice. Any results depend heavily on data quality, assumptions, costs, constraints, and implementation details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html",
    "title": "1. Fixed income and yield curve construction",
    "section": "",
    "text": "1) Bonds, Coupons, Treasuries, and Par Yields\nIn this project we will implement",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bonds-coupons-treasuries-and-par-yields",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bonds-coupons-treasuries-and-par-yields",
    "title": "1. Fixed income and yield curve construction",
    "section": "",
    "text": "1.1 Fixed-coupon bond cashflows\nA standard coupon bond is defined by: - notional \\(N\\) (principal. the money that you get back) - annual coupon rate \\(c\\) (the interest rate of the bond) - maturity \\(T\\) (the time that bond ends and you get back principal and interest) - coupon frequency \\(f\\) (the amount of payments per year)\nCoupon payment each period is \\(\\dfrac{c}{f}N\\).\nPayment times are \\(t_i=\\dfrac{i}{f}\\) for \\(i=1,2,\\dots,n\\)      where \\(n=fT\\)\nCashflows for each period \\(i\\) is \\(CF_i=\\dfrac{c}{f}N\\)         \\(i=1,\\dots,n-1\\)\nfinal cash flow: \\(CF_n=\\dfrac{c}{f}N+N\\)\n\n\n\n1.2 Price from discount factors\nFor calculating price of a bond we need to discount all the cash flows to today value to compute the present value of the bond. for that we need to have a discount rate for every \\(t\\) that a cash flow accurs. this comes from a continous function of time which is discount curve \\(D(t)\\), the price is \\(P=\\sum_{i=1}^{n} CF_i\\,D(t_i)\\)\n\n\n\n1.3 Year fraction from dates\nGiven dates \\(d_0\\) and \\(d_1\\), a day-count year fraction is \\(\\tau(d_0,d_1)=\\dfrac{\\text{days}(d_0,d_1)}{365}\\)\n\n\n\n1.4 Tenor mapping\nTenor labels map to maturities: - \\(k\\) M → \\(T=k/12\\) - \\(k\\) Y → \\(T=k\\)\nCollect maturities into a numeric vector: \\(\\mathbf{T}=(T_1,T_2,\\dots,T_m)\\)\nObserved market par yields: \\(\\mathbf{y}=(y_1,y_2,\\dots,y_m)\\) with \\(y_j=y(T_j)\\)\n\n\n\n1.5 What “Treasury par yield” means\nA par yield at maturity \\(T\\) is the coupon rate \\(c(T)\\) that makes a standard coupon bond price equal to par: \\(P(T,c(T))=N\\) If we normalize \\(N=1\\), then par means \\(P=1\\).\nFor discounting, we use these yields but the problem is we have some of the standard maturities which may not be the same as other bonds. so that’s why we have to make this yield curve from the treasury maturities to be able to discount any time to present.\nyou can download the data used in this notebook here (treasury par yields from 1990 to 2026)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#imports-and-plotting-style-and-loading-data",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#imports-and-plotting-style-and-loading-data",
    "title": "1. Fixed income and yield curve construction",
    "section": "Imports and plotting style and loading data",
    "text": "Imports and plotting style and loading data\n\nimport math\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom cycler import cycler\n\nwarnings.filterwarnings(\"ignore\")\n\n\ncolors = [\"#069AF3\",\"#FE420F\", \"#00008B\", \"#008080\", \"#7BC8F6\",\"#800080\",\"#0072B2\",\"#008000\",\"#CC79A7\", \"#DC143C\", \"#04D8B2\"]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\nplt.rcParams.update({\n    \"figure.figsize\": (6, 3),\n    \"figure.dpi\": 300,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 6,\n})\n\n\ndf = pd.read_csv(r\"E:\\daneshgah\\quantitative-finance-lab\\data\\par-yield-curve-rates-1990-2026.csv\")\n\ncol_map = {\"date\": \"Date\",\"1 mo\": \"1M\",\"2 mo\": \"2M\",\"3 mo\": \"3M\",\"4 mo\": \"4M\",\"6 mo\": \"6M\",\"1 yr\": \"1Y\",\"2 yr\": \"2Y\",\"3 yr\": \"3Y\",\"5 yr\": \"5Y\",\"7 yr\": \"7Y\",\"10 yr\": \"10Y\",\"20 yr\": \"20Y\",\"30 yr\": \"30Y\"}\n\ndf = df.rename(columns={k.lower(): v for k, v in col_map.items()})\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"Date\"], ).set_index(\"Date\").sort_index()\n\ndf\n\n\n\n\n\n\n\n\n1M\n2M\n3M\n4M\n6M\n1Y\n2Y\n3Y\n5Y\n7Y\n10Y\n20Y\n30Y\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-02\nNaN\nNaN\n7.83\nNaN\n7.89\n7.81\n7.87\n7.90\n7.87\n7.98\n7.94\nNaN\n8.00\n\n\n1990-01-03\nNaN\nNaN\n7.89\nNaN\n7.94\n7.85\n7.94\n7.96\n7.92\n8.04\n7.99\nNaN\n8.04\n\n\n1990-01-04\nNaN\nNaN\n7.84\nNaN\n7.90\n7.82\n7.92\n7.93\n7.91\n8.02\n7.98\nNaN\n8.04\n\n\n1990-01-05\nNaN\nNaN\n7.79\nNaN\n7.85\n7.79\n7.90\n7.94\n7.92\n8.03\n7.99\nNaN\n8.06\n\n\n1990-01-08\nNaN\nNaN\n7.79\nNaN\n7.88\n7.81\n7.90\n7.95\n7.92\n8.05\n8.02\nNaN\n8.09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2026-01-22\n3.79\n3.72\n3.71\n3.67\n3.61\n3.53\n3.61\n3.68\n3.85\n4.05\n4.26\n4.79\n4.84\n\n\n2026-01-23\n3.78\n3.72\n3.70\n3.67\n3.61\n3.53\n3.60\n3.67\n3.84\n4.03\n4.24\n4.78\n4.82\n\n\n2026-01-26\n3.77\n3.70\n3.67\n3.67\n3.62\n3.52\n3.56\n3.66\n3.82\n4.02\n4.22\n4.75\n4.80\n\n\n2026-01-27\n3.77\n3.70\n3.67\n3.66\n3.61\n3.50\n3.53\n3.65\n3.81\n4.03\n4.24\n4.79\n4.83\n\n\n2026-01-28\n3.76\n3.71\n3.68\n3.70\n3.63\n3.52\n3.56\n3.66\n3.83\n4.05\n4.26\n4.81\n4.85\n\n\n\n\n9024 rows × 13 columns\n\n\n\n\ntenor_cols = [\"1M\",\"2M\",\"3M\",\"4M\",\"6M\",\"1Y\",\"2Y\",\"3Y\",\"5Y\",\"7Y\",\"10Y\",\"20Y\",\"30Y\"]\nfor c in tenor_cols:\n    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\nfirst_valid = df[tenor_cols].apply(lambda s: s.first_valid_index())\navailability = pd.DataFrame({\n    \"tenor\": tenor_cols,\n    \"first_valid_date\": [first_valid[t] for t in tenor_cols],\n})\navailability[\"first_valid_date\"] = pd.to_datetime(availability[\"first_valid_date\"])\navailability = availability.sort_values(\"first_valid_date\")\n\n\n\nprint(\"\\nData shape:\", df.shape)\nprint(\"Date range:\", df.index.min().date(), \"to\", df.index.max().date())\ndisplay(df[tenor_cols].describe().T)\n\n\nplt.figure()\nfor c in df.columns:\n    plt.plot(df.index, df[c], label=c)\nplt.title(\"Par Yields Over Time\")\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Date\")\nplt.legend(ncol= 4)\nplt.show()\n\n\nprint(\"First available date per tenor:\")\ndisplay(availability)\n\n\nData shape: (9024, 13)\nDate range: 1990-01-02 to 2026-01-28\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n1M\n6124.0\n1.669061\n1.834318\n0.00\n0.08\n0.98\n2.700\n6.02\n\n\n2M\n1819.0\n2.731045\n2.092637\n0.00\n0.14\n2.43\n4.635\n5.61\n\n\n3M\n9020.0\n2.793844\n2.279473\n0.00\n0.23\n2.74\n5.010\n8.26\n\n\n4M\n817.0\n4.826524\n0.600693\n3.58\n4.35\n4.77\n5.440\n5.64\n\n\n6M\n9023.0\n2.908257\n2.296244\n0.02\n0.41\n3.00\n5.100\n8.49\n\n\n1Y\n9023.0\n3.004545\n2.274365\n0.04\n0.56\n3.12\n5.025\n8.64\n\n\n2Y\n9023.0\n3.243374\n2.267325\n0.09\n0.94\n3.35\n4.990\n9.05\n\n\n3Y\n9023.0\n3.422187\n2.212582\n0.10\n1.37\n3.55\n5.050\n9.11\n\n\n5Y\n9023.0\n3.761102\n2.107014\n0.19\n1.81\n3.73\n5.380\n9.10\n\n\n7Y\n9023.0\n4.037397\n2.028457\n0.36\n2.22\n3.93\n5.600\n9.12\n\n\n10Y\n9023.0\n4.251091\n1.937830\n0.52\n2.61\n4.17\n5.710\n9.09\n\n\n20Y\n8084.0\n4.377290\n1.629305\n0.87\n2.89\n4.53\n5.540\n8.30\n\n\n30Y\n8029.0\n4.735390\n1.879145\n0.99\n3.08\n4.56\n6.130\n9.18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst available date per tenor:\n\n\n\n\n\n\n\n\n\ntenor\nfirst_valid_date\n\n\n\n\n2\n3M\n1990-01-02\n\n\n4\n6M\n1990-01-02\n\n\n5\n1Y\n1990-01-02\n\n\n6\n2Y\n1990-01-02\n\n\n7\n3Y\n1990-01-02\n\n\n8\n5Y\n1990-01-02\n\n\n9\n7Y\n1990-01-02\n\n\n10\n10Y\n1990-01-02\n\n\n12\n30Y\n1990-01-02\n\n\n11\n20Y\n1993-10-01\n\n\n0\n1M\n2001-07-31\n\n\n1\n2M\n2018-10-16\n\n\n3\n4M\n2022-10-19\n\n\n\n\n\n\n\n\nsample_dates = [\n    df.index[0],\n    df.index[len(df)//2],\n    df.index[-252],\n    df.index[df.index &lt;= pd.Timestamp(\"2007-03-01\")][-1],\n    df.index[-1]\n]\n\nx = np.arange(len(tenor_cols))  \n\nplt.figure()\nfor d in sample_dates:\n    y = df.loc[d, tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    plt.plot(x[mask], y.values[mask], marker=\"o\", label=d.strftime(\"%Y-%m-%d\"))\n\nplt.title(\"Yield Curve Snapshots (Par Yields)\")\nplt.xticks(x, tenor_cols)\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Tenor\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#discount-factors-zero-rates-and-forward-rates",
    "title": "1. Fixed income and yield curve construction",
    "section": "3) Discount Factors, Zero Rates, and Forward Rates",
    "text": "3) Discount Factors, Zero Rates, and Forward Rates\n\n3.1 Discount factor\n\\(D(t)\\) is the present value of receiving 1 unit of currency at time \\(t\\). For example what does 1 dollar in 20 years worth now.\n\n\n3.2 Zero rate (continuous compounding)\n\\(z(t)\\) is the constant rate that discounts a payment in \\(t\\) to present. for example, if we want to know discount factor of 1 dollar in 20 years we need an annual rate to compute the present value. that’s zero rate.\nDefine \\(z(t)\\) by \\[\nD(t)=e^{-z(t)t}\n\\] \\[\nz(t)=-\\dfrac{\\ln D(t)}{t}\n\\]\n\n\n3.3 Instantaneous forward rate\n\\(f(t)\\) is the slope of \\(z(t)\\) which tells us how much the rate of return is very close to the time of maturity. it is used because zero rate is smooth and in instant time we need to have exact forward rate\n\\[\nf(t)=-\\dfrac{d}{dt}\\ln D(t)\n\\] \\[\nD(t)=\\exp\\left(-\\int_0^t f(u)\\,du\\right)\n\\]\n\n\n3.4 Discrete forward over an interval\nFor \\(t_1&lt;t_2\\), the continuously-compounded forward rate for the interval is \\[\nF(t_1,t_2)=\\dfrac{\\ln D(t_1)-\\ln D(t_2)}{t_2-t_1}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#par-yield-implied-by-a-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "4) Par Yield Implied by a Curve",
    "text": "4) Par Yield Implied by a Curve\nThis is used for building yield curve and validation to see if the predicted rate is close to real rate based on curve. what is the rate that makes the price of bond (PV of cashflows) equal to 1?\nGiven a curve \\(D(t)\\), the par coupon rate for maturity \\(T\\) and frequency \\(f\\) solves\n\\(1=\\sum_{i=1}^{n}\\dfrac{c}{f}D(t_i)+D(T)\\)            \\(t_i=i/f\\), \\(n=fT\\).\n\\(1=\\dfrac{c}{f}\\sum_{i=1}^{n}D(t_i)+D(T)\\)\nand finally we get to \\(c=f\\,\\dfrac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nShort maturities (&lt;1Y) often use money-market conventions because they are mostly single payment. Two common ones: - continuous: \\(y=-\\dfrac{\\ln D(T)}{T}\\) - simple: \\(y=\\dfrac{1/D(T)-1}{T}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#bootstrapping-discount-factors-from-par-yields",
    "title": "1. Fixed income and yield curve construction",
    "section": "5) Bootstrapping Discount Factors from Par Yields",
    "text": "5) Bootstrapping Discount Factors from Par Yields\nBootstrapping constructs \\(D(T)\\) at market tenors from observed par yields. in this way we can have a function of time to discount a payment in any maturity based on the real yields that we have.\n\n5.1 Short end (&lt;1Y) convention\nFor \\(T&lt;1\\) we use the money market convention again. for calculating discount factor:\n\ncontinuous convention: \\(D(T)=e^{-y(T)T}\\)\nsimple convention: \\(D(T)=\\dfrac{1}{1+y(T)T}\\)\n\n\n\n5.2 Bootstrapping for coupon tenors (T ≥ 1)\nLet \\(c=y(T)\\) be the market par yield at maturity \\(T\\) (used as coupon rate). With frequency \\(f\\) and cashflow times \\(t_i=i/f\\):\nPar condition (normalized notional 1): \\[\n1=\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)+\\left(1+\\dfrac{c}{f}\\right)D(T)\n\\]\nSolve for the new unknown \\(D(T)\\): \\[D(T)=\\dfrac{1-\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)}{1+\\dfrac{c}{f}}\n\\]\nif we have the earlier coupons discount factor, we know everything on the right side of equation. so we can solve it and get to \\(D(T)\\)\nit’s called bootstrapping because we first compute the \\(D(T&lt;1Y)\\) with short end convention, then we use that to compute \\(D(1Y)\\) and then use them for \\(D(2Y)\\) until the last maturity (30Y)\n\n\n5.3 Interpolating discount factors at coupon dates\nBootstrapping needs \\(D(t_i)\\) at coupon dates, but you often only have DFs at pillar maturities.\nA robust choice is log-linear interpolation: If \\(T_a&lt;t&lt;T_b\\), then\n\\[\\ln D(t)=\\ln D(T_a)+\\dfrac{t-T_a}{T_b-T_a}\\left(\\ln D(T_b)-\\ln D(T_a)\\right)\n\\]\nSo \\[D(t)=\\exp\\left(\\ln D(t)\\right)\n\\]\n\ndf_dec = df.copy()\ndf_dec[tenor_cols] = df_dec[tenor_cols] / 100.0\n\nshort_end_convention = \"continuous\"\nf = 2\nmin_d = 1e-12\n\n\ndef labels_to_T(labels):\n    T = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        if labu.endswith(\"M\"):\n            T.append(int(labu[:-1]) / 12.0)\n        else:\n            T.append(float(int(labu[:-1])))\n    return np.array(T, dtype=float)\n\n\ndef get_par_from_row(row):\n    y = row[tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    labels = [tenor_cols[i] for i in range(len(tenor_cols)) if mask[i]]\n    if len(labels) == 0:\n        return None\n\n    par = y.values[mask].astype(float)\n    T = labels_to_T(labels)\n\n    i = np.argsort(T)\n    T = T[i]\n    par = par[i]\n    labels = [labels[i] for i in i]\n\n    return T, par, labels\n\n\ndef get_par_for_date(date):\n    row = df_dec.loc[date]\n    return get_par_from_row(row)\n\n\ndef short_end_df(Ti, ri):\n    if short_end_convention == \"continuous\":\n        return math.exp(-ri * Ti)\n    return 1.0 / (1.0 + ri * Ti)\n\n\ndef price_error_loglinear(d_T, Ti, t_prev, d_prev, times_interp, c, pv_known):\n    d_T = max(float(d_T), min_d)\n    pv_interp = 0.0\n    if len(times_interp) &gt; 0:\n        w = (times_interp - t_prev) / (Ti - t_prev)\n        log_d = (1 - w) * np.log(d_prev) + w * np.log(d_T)\n        d_interp = np.exp(log_d)\n        pv_interp = np.sum((c / f) * d_interp)\n    return pv_known + pv_interp + d_T - 1.0\n\n\ndef solve_df_long_end(Ti, ri, d_map):\n    c = float(ri)\n    n = int(round(Ti * f))\n    times = np.array([k / f for k in range(1, n + 1)], dtype=float)\n\n\n    known_T = np.array(sorted(d_map.keys()), dtype=float)\n    known_D = np.array([d_map[t] for t in known_T], dtype=float)\n    known_D = np.clip(known_D, min_d, None)\n\n\n    t_prev = known_T[-1]\n    d_prev = known_D[-1]\n\n    times_known = times[times &lt;= t_prev + 1e-12]\n    times_interp = times[times &gt; t_prev + 1e-12]\n\n    pv_known = 0.0\n    if len(times_known) &gt; 0:\n        log_known_D = np.log(known_D)\n        log_df_known = np.interp(times_known, known_T, log_known_D)\n        d_known = np.exp(log_df_known)\n        pv_known = np.sum((c / f) * d_known)\n\n    lo = min_d\n    hi = d_prev\n    f_lo = price_error_loglinear(lo, Ti, t_prev, d_prev, times_interp, c, pv_known)\n    f_hi = price_error_loglinear(hi, Ti, t_prev, d_prev, times_interp, c, pv_known)\n\n    if f_lo * f_hi &gt; 0:\n    \n        log_known_D = np.log(known_D)\n        log_df_cpn = np.interp(\n            times[:-1],\n            known_T,\n            log_known_D,\n            left=log_known_D[0],\n            right=log_known_D[-1],\n        )\n        d_cpn = np.exp(log_df_cpn)\n        pv_coupons = np.sum((c / f) * d_cpn)\n        d_T = (1.0 - pv_coupons) / (1.0 + c / f)\n    else:\n        for _ in range(100):\n            mid = 0.5 * (lo + hi)\n            f_mid = price_error_loglinear(mid, Ti, t_prev, d_prev, times_interp, c, pv_known)\n            if f_lo * f_mid &lt;= 0:\n                hi = mid\n                f_hi = f_mid\n            else:\n                lo = mid\n                f_lo = f_mid\n            if abs(hi - lo) &lt; 1e-12:\n                break\n        d_T = 0.5 * (lo + hi)\n\n    return d_T\n\n\ndef bootstrap_from_inputs(T, par, labels, date=None):\n    d_map = {}\n\n    # short convention\n    for Ti, ri in zip(T, par, strict=True):\n        if Ti &lt; 1.0:\n            d_T = short_end_df(Ti, ri)\n            d_map[Ti] = max(float(d_T), min_d)\n            continue\n\n        d_T = solve_df_long_end(Ti, ri, d_map)\n        if (not np.isfinite(d_T)) or (d_T &lt;= 0):\n            d_T = min_d\n        d_map[Ti] = max(float(d_T), min_d)\n\n    dfs = np.array([d_map[t] for t in T], dtype=float)\n\n    return {\n        \"date\": date,\n        \"T\": T,\n        \"par\": par,\n        \"labels\": labels,\n        \"dfs\": dfs,\n    }\n\n\ndef bootstrap_pillars(date):\n    result = get_par_for_date(date)\n    if result is None:\n        return None\n\n    T, par, labels = result\n    return bootstrap_from_inputs(T, par, labels, date=date)\n\n\n# we bootstrap discount factors at the last available date for now\nbase_date = df_dec.index[-1]\npillars = bootstrap_pillars(base_date)\n\nT = pillars[\"T\"]\npar = pillars[\"par\"]\nlabels = pillars[\"labels\"]\ndfs = pillars[\"dfs\"]\n\nprint(\"Base date:\", base_date.date())\nprint(\"Tenors used:\", labels)\nprint(\"First 5 pillar DFs:\", dfs)\n\nplt.figure()\nplt.plot(T, dfs, marker=\"o\")\nplt.title(\"Bootstrapped Discount Factors\")\nplt.xlabel(\"Maturity T\")\nplt.ylabel(\"Discount Factor D(t)\")\nplt.show()\n\nBase date: 2026-01-28\nTenors used: ['1M', '2M', '3M', '4M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\nFirst 5 pillar DFs: [0.99687157 0.99383574 0.99084219 0.98774241 0.98201372 0.96571989\n 0.93185753 0.89680276 0.82670445 0.75353422 0.65216175 0.37090153\n 0.22566195]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#turning-bootstrapped-pillars-into-a-full-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "6) Turning Bootstrapped Pillars into a Full Curve",
    "text": "6) Turning Bootstrapped Pillars into a Full Curve\nAfter bootstrapping we have pillars \\((T_j, D(T_j))\\) or \\((T_j, z(T_j))\\). Now we want to define continuous functions \\(D(t)\\) and \\(z(t)\\) for all \\(t\\).\n\n6.1 Method A: Log-linear discount factors\nInterpolate \\(\\ln D(t)\\) linearly between pillars (just like for the T we had. we do the same thing between them):\n\\(\\ln D(t)=\\text{linear interp of } \\{\\ln D(T_j)\\}\\)\nThen \\(D(t)=\\exp(\\ln D(t))\\)\nand \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\ndef loglinear_curve(T, dfs):\n    loglinear_log_dfs = np.log(dfs)\n\n    loglinear_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    loglinear_log_df_grid = np.interp(\n        loglinear_grid,\n        T,\n        loglinear_log_dfs,\n        left=loglinear_log_dfs[0],\n        right=loglinear_log_dfs[-1],\n    )\n    loglinear_df_grid = np.exp(loglinear_log_df_grid)\n\n    # zero rate\n    loglinear_z_grid = -np.log(loglinear_df_grid) / loglinear_grid\n\n    # instantaneous forward rate\n    loglinear_fwd_grid = -np.gradient(np.log(loglinear_df_grid), loglinear_grid)\n\n    def loglinear_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(\n            t,\n            T,\n            loglinear_log_dfs,\n            left=loglinear_log_dfs[0],\n            right=loglinear_log_dfs[-1],\n        )\n        return np.exp(log_df)\n\n    return {\n        \"name\": \"Log-linear DF\",\n        \"grid\": loglinear_grid,\n        \"df_grid\": loglinear_df_grid,\n        \"z_grid\": loglinear_z_grid,\n        \"fwd_grid\": loglinear_fwd_grid,\n        \"df_func\": loglinear_df_func,\n        \"log_dfs\": loglinear_log_dfs,\n    }\n\n\nloglinear_curve_data = loglinear_curve(T, dfs)\nloglinear_grid = loglinear_curve_data[\"grid\"]\nloglinear_df_grid = loglinear_curve_data[\"df_grid\"]\nloglinear_z_grid = loglinear_curve_data[\"z_grid\"]\nloglinear_fwd_grid = loglinear_curve_data[\"fwd_grid\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"loglinear\"] = loglinear_curve_data\n\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_z_grid * 100.0)\nplt.title(\"Zero Curve (log-linear)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Zero Rate (%)\")\nplt.show()\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_fwd_grid * 100.0)\nplt.title(\"Instantaneous Forward Rate (derivative of ln(DF))\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Forward Rate (%)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#curve-models-using-zero-rate-smoothing",
    "title": "1. Fixed income and yield curve construction",
    "section": "7) Curve Models Using Zero-Rate Smoothing",
    "text": "7) Curve Models Using Zero-Rate Smoothing\nInstead of interpolating \\(D\\), we can interpolate \\(z\\) and recover \\(D(t)=e^{-z(t)t}\\).\n\n7.1 PCHIP on zero rates (Piecewise Cubic Hermite Interpolating Polynomial)\nGiven nodes \\(x_j=T_j\\) and \\(y_j=z(T_j)\\), PCHIP builds a piecewise cubic polynomial on each interval:\n\\[\np_j(t)=a_j(t-x_j)^3+b_j(t-x_j)^2+c_j(t-x_j)+d_j  \\quad t\\in[x_j,x_{j+1}]\n\\]\nConstraints include: - \\(p_j(x_j)=y_j\\) and \\(p_j(x_{j+1})=y_{j+1}\\) - first derivatives are chosen by shape-preserving slope rules to reduce overshoot\nwe define \\(z(t)=p_j(t)\\)\nthen \\(D(t)=e^{-z(t)t}\\)\n\nfrom scipy.interpolate import PchipInterpolator\n\n\ndef pchip_curve(T, dfs):\n    pchip_zeros = -np.log(np.clip(dfs, min_d, None)) / T\n\n    pchip_z = PchipInterpolator(T, pchip_zeros, extrapolate=True)\n    pchip_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    pchip_z_grid = pchip_z(pchip_grid)\n    pchip_df_grid = np.exp(-pchip_z_grid * pchip_grid)\n\n    pchip_fwd_grid = -np.gradient(\n        np.log(np.clip(pchip_df_grid, min_d, None)), pchip_grid\n    )\n\n    def pchip_df_func(t):\n        t = np.array(t, dtype=float)\n        z = pchip_z(t)\n        return np.exp(-z * t)\n\n    return {\n        \"name\": \"PCHIP zero\",\n        \"grid\": pchip_grid,\n        \"df_grid\": pchip_df_grid,\n        \"z_grid\": pchip_z_grid,\n        \"fwd_grid\": pchip_fwd_grid,\n        \"df_func\": pchip_df_func,\n        \"pillar_zeros\": pchip_zeros,\n    }\n\n\npchip_curve_data = pchip_curve(T, dfs)\ncurves[\"pchip\"] = pchip_curve_data\n\n\nplt.figure()\nplt.plot(T, pchip_curve_data[\"pillar_zeros\"] * 100.0, \"o\", label=\"pillar zeros\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"z_grid\"] * 100.0, \"-\", label=\"PCHIP zero\")\nplt.title(\"Zero Curve (PCHIP)\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(T, dfs, \"o\", label=\"Pillar DFs\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"df_grid\"], \"-\", label=\"DF from PCHIP\")\nplt.title(\"Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"Instantaneous Forward Rate\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#nelsonsiegelsvensson-nss-yield-curve",
    "title": "1. Fixed income and yield curve construction",
    "section": "8) Nelson–Siegel–Svensson (NSS) yield curve",
    "text": "8) Nelson–Siegel–Svensson (NSS) yield curve\nwe represent the continuous-compounded zero rate curve \\(z(t)\\) with a small number of parameters, then derive discount factors, par yields and forwards\n\n8.1 NSS zero-rate function\nFor maturity \\(t&gt;0\\), the NSS zero rate is:\n\\[\nz(t)=\\beta_0 +\\beta_1\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}\\right) +\\beta_2\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}-e^{-t/\\tau_1}\\right) +\\beta_3\\left(\\frac{1-e^{-t/\\tau_2}}{t/\\tau_2}-e^{-t/\\tau_2}\\right)\n\\]\nParameters:\n\n\\(\\beta_0\\) = long-run “level”\n\\(\\beta_1\\) = “slope” (short-end effect)\n\\(\\beta_2\\) = medium-term “curvature” (first hump)\n\\(\\beta_3\\) = additional curvature (second hump)\n\\(\\tau_1,\\tau_2&gt;0\\) control where humps occur\n\n\ndef nss_zero(t, b0,b1,b2,b3,tau1,tau2):\n    t = np.array(t, dtype=float)\n    x1 = t / tau1\n    x2 = t / tau2\n    L1 = (1.0 - np.exp(-x1)) / x1\n    C1 = L1 - np.exp(-x1)\n    C2 = (1.0 - np.exp(-x2)) / x2 - np.exp(-x2)\n    return b0 + b1*L1 + b2*C1 + b3*C2\n\n\n\n8.3 Par yield implied by NSS\nFor a coupon bond with maturity \\(T\\) and coupon frequency \\(f\\), coupon rate \\(c(T)\\) is the rate that makes the bond price equal to par (normalize notional to 1):\n\\[\n1=\\sum_{i=1}^{n}\\frac{c(T)}{f}D(t_i)+D(T)\n\\]\nSolve for \\(c(T)\\):\n\\[\nc(T)=f\\cdot\\frac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\n\\]\nFor short maturities (money-market style), a common mapping is:\ncontinuous: \\(y(T)=-\\ln D(T)/T\\)\nsimple: \\(y(T)=(1/D(T)-1)/T\\)\nbut first we have to have \\(D(t)\\)\n\ndef par_from_d(df_func, T_list, f=2):\n    T_arr = np.asarray(T_list, dtype=float)\n    out = np.full_like(T_arr, np.nan, dtype=float)\n\n    step = 1.0 / float(f)\n\n    for k, Tk in enumerate(T_arr):\n        if not np.isfinite(Tk) or Tk &lt;= 0:\n            continue\n\n        D_T = float(np.asarray(df_func([Tk],), dtype=float)[0])\n        D_T = max(D_T, min_d)\n\n        if Tk &lt; 1.0:\n            if short_end_convention == \"simple\":\n                out[k] = (1.0 / D_T - 1.0) / Tk\n            else:\n                out[k] = -np.log(D_T) / Tk\n            continue\n\n        n_full = int(np.floor(Tk * f + 1e-12))\n        times = np.arange(step, n_full * step + 1e-12, step)\n        if len(times) == 0 or abs(times[-1] - Tk) &gt; 1e-10:\n            times = np.append(times, Tk)\n\n        accr = np.diff(np.concatenate([[0.0], times]))\n        dfs = np.asarray(df_func(times), dtype=float)\n        dfs = np.clip(dfs, min_d, None)\n\n        denom = float(np.sum(accr * dfs))\n        out[k] = (1.0 - dfs[-1]) / denom if denom &gt; 0 else np.nan\n\n    return out\n\n\n\n8.2 Discount factor and forward from NSS\nOnce we have \\(z(t)\\), using continuous compounding:\n\\(D(t)=e^{-z(t)t}\\)\nand the instantaneous forward rate is:\n\\(f(t)=-\\frac{d}{dt}\\ln D(t)\\)\nWith NSS you often compute \\(f(t)\\) numerically on a grid: \\[f(t_i)\\approx -\\frac{\\ln D(t_{i+1})-\\ln D(t_{i-1})}{t_{i+1}-t_{i-1}}\n\\]\n\n\n\n8.4 Calibrating NSS to market par yields\nGiven observed par yields \\(y^{mkt}(T_j)\\) at tenors \\(T_j\\), we choose parameters \\(\\theta=(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\tau_1,\\tau_2)\\) to minimize a least-squares objective:\n\\[\\min_{\\theta}\\sum_{j}\\left(c^{model}(T_j;\\theta)-y^{mkt}(T_j)\\right)^2\n\\]\nOptionally add regularization to discourage extreme shapes: \\(\\lambda\\sum_j (z''(t_j))^2\\) or bounds on parameters.\n\nfrom scipy.optimize import minimize\n\n\ndef nss_curve(T, par):\n    def obj(theta):\n        b0, b1, b2, b3, tau1, tau2 = theta\n        z = nss_zero(T, b0, b1, b2, b3, tau1, tau2)\n        dfs = np.exp(-z * T)\n\n        # we use log-linear DF interpolation on pillars for keeping it positive\n        log_dfs = np.log(np.clip(dfs, min_d, None))\n\n        def df_func(t):\n            t = np.array(t, dtype=float)\n            log_df = np.interp(t, T, log_dfs, left=log_dfs[0], right=log_dfs[-1])\n            return np.exp(log_df)\n\n        par_model = par_from_d(df_func, T)\n        err = par_model - par\n        return float(np.mean(err**2))\n\n    # initializing with a first guess. for long run level we need something like the long-run yield level. that's why we use long term yields as guess\n    b0_0 = float(np.nanmedian(par[-3:])) if len(par) &gt;= 3 else float(np.nanmedian(par))\n    x0 = np.array([b0_0, -0.02, 0.02, 0.01, 1.5, 5.0], dtype=float)\n\n    # we use a type of quasi-Newton method for nonlinear optimization of parameters\n    pred = minimize(obj, x0, method=\"L-BFGS-B\")\n    theta = pred.x\n\n    nss_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    nss_z_grid = nss_zero(nss_grid, *theta)\n    nss_df_grid = np.exp(-nss_z_grid * nss_grid)\n\n    nss_fwd_grid = -np.gradient(\n        np.log(np.clip(nss_df_grid, min_d, None)), nss_grid\n    )\n\n    def nss_df_func(t):\n        t = np.array(t, dtype=float)\n        return np.exp(-nss_zero(t, *theta) * t)\n\n    curve = {\n        \"name\": \"NSS\",\n        \"grid\": nss_grid,\n        \"df_grid\": nss_df_grid,\n        \"z_grid\": nss_z_grid,\n        \"fwd_grid\": nss_fwd_grid,\n        \"df_func\": nss_df_func,\n        \"theta\": theta,\n    }\n\n\n    z_p = nss_zero(T, *theta)\n    d_p = np.exp(-z_p * T)\n    log_d_p = np.log(np.clip(d_p, min_d, None))\n    def df_func_p(tt):\n        return np.exp(\n            np.interp(np.array(tt, float), T, log_d_p, left=log_d_p[0], right=log_d_p[-1])\n        )\n    par_fit = par_from_d(df_func_p, T)\n\n    return curve, par_fit, pred\n\n\nnss_curve_data, par_fit, pred = nss_curve(T, par)\ntheta = nss_curve_data[\"theta\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"nss\"] = nss_curve_data\n\nprint(\"final MSE:\", pred.fun)\nprint(\"theta = [b0,b1,b2,b3,tau1,tau2] =\", np.round(theta, 6))\n\nplt.figure()\nplt.plot(T, par * 100.0, \"o\", label=\"Market par\")\nplt.plot(T, par_fit * 100.0, \"-o\", label=\"NSS implied par\")\nplt.title(\"NSS Fit to Par Yields\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Par Yield\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"NSS Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"NSS Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nfinal MSE: 3.5093289069366877e-07\ntheta = [b0,b1,b2,b3,tau1,tau2] = [ 0.053115 -0.014698 -0.031572 -0.007968  1.500251  5.000021]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "title": "1. Fixed income and yield curve construction",
    "section": "9) QP curve: smooth discount factors under exact par-bond fit",
    "text": "9) QP curve: smooth discount factors under exact par-bond fit\nAnother approach for building a yield curve that:\n\nmatches par-bond pricing equations exactly (like bootstrapping) or near-exactly (like NSS),\nis smooth,\nwill result in a positive and increasing DF curve.\n\ncan come from a Quadratic Program (QP) if the variables are discount factors on a grid and constraints are linear.\n\n9.1 Variables\nWe Pick a grid of cashflow times (like semiannual up to 30Y): \\(t_1,t_2,\\dots,t_M\\)\nwe want to get to $ = (D(t_1),,D(t_M)) $\n\n\n9.2 constraints\nFor a maturity \\(T\\) (present on the grid), par yield \\(c\\) and frequency \\(f\\):\n\\(1=\\sum_{i=1}^{n}\\frac{c}{f}D(t_i)+D(T)\\)\nThis is linear in \\(D(\\cdot)\\), so it becomes one row of: \\(A\\mathbf{d}=\\mathbf{1}\\)\nPositivity: \\(D(t_k)\\ge D_{min}\\)\nMonotone decreasing: \\(D(t_{k+1})\\le D(t_k)\\)\nThese are linear inequalities, so the problem stays convex and QP-solvable.\n\nimport cvxpy as cp\n\n\ndef qp_build_t_grid(T_obs, f):\n    T_max = float(np.max(T_obs))\n    n_grid = int(round(T_max * f))\n    t_grid = np.unique(\n        np.concatenate(\n            [\n                np.array([i / f for i in range(1, n_grid + 1)], dtype=float),\n                T_obs,\n            ]\n        )\n    )\n    t_grid = np.array(sorted(t_grid), dtype=float)\n    grid_index = {float(np.round(t, 10)): i for i, t in enumerate(t_grid)}\n    return t_grid, grid_index\n\n\ndef qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d):\n    d = cp.Variable(len(t_grid))\n\n    constraints = []\n    constraints += [d &gt;= min_d]\n    constraints += [d[1:] &lt;= d[:-1]]\n\n    for Tk, yk in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            key = float(np.round(Tk, 10))\n            if key in grid_index:\n                i = grid_index[key]\n                df_target = float(np.exp(-yk * Tk))\n                constraints += [d[i] == df_target]\n\n    for Tk, ck in zip(T_obs, par_mkt, strict=True):\n        if Tk &lt; 1.0:\n            continue\n        keyT = float(np.round(Tk, 10))\n        if keyT not in grid_index:\n            continue\n        iT = grid_index[keyT]\n        n = int(round(Tk * f))\n\n        coupon_idx = []\n        for j in range(1, n + 1):\n            key = float(np.round(j / f, 10))\n            coupon_idx.append(grid_index[key])\n\n        constraints += [cp.sum((ck / f) * d[coupon_idx]) + d[iT] == 1.0]\n\n    return d, constraints\n\n\n\n\n\n\n9.3 Smoothness objective (quadratic)\nA simple convex smoothness penalty is the squared second difference of Discount Factors:\n\\(\\min_{\\mathbf{d}} \\ |\\Delta^2\\mathbf{d}|_2^2\\)\nwhere \\(\\Delta^2 d_k = d_{k+2}-2d_{k+1}+d_k\\). (Discrete version)\nThis makes the optimizer prefer sequences of discount factors that have small curvature everywhere, which results a smooth DF curve with fewer oscillations and jumps.\nwe can also add a mild “keep close to a prior curve” penalty: \\(\\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\nTotal objective: \\(\\min_{\\mathbf{d}} \\ \\lambda|\\Delta^2\\mathbf{d}|_2^2 + \\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\n\n\n9.4 zero and forward curves\nOnce we have \\(D(t)\\) on a grid:\n\\(z(t_k)=-\\ln D(t_k)/t_k\\)\n\\(f(t_k)\\approx -\\dfrac{\\ln D(t_{k+1})-\\ln D(t_k)}{t_{k+1}-t_k}\\)\n\ndef qp_solve(t_grid, d, constraints, par_mkt, f, min_d):\n    lam = 1e4\n    eps = 1e-4\n    prior_rate = (\n        float(np.nanmedian(par_mkt[-3:])) if len(par_mkt) &gt;= 3 else float(np.nanmedian(par_mkt))\n    )\n    d_prior = np.exp(-prior_rate * t_grid)\n\n    d2 = d[2:] - 2 * d[1:-1] + d[:-2]\n    obj = cp.Minimize(lam * cp.sum_squares(d2) + eps * cp.sum_squares(d - d_prior))\n\n    prog = cp.Problem(obj, constraints)\n    prog.solve(solver=cp.OSQP)\n\n    d_sol = np.array(d.value).astype(float)\n    d_sol = np.clip(d_sol, min_d, None)\n\n    return d_sol, prog.status, prog.value\n\n\ndef qp_build_curve(t_grid, d_sol):\n    qp_grid = np.linspace(max(1 / 12, t_grid.min()), 30.0, 1000)\n    qp_log_d = np.log(d_sol)\n    qp_log_df_grid = np.interp(qp_grid, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n\n    qp_df_grid = np.exp(qp_log_df_grid)\n    qp_z_grid = -np.log(qp_df_grid) / np.maximum(qp_grid, 1e-8)\n    qp_fwd_grid = -np.gradient(np.log(qp_df_grid), qp_grid)\n\n    def qp_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(t, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n        return np.exp(log_df)\n\n    curve = {\n        \"name\": \"QP DF\",\n        \"grid\": qp_grid,\n        \"df_grid\": qp_df_grid,\n        \"z_grid\": qp_z_grid,\n        \"fwd_grid\": qp_fwd_grid,\n        \"df_func\": qp_df_func,\n    }\n\n    return curve\n\n\ndef qp_curve(labels, par_mkt, f=2, min_d=1e-10):\n    T_obs = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        T_obs.append(int(labu[:-1]) / 12.0 if labu.endswith(\"M\") else float(int(labu[:-1])))\n    T_obs = np.array(T_obs, dtype=float)\n\n    idx = np.argsort(T_obs)\n    T_obs = T_obs[idx]\n    par_mkt = par_mkt[idx]\n    labels = [labels[i] for i in idx]\n\n    t_grid, grid_index = qp_build_t_grid(T_obs, f)\n    d, constraints = qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d)\n    d_sol, status, value = qp_solve(t_grid, d, constraints, par_mkt, f, min_d)\n\n    curve = qp_build_curve(t_grid, d_sol)\n\n    state = {\n        \"t_grid\": t_grid,\n        \"d_sol\": d_sol,\n        \"constraints\": constraints,\n        \"status\": status,\n        \"value\": value,\n    }\n\n    return curve, state\n\n\nqp_curve_data, qp_state = qp_curve(labels, par)\n\ncurves[\"qp\"] = qp_curve_data\n\nprint(\"status:\", qp_state[\"status\"], \",  value:\", qp_state[\"value\"])\n\n\nplt.figure()\nplt.plot(qp_state[\"t_grid\"], qp_state[\"d_sol\"], \"o\", markersize=3, label=\"QP DF\")\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"df_grid\"], \"-\", label=\"Discount Factor (log-linear)\")\nplt.title(\"QP Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"QP Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"QP Instantaneous Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nstatus: optimal ,  value: 1.2368745747617473",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#synthetic-bond-issuance-issued-at-par",
    "title": "1. Fixed income and yield curve construction",
    "section": "11) Synthetic Bond Issuance (Issued at Par)",
    "text": "11) Synthetic Bond Issuance (Issued at Par)\nbecause we don’t have official data for this part, We simulate a monthly issuance program using par yields to create a rolling book of bonds so we can analyze our models and implement the next topics. Each month, we pretend the Treasury issues new par bonds at a few maturities (like 2Y, 5Y, 10Y, 30Y). The coupon of each new bond is set to that month’s par yield at that maturity (from the dataset). Because coupon = par yield at issuance, each new bond starts at price approximately equal to 1 (par).\n\nDetails of the bond\n\nfor each month \\(t_0\\) we create a new bond with maturity \\(T\\) and frequency \\(f\\)\n\\(f\\) = semiannual\n\\(T = {2Y, 5Y, 10Y, 30Y}\\)\ncoupon: \\(c_d(T)=y_d(T)\\) (the market par yield at that date and maturity)\nnotional \\(N=1\\)\n\nBasically we buy bonds with 4 different maturities every month and keep it and get interest every month until maturity of those bonds. so we have 4 books for 4 different bonds (maturities). and our portfolio is based on these four books.\nSo each month the outgoing cashflow is buying the bonds and ingoing is all the interest and maybe principal of all the bonds that we have bought.\n\nissue_maturities = [2, 5, 10, 30]\nissue_labels = {2: \"2Y\", 5: \"5Y\", 10: \"10Y\", 30: \"30Y\"}\n\n\nmonth_end_curve = df_dec[tenor_cols].resample(\"ME\").last()\nissue_dates = month_end_curve.index\n\n\ndef yearfrac(t0, t1):\n    return (t1 - t0).days / 365\n\n\ndef bond_cashflows(c, T, f=2):\n    times = np.arange(1 / f, T + 1e-9, 1 / f)\n    cfs = np.full_like(times, c / f)\n    cfs[-1] += 1.0\n    return times, cfs\n\n\ndef price_bond(df_func, times, cfs, delta):\n    mask = times &gt; delta + 1e-12\n    if not np.any(mask):\n        return 0.0\n    t_rem = times[mask] - delta\n    cf_rem = cfs[mask]\n    return float(np.sum(cf_rem * df_func(t_rem)))\n\n\n\nissuance_book = {T: [] for T in issue_maturities}\nfor d in issue_dates:\n    row = month_end_curve.loc[d]\n    for T in issue_maturities:\n        label = issue_labels[T]\n        c = float(row.get(label, np.nan))\n        if not np.isfinite(c):\n            continue\n        times, cfs = bond_cashflows(c, T, f)\n        issuance_book[T].append({\n            \"issue_date\": d,\n            \"coupon\": c,\n            \"times\": times,\n            \"cfs\": cfs,\n        })\n\nissuance_summary = pd.DataFrame({\n    \"n_bonds\": {T: len(issuance_book[T]) for T in issue_maturities},\n    \"first_issue\": {T: issuance_book[T][0][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n    \"last_issue\": {T: issuance_book[T][-1][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n})\n\nprint(\"Issuance summary:\")\ndisplay(issuance_summary)\n\nIssuance summary:\n\n\n\n\n\n\n\n\n\nn_bonds\nfirst_issue\nlast_issue\n\n\n\n\n2\n433\n1990-01-31\n2026-01-31\n\n\n5\n433\n1990-01-31\n2026-01-31\n\n\n10\n433\n1990-01-31\n2026-01-31\n\n\n30\n386\n1990-01-31\n2026-01-31",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#pricing-an-issued-bond-at-a-later-date",
    "title": "1. Fixed income and yield curve construction",
    "section": "12) Pricing an Issued Bond at a Later Date",
    "text": "12) Pricing an Issued Bond at a Later Date\nIf issue date be \\(t_0\\) and valuation date be \\(t\\). Elapsed time is the amount of time that has been passed from the bond issued at \\(t_0\\) in valuation time \\(t\\): \\(\\Delta=\\tau(t_0,t)\\)\nreminder: \\(\\tau(t_0,t_1)=\\dfrac{\\text{days}(t_0,t_1)}{365}\\)\nOriginal scheduled payment times from issue are \\(t_i=i/f\\). we only price the bonds that \\(t_i&gt;\\Delta\\) because these are the cashflows that have accured up until time \\(t\\) and the remaining time to payment from valuation date for each cashflow is: \\(\\tau_i(t)=t_i-\\Delta\\)\nPrice using the curve at valuation date \\(t\\): \\(P_t=\\sum_{i:t_i&gt;\\Delta} CF_i\\,D_t(\\tau_i(t))\\)\nFor a book of issues \\(\\mathcal{B}_t\\) with weights \\(w_b\\) (we don’t have weights here): \\(PV_t=\\sum_{b\\in \\mathcal{B}_t} w_b\\,P_t(b)\\)\n\ndef book_pv_cutoff(valuation_date, df_func, cutoff_date):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], valuation_date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ndef shifted_df_func(df_func, shift_func):\n    def _f(t):\n        t = np.array(t, dtype=float)\n        return df_func(t) * np.exp(-shift_func(t) * t)\n    return _f\n\n\ndef key_bump_func(key, bump_bp=1.0):\n    values = np.zeros(len(issue_maturities), dtype=float)\n    key_idx = issue_maturities.index(key)\n    values[key_idx] = bump_bp / 10000.0\n\n    def shift(t):\n        t = np.array(t, dtype=float)\n        return np.interp(t, issue_maturities, values, left=0.0, right=0.0)\n\n    return shift\n\n\ndef curve_date_for(d):\n    if d in df_dec.index:\n        return d\n    idx = df_dec.index.searchsorted(d, side=\"right\") - 1\n    if idx &lt; 0:\n        return None\n    return df_dec.index[idx]\n\n\ndef book_pv(date, df_func):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ncurves_cache = {}\n\n\ndef get_curves_for(date):\n    if date in curves_cache:\n        return curves_cache[date]\n    out = build_curves_for_date(date)\n    if out is None:\n        curves_cache[date] = None\n        return None\n    _, curves_d, _ = out\n    curves_cache[date] = curves_d\n    return curves_d\n\n\n\n\nmetrics_rows = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df0 = curve[\"df_func\"]\n        pv0, _ = book_pv_cutoff(date, df0, cutoff_date=date)\n        metrics_rows.append({\"date\": date, \"method\": method, \"pv\": pv0})\n\nmetrics_df = pd.DataFrame(metrics_rows).set_index([\"date\", \"method\"]).sort_index()\n\n\n\nmethods = [\"loglinear\", \"pchip\", \"nss\", \"qp\"]\n\npv_total = (\n    metrics_df.reset_index()\n    .pivot(index=\"date\", columns=\"method\", values=\"pv\")\n    .sort_index()\n)\n\npv_rows = []\nfailed = []\n\nfor date in issue_dates:\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df_func = curve[\"df_func\"]\n        _, buckets = book_pv_cutoff(date, df_func, cutoff_date=date)\n        for T in issue_maturities:\n            pv_rows.append({\"date\": date, \"method\": method, \"maturity\": T, \"pv\": buckets.get(T, 0.0)})\n\npv_df = pd.DataFrame(pv_rows)\npv_buckets = pv_df.pivot_table(index=\"date\", columns=[\"method\", \"maturity\"], values=\"pv\").sort_index()\n\nprint(\"Total PV by method (last day):\")\ndisplay(pv_total)\n\nprint(\"Bucket PV (last date)\")\nlast_date = pv_df[\"date\"].max()\nlast_bucket = pv_df[pv_df[\"date\"] == last_date].pivot_table(index=\"maturity\", columns=\"method\", values=\"pv\")\ndisplay(last_bucket)\n\nplt.figure()\nfor method in pv_total.columns:\n    plt.plot(pv_total.index, pv_total[method], label=method)\nplt.title(\"Synthetic Book Total PV (Monthly)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nplt.figure()\nlast_bucket.plot(kind=\"bar\", ax=plt.gca())\nplt.title(f\"Each bucket PV by Method ({last_date.date()})\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nTotal PV by method (last day):\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\ndate\n\n\n\n\n\n\n\n\n1990-01-31\n3.999816\n3.999349\n4.001830\n4.004113\n\n\n1990-02-28\n8.007579\n8.006109\n8.012242\n8.016726\n\n\n1990-03-31\n12.003136\n12.000914\n12.004738\n12.017158\n\n\n1990-04-30\n15.838917\n15.837035\n15.838465\n15.859415\n\n\n1990-05-31\n20.350563\n20.347476\n20.353669\n20.375817\n\n\n...\n...\n...\n...\n...\n\n\n2025-09-30\n484.048043\n485.413949\n485.289343\n486.914561\n\n\n2025-10-31\n486.071558\n486.930897\n487.319872\n488.950688\n\n\n2025-11-30\n487.714383\n489.811404\n489.252157\n491.052541\n\n\n2025-12-31\n482.340428\n484.351486\n483.847004\n485.803466\n\n\n2026-01-31\n481.069937\n482.642714\n482.367938\n484.081282\n\n\n\n\n433 rows × 4 columns\n\n\n\nBucket PV (last date)\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\nmaturity\n\n\n\n\n\n\n\n\n2\n24.258462\n24.266940\n24.265987\n24.267663\n\n\n5\n60.518216\n60.565183\n60.549000\n60.551390\n\n\n10\n115.921913\n116.063170\n116.097292\n116.098827\n\n\n30\n280.371346\n281.747421\n281.455658\n283.163402",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "href": "notebooks/01_yield_curve_bond_pricing_and_risk.html#risk-and-sensitivity-measures-of-bonds",
    "title": "1. Fixed income and yield curve construction",
    "section": "15) Risk and sensitivity measures of bonds",
    "text": "15) Risk and sensitivity measures of bonds\n\n15.1 PV01/ DV01\nfor measuring the risk of these bonds, we can use the sensitivity of the price of bond to a little change in interest rates.\nPV01 is the change in PV of the bond when the curve bumps by 1 basis point. DV01 is the value in dollar unit of that which is kind of the same concept\n\\(\\dfrac{dP}{dy}=-\\sum_{i=1}^n CF_i,t_i,D(t_i)\\)\nFor approximation, we take \\(\\Delta=0.0001\\) (1 basis point move), Parallel-shifted discount factor will be:\n\\(D^{up}(t)=\\exp(-(z(t)+\\Delta)t)\\)\nPV01 will be:\n\\(PV01=P_0-P_{up}\\)\n\n\n15.2 Convexity\nconvexity is basically the sensitivity of PV01 to a little change in curve. and it’s the second order partial derivative of price from yield\n\\(\\dfrac{d^2P}{dy^2}=\\sum_{i=1}^n CF_i,t_i^2,D(t_i)\\)\nUsing central differences with shift size \\(\\Delta\\):\n\\(Conv=\\dfrac{P_{down}+P_{up}-2P_0}{P_0\\Delta^2}\\) where \\(P_{up}\\) uses \\(+\\Delta\\) and \\(P_{down}\\) uses \\(-\\Delta\\).\n\n\n\n15.3 Key Rate Duration (KRD)\nthe problem is that we assume that yields parallel shift. but they can twist. if 2Y goes up it doesn’t mean 10Y will go up exactly the same amount. (Duration is a normalized version of PV01)\nSo we choose key tenors \\(k_1&lt;k_2&lt;\\dots&lt;k_m\\). as our key rates and measure the sensitivity of them while the other tenors in the curve stay the same.\nTent / triangular bump shape\nDefine localized bump functions \\(b_j(t)\\) such that \\(b_j(K_j)=1\\) and \\(b_j(t)=0\\) outside a neighborhood.\nA triangular bump with edges \\(L_j&lt;R_j\\):\n\\(b_j(t)=0\\) for \\(t\\le L_j\\)\n\\(b_j(t)=\\dfrac{t-L_j}{K_j-L_j}\\) for \\(L_j&lt;t\\le K_j\\)\n\\(b_j(t)=\\dfrac{R_j-t}{R_j-K_j}\\) for \\(K_j&lt;t\\le R_j\\)\n\\(b_j(t)=0\\) for \\(t&gt;R_j\\)\nBumped zero curve (only around key \\(k_j\\)):\n\\(z^{(j)}(t)=z(t)+\\Delta b_j(t)\\)\nSo bumped discount factors:\n\\(D^{(j)}(t)=\\exp(-(z(t)+\\Delta b_j(t))t)\\)\nPrice under the key bump:\n\\(P^{(j)}(\\Delta)=\\sum_{i=1}^n CF_i,D(t_i),e^{-\\Delta,b_j(t_i),t_i}\\)\nKRD definition (finite-difference)\n\\(KRD_j=\\dfrac{P_0-P^{(j)}}{P_0\\Delta}\\)\n\n\nrisk_rows = []\nkrd_rows = []\n\nfor (date, method), row in metrics_df.iterrows():\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    df_func = curves_d[method][\"df_func\"]\n    pv0 = row[\"pv\"]\n\n    shift_up = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up, _ = book_pv(date, shift_up)\n    pv_dn, _ = book_pv(date, shift_dn)\n\n    pv01 = (pv_dn - pv_up) / 2.0\n    convexity = (pv_up + pv_dn - 2.0 * pv0) / (pv0 * (0.0001 ** 2)) if pv0 != 0 else np.nan\n\n    risk_rows.append({\n        \"date\": date,\n        \"method\": method,\n        \"pv01\": pv01,\n        \"convexity\": convexity,\n    })\n\n    for k in issue_maturities:\n        bump = key_bump_func(k, bump_bp=1.0)\n        df_bump = shifted_df_func(df_func, bump)\n        pv_bump, _ = book_pv(date, df_bump)\n        krd = (pv0 - pv_bump) / 0.0001\n        krd_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"key\": k,\n            \"krd\": krd,\n        })\n\nrisk_df = pd.DataFrame(risk_rows).set_index([\"date\", \"method\"]).sort_index()\nmetrics_df = metrics_df.join(risk_df)\n\nkrd_df = pd.DataFrame(krd_rows).set_index([\"date\", \"method\", \"key\"]).sort_index()\n\nprint(\"Risk metrics (last date)\")\ndisplay(metrics_df[[\"pv01\", \"convexity\"]].tail(4))\n\nprint(\"KRD for last date\")\ndisplay(krd_df.tail(4))\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"pv01\"], label=method)\nplt.title(\"PV01 of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV01\")\nplt.legend()\nplt.show()\n\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"convexity\"], label=method)\nplt.title(\"Convexity of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Convexity\")\nplt.legend()\nplt.show()\n\n\n\nmethods = sorted(metrics_df.index.get_level_values(\"method\").unique())\nkrd_panel = krd_df.reset_index()\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\naxes = axes.flatten()\nfig.subplots_adjust(right=0.88)\n\nfor ax, method in zip(axes, methods, strict=True):\n    data = krd_panel[krd_panel[\"method\"] == method]\n    pivot = data.pivot(index=\"date\", columns=\"key\", values=\"krd\").reindex(columns=issue_maturities)\n    im = ax.imshow(pivot.values.T, aspect=\"auto\", origin=\"lower\")\n    ax.set_title(method)\n    ax.set_yticks(range(len(issue_maturities)))\n    ax.set_yticklabels([f\"{k}Y\" for k in issue_maturities])\n\n    tick_idx = np.linspace(0, len(pivot.index) - 1, 6).astype(int)\n    ax.set_xticks(tick_idx)\n    ax.set_xticklabels([pivot.index[i].strftime(\"%Y\") for i in tick_idx])\n\ncax = fig.add_axes([0.90, 0.15, 0.02, 0.7])\nfig.colorbar(im, cax=cax, label=\"KRD\")\nfig.suptitle(\"KRD Heatmap Across Time\", y=1.02)\nplt.tight_layout(rect=[0, 0, 0.88, 1])\nplt.show()\n\nRisk metrics (last date)\n\n\n\n\n\n\n\n\n\n\npv01\nconvexity\n\n\ndate\nmethod\n\n\n\n\n\n\n2026-01-31\nloglinear\n0.357311\n107.577415\n\n\nnss\n0.359623\n108.064523\n\n\npchip\n0.358863\n107.715321\n\n\nqp\n0.362471\n108.928968\n\n\n\n\n\n\n\nKRD for last date\n\n\n\n\n\n\n\n\n\n\n\nkrd\n\n\ndate\nmethod\nkey\n\n\n\n\n\n2026-01-31\nqp\n2\n199.706778\n\n\n5\n584.865209\n\n\n10\n1608.487463\n\n\n30\n1104.026025",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html",
    "href": "notebooks/02_portfolio_optimization_MV_models.html",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "",
    "text": "1) Introduction\nThis notebook builds and backtests long-only portfolios under a realistic workflow:\nIn this project we have a dataset for all the stocks in Nasdaq from 1970 to 2026. we want to pick 100 stocks from them and see how we can weight them to reach a stable positive return that would be considered a better investment decision than just investing in all the stocks in equal weights.\nFor comparing the strategies and different models we can use return and risk. we always want higher return and lower risk. lower risk decreases the probability of negative or too negative return and higher return is basiacally what we get on our money. there is a trade off between wanting higher return and lower risk. we can’t minimize risk and reach the highest return. if we want lower risk we have to accept lower return.\nIn this project we use some of the models for weighting assets so we can reach the best portfolios in the risk-return trade off. we use other models and approaches in future projects\nWe trade \\(N\\) stocks, indexed by \\(i \\in \\{1,\\dots,N\\}\\), at daily dates \\(t\\).\nMarket data - Close price: \\(P_{t,i}\\) - Volume (shares): \\(V_{t,i}\\) - Dollar volume (we use these to identify the most liquid stocks in each rebalance period): \\(DV_{t,i} = P_{t,i} V_{t,i}\\)\nReturns - Simple return: \\(r_{t,i} = \\frac{P_{t,i}}{P_{t-1,i}} - 1\\) - Log return: \\(r_{t,i} = \\log(P_{t,i}) - \\log(P_{t-1,i})\\) (we use simple here)\nWe stack daily returns into a vector \\(r_t \\in \\mathbb{R}^N\\) and an estimation matrix \\(R_t \\in \\mathbb{R}^{T \\times N}\\) using the last \\(T\\) days before a rebalance.\nPortfolio - Portfolio weights (held through each period \\(t\\)): \\(w_t \\in \\mathbb{R}^N\\) - Budget constraint (sum of all the weights should be 1. we don’t use leverage for this project): \\(\\mathbf{1}^\\top w_t = 1\\)\nRisk-free rate is used for calculation of Sharpe Ratio - Annual: \\(r_f^{(ann)}\\) - Daily (compounded): \\(r_f^{(d)} = (1+r_f^{(ann)})^{1/252} - 1\\)\nAnnualization - If \\(\\mu^{(d)}\\) is a daily mean return vector, then \\(\\mu^{(ann)} = 252\\,\\mu^{(d)}\\) - If \\(\\Sigma^{(d)}\\) is a daily covariance matrix, then \\(\\Sigma^{(ann)} = 252\\,\\Sigma^{(d)}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#introduction",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#introduction",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "",
    "text": "Long-only constraint: \\(w_t \\ge 0\\)\nOptional cap (for making models diverse more and don’t overfit on some assets, we define a \\(w_{\\max}\\) which is the max weight an asset can get in the portfolio): \\(w_{t,i} \\le w_{\\max}\\)\n\n\n\n\n\nImports and plotting style\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport cvxpy as cp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.dates import DateFormatter\nfrom scipy.optimize import minimize\nfrom sklearn.covariance import LedoitWolf, OAS\nfrom cycler import cycler\n\ncolors = [\"#069AF3\",\"#FE420F\", \"#00008B\", \"#008080\", \"#800080\",\n          \"#7BC8F6\", \"#0072B2\",\"#04D8B2\", \"#CC79A7\", \"#FF8072\", \"#9614fa\", \"#DC143C\"]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\nplt.rcParams.update({\n    \"figure.figsize\": (6, 3),\n    \"figure.dpi\": 300,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 10,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 10,\n})\n\ndef make_color_map(names, palette=colors):\n    names = list(names)\n    return {name: palette[i % len(palette)] for i, name in enumerate(names)}\n\n\n\ninitializing the parameters and data\n\nrf = 0.04\n\nblend = {\n    \"MV (SampleCov)\": 0.15,\n    \"MV (LedoitWolf)\": 0.15,\n    \"MV (OAS)\": 0.15,\n    \"MV (EWMA)\": 0.15,\n    \"Ridge MV\": 0.15,\n\n    \"MaxSharpe\": 0.10,                 # keep your SLSQP baseline\n    \"MaxSharpe (FrontierGrid)\": 0.10,  # NEW\n\n    \"MinVar (SampleCov)\": 0.20,\n    \"MinVar (LedoitWolf)\": 0.20,\n    \"MinVar (OAS)\": 0.20,\n    \"MinVar (EWMA)\": 0.20,\n}\n\nstrategy_cov_key = {\n    \"EW\": \"LedoitWolf\",\n\n    \"MinVar (SampleCov)\": \"SampleCov\",\n    \"MinVar (LedoitWolf)\": \"LedoitWolf\",\n    \"MinVar (OAS)\": \"OAS\",\n    \"MinVar (EWMA)\": \"EWMA\",\n\n    \"MV (SampleCov)\": \"SampleCov\",\n    \"MV (LedoitWolf)\": \"LedoitWolf\",\n    \"MV (OAS)\": \"OAS\",\n    \"MV (EWMA)\": \"EWMA\",\n\n    \"Ridge MV\": \"LedoitWolf\",\n\n    \"MaxSharpe\": \"LedoitWolf\",                 # baseline SLSQP\n    \"MaxSharpe (FrontierGrid)\": \"LedoitWolf\",  # NEW\n}\nstrategy_names = list(strategy_cov_key.keys())\nstrategy_colors = make_color_map(strategy_names)\n\n\ndef print_warn(msg):\n    print(f\"[WARN] {msg}\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#load-data-and-compute-returns",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#load-data-and-compute-returns",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "2) Load data and compute returns",
    "text": "2) Load data and compute returns\nthe data used in this project can be downloaded from here (Stooq US (nasdaq) daily market data)\n\ndf = pd.read_parquet(r\"..\\data\\nasdaq_all_close_volume.parquet\")\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"Date\"]).sort_values(\"Date\")\n\nclose_map, vol_map = {}, {}\nfor c in df.columns:\n    c = str(c)\n    if c == \"Date\" or \"__\" not in c:\n        continue\n    t, f = c.rsplit(\"__\", 1)\n    f = f.lower()\n    if f == \"close\":\n        close_map[t] = c\n    elif f == \"volume\":\n        vol_map[t] = c\n\ncommon = sorted(set(close_map).intersection(vol_map))\n\nclose_prices = df[[close_map[t] for t in common]].copy(); close_prices.columns = common\nvolumes = df[[vol_map[t] for t in common]].copy(); volumes.columns = common\nclose_prices.index = df[\"Date\"]\nvolumes.index = df[\"Date\"]\n\nclose_prices = close_prices.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).astype(np.float32)\nvolumes = volumes.apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan).astype(np.float32)\n\nstart = pd.Timestamp(\"2016-01-01\")\nclose_prices = close_prices.loc[close_prices.index &gt;= start]\nvolumes = volumes.loc[volumes.index &gt;= start]\nend = close_prices.index.max()\nclose_prices = close_prices.loc[close_prices.index &lt;= end]\nvolumes = volumes.loc[volumes.index &lt;= end]\n\nidx = close_prices.index.intersection(volumes.index)\ncols = close_prices.columns.intersection(volumes.columns)\nclose_prices = close_prices.loc[idx, cols]\nvolumes = volumes.loc[idx, cols]\nfirst_close = close_prices.apply(pd.Series.first_valid_index)\nfirst_vol   = volumes.apply(pd.Series.first_valid_index)\n\nfirst_date = pd.concat([first_close, first_vol], axis=1).max(axis=1)\n\nreturns = close_prices.pct_change(fill_method=None)\nreturns = returns.replace([np.inf, -np.inf], np.nan).astype(np.float32)\n\nprint(\"close_prices:\", close_prices.shape, \"volumes:\", volumes.shape, \"returns:\", returns.shape)\nprint(\"Date range:\", returns.index.min().date(), \"to\", returns.index.max().date())\n\nclose_prices: (2532, 4382) volumes: (2532, 4382) returns: (2532, 4382)\nDate range: 2016-01-04 to 2026-01-28",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#rebalance-dates",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#rebalance-dates",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "3) Rebalance dates",
    "text": "3) Rebalance dates\nfor optimizing a portfolio we have to use past data (like mu and cov estimation) to optimize the model on past and use the optimal weights in future and expect to get same results as what we got from past. we will never get the same results unless market exactly repeats itself. so we have to test our model out of sample to see the real performance. Also we have to use rebalancing. for example if we want to test a model in one year, we can optimize the model on the past 5 years and test it on this year, but in one year markets can change a lot and estimations of model become irrelevant. so we can use rebalancing and for each month of that year, take the past year of that month as our in-sample and test the optimal weights on that month and then go to the next month and repeat this process every month. in this way we include up to date data in our model and update the weights faster and adapt to market regimes faster.\nIn this project we use monthly rebalancing with 1 year lookback window for each month.\nWe rebalance at the last available trading day of each period\n\nrebal_dates_raw = (\n    returns.groupby(pd.Grouper(freq=\"ME\"))\n           .apply(lambda x: x.index[-1])\n           .dropna()\n)\nrebal_dates = pd.DatetimeIndex(rebal_dates_raw.values)\n\n\nprint(\"Candidate rebalance dates:\", len(rebal_dates))\nprint(\"First 3:\", [d.date() for d in rebal_dates[:3]])\nprint(\"Last 3:\", [d.date() for d in rebal_dates[-3:]])\n\nCandidate rebalance dates: 121\nFirst 3: [datetime.date(2016, 1, 29), datetime.date(2016, 2, 29), datetime.date(2016, 3, 31)]\nLast 3: [datetime.date(2025, 11, 28), datetime.date(2025, 12, 31), datetime.date(2026, 1, 28)]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#liquidity-filtered-stock-selection",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#liquidity-filtered-stock-selection",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "4) Liquidity-filtered stock selection",
    "text": "4) Liquidity-filtered stock selection\nRight now our dataset contains hundreds of stocks. At each rebalance date \\(t \\in \\mathcal{T}\\), we want to include some of the stocks that have the most liquidity and certain data in that date. So we want to build a tradable combination of stocks \\({U}_t\\).\n\n4.1 Minimum history\nA ticker is included only if it has at least \\(D\\) valid daily observations before \\(t\\). The asset must exist long enough that estimates are meaningful.\nWe set \\(D\\) as 252 days or 1 year\n\n\n4.2 Average Dollar Volume (ADV)\nWe define daily dollar volume as Volume multiplied by Prices: \\[\nDV_{\\tau,i} = P_{\\tau,i} V_{\\tau,i}\n\\]\nCompute average dollar volume over a window of length \\(L\\) (using only \\(\\tau &lt; t\\)): \\[\nADV_{t,i} = \\frac{1}{L} \\sum_{\\tau=t-L}^{t-1} DV_{\\tau,i}\n\\]\nWe set \\(L\\) as 1 year to capture the stocks with most \\(ADV\\) in the past year of that month.\n\n\n4.3 Selection rule (Top-K liquidity)\nLet \\(K\\) be the target universe size (We use 100).\nWe select: \\[\n{U}_t = \\operatorname{TopK}\\big(ADV_{t,i}\\big)\n\\]\nin this way we don’t have survivorship bias and we only use big stocks of that time, not the stocks that we already know are big now but were not back then.\n\ndef select_liquid_universe(dt, close_prices, volumes, top_n, liq_lookback, min_listing_days, min_obs):\n    idx = close_prices.index\n    if dt not in idx:\n        return [], pd.Series(dtype=np.float32)\n\n    pos = idx.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n\n    need = max(min_listing_days, liq_lookback)\n    if pos &lt; need:\n        return [], pd.Series(dtype=np.float32)\n\n    cutoff_date = idx[pos - min_listing_days]\n    seasoned = (first_date.notna()) & (first_date &lt;= cutoff_date)\n    cols = close_prices.columns[seasoned.reindex(close_prices.columns).fillna(False).values]\n    \n    start = pos - liq_lookback\n    end = pos\n    c = close_prices.iloc[start:end][cols]\n    v = volumes.iloc[start:end][cols]\n    dv = c * v\n\n    obs_ok = dv.notna().sum(axis=0) &gt;= min_obs\n    pos_ok = (dv &gt; 0).sum(axis=0) &gt;= min_obs\n\n    selected = dv.columns[obs_ok & pos_ok]\n\n    adv = dv[selected].mean(axis=0, skipna=True).replace([np.inf, -np.inf], np.nan).dropna()\n    if len(adv) == 0:\n        return [], pd.Series(dtype=np.float32)\n\n    top = adv.nlargest(min(int(top_n), len(adv)))\n    return top.index.tolist(), top.astype(np.float32)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#expected-return-model-momentum-signal-for-mu_t",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#expected-return-model-momentum-signal-for-mu_t",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "5) Expected return model: momentum signal for \\(\\mu_t\\)",
    "text": "5) Expected return model: momentum signal for \\(\\mu_t\\)\nMin-Var models only try to optimize based on risk (Covariance) but Mean-Var and Max-Sharpe models need an expected-return vector \\(\\mu_t\\). if we use an average of returns in a period, it can be noisy and the model will not generalize based on that. so we need a clear and stable estimation of \\(\\mu\\) in each period and rebalance.\nWe use a simple momentum model that is:\n\neasy to compute\nconsistent across universes\nuses only past returns\n\n\n5.1 Lookback cumulative return\nWe pick a momentum lookback length \\(H\\) (like 6 months or 126 trading days) and then Define cumulative simple return for asset \\(i\\): \\[\nm_{t,i} = \\prod_{\\tau=t-H}^{t-1} (1+r_{\\tau,i}) - 1\n\\]\n(If you use log returns, you can equivalently use a sum of log returns.)\n\n\n5.2 Cross-sectional standardization (z-score)\nMomentum values are not comparable across time unless we standardize.\nWe compute a cross-sectional z-score within the selected universe \\(\\mathcal{U}_t\\):\nIf \\(\\bar{m}_t\\) is the mean of \\(m_{t,i}\\) across \\(i \\in \\mathcal{U}_t\\), and \\(s_t\\) is the cross-sectional standard deviation. Define: \\[\nz_{t,i} = \\frac{m_{t,i} - \\bar{m}_t}{s_t}\n\\]\nThis makes \\(z_{t,i}\\) dimensionless and stable across different regimes.\n\n\n5.3 Mapping the signal to expected returns\nA common simple mapping is: \\[\n\\mu_{t,i}^{(d)} = \\kappa\\, z_{t,i}\n\\]\nHere \\(\\kappa\\) is a scaling constant that controls the magnitude of expected returns.\nTwo main ways to calculate \\(\\kappa\\):\n(A) Target cross-sectional dispersion Choose a target daily standard deviation of expected returns, call it \\(\\sigma_\\mu^{(d)}\\), and set: \\[\n\\kappa = \\frac{\\sigma_\\mu^{(d)}}{\\operatorname{std}(z_t)}\n\\]\n(B) Target annual expected-return range If you want a typical annual spread of, say, \\(\\sigma_\\mu^{(ann)}\\), use: \\[\n\\sigma_\\mu^{(d)} = \\frac{\\sigma_\\mu^{(ann)}}{252}\n\\] then we apply option (A) on \\(\\sigma_\\mu^{(d)}\\) to get to \\(\\kappa\\).\nThis model is deliberately simple: it gives the optimizer a stable ranking signal without overfitting.\n\ndef momentum_score_from_returns(ret_window, mode=\"6-1\"):\n    R = ret_window.replace([np.inf, -np.inf], np.nan).dropna(how=\"any\")\n    T = len(R)\n    if T &lt; 80:\n        return R.mean().values.astype(np.float64)\n    if mode == \"12-1\":\n        lookback, skip = 252, 21\n    elif mode == \"6-1\":\n        lookback, skip = 126, 21\n    elif mode == \"3-0\":\n        lookback, skip = 63, 0\n    else:\n        raise ValueError(\"Unknown momentum mode\")\n    if T &lt; lookback + skip + 5:\n        lookback = min(lookback, max(63, T - skip - 1))\n    R_use = R.iloc[-(lookback + skip):]\n    R_mom = R_use.iloc[:-skip] if skip &gt; 0 else R_use\n    return ((1.0 + R_mom).prod(axis=0) - 1.0).values.astype(np.float64)\n\n\n\ndef winsorize_and_zscore(x, p_lo=0.05, p_hi=0.95):\n    x = np.asarray(x, dtype=np.float64)\n    lo, hi = np.quantile(x, [p_lo, p_hi])\n    x = np.clip(x, lo, hi)\n    x = x - x.mean()\n    return x / (x.std() + 1e-12)\n\n\ndef scale_mu_to_target_sharpe(mu_dir, cov_ann, target_sharpe_ann, mu_cap_ann):\n    mu = np.asarray(mu_dir, dtype=np.float64).flatten()\n    if np.all(np.abs(mu) &lt; 1e-12):\n        return np.zeros_like(mu)\n    A = cov_ann + 1e-8 * np.eye(cov_ann.shape[0])\n    try:\n        x = np.linalg.solve(A, mu)\n    except np.linalg.LinAlgError:\n        x = np.linalg.lstsq(A, mu, rcond=None)[0]\n    q = float(mu @ x)\n    if (not np.isfinite(q)) or q &lt;= 1e-18:\n        return np.zeros_like(mu)\n    s = float(target_sharpe_ann) / np.sqrt(q)\n    return np.clip(s * mu, -mu_cap_ann, mu_cap_ann)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#covariance-estimation-building-sigma_t",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#covariance-estimation-building-sigma_t",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "6) Covariance estimation: building \\(\\Sigma_t\\)",
    "text": "6) Covariance estimation: building \\(\\Sigma_t\\)\nRisk is represented by the covariance matrix \\(\\Sigma_t\\) of returns for the current set of stocks \\(\\mathcal{S}_t\\).\nIf \\(R_t \\in \\mathbb{R}^{T \\times N}\\) is the matrix of past returns (columns are assets), in the estimation window \\([t-T,\\,t)\\).\nWe set \\(\\bar{r}\\) as the sample mean vector in the window.the demeaned matrix will be: \\[\n\\tilde{R}_t = R_t - \\mathbf{1}\\bar{r}^\\top\n\\]\n\\[\n\\tilde{R}_t \\;=\\;\n\\begin{bmatrix}\nr_{t-T,1}-\\bar r_1 & r_{t-T,2}-\\bar r_2 & \\cdots & r_{t-T,N}-\\bar r_N\\\\\nr_{t-T+1,1}-\\bar r_1 & r_{t-T+1,2}-\\bar r_2 & \\cdots & r_{t-T+1,N}-\\bar r_N\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nr_{t-1,1}-\\bar r_1 & r_{t-1,2}-\\bar r_2 & \\cdots & r_{t-1,N}-\\bar r_N\n\\end{bmatrix}\n\\]\n\n6.1 Sample covariance\nThe classic estimator is: \\[\nS_t = \\frac{1}{T-1}\\tilde{R}_t^\\top \\tilde{R}_t\n\\]\nThis is unbiased under ideal assumptions, but \\(S_t\\) can be noisy when \\(T\\) is not much larger than \\(N\\).\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\operatorname{Var}(r_1) & \\operatorname{Cov}(r_1,r_2) & \\cdots & \\operatorname{Cov}(r_1,r_N)\\\\\n\\operatorname{Cov}(r_2,r_1) & \\operatorname{Var}(r_2) & \\cdots & \\operatorname{Cov}(r_2,r_N)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\operatorname{Cov}(r_N,r_1) & \\operatorname{Cov}(r_N,r_2) & \\cdots & \\operatorname{Var}(r_N)\n\\end{bmatrix}\n\\qquad\n\\operatorname{Cov}(r_i,r_j)=s_{ij}=\\frac{1}{T-1}\\sum_{k=1}^{T}\\tilde r_{k,i}\\tilde r_{k,j}\n\\]\n\n\n6.2 Diagonal covariance (no correlations)\nA “failsafe” stable model sets correlations to zero: \\[\n\\Sigma_t = \\operatorname{diag}(S_t)\n\\]\nFrom \\(S_t=[s_{ij}]\\), the diagonal-only covariance is\n\\[\n\\Sigma_{\\text{diag}} =\n\\begin{bmatrix}\n\\operatorname{Var}(r_1) & 0 & \\cdots & 0\\\\\n0 & \\operatorname{Var}(r_2) & \\cdots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & \\cdots & \\operatorname{Var}(r_N)\n\\end{bmatrix}\n\\] This reduces estimation error but the cost is ignoring diversification effects.\n\n\n6.3 Shrinkage estimators (Ledoit–Wolf / OAS intuition)\nShrinkage stabilizes covariance by mixing the noisy sample estimate with a structured target: \\[\n\\Sigma_t = (1-\\delta)S_t + \\delta F_t\n\\]\nTypical target choices are: - scaled identity: \\(F_t = \\bar{\\sigma}^2 I\\) where \\(\\bar{\\sigma}^2\\) is average variance - diagonal target: \\(F_t = \\operatorname{diag}(S_t)\\)\nThe shrinkage intensity \\(\\delta \\in [0,1]\\) is chosen automatically by the estimator (Ledoit–Wolf or OAS).\nInterpretation: when data is noisy, we can increase \\(\\delta\\) to reduce extreme correlations.\n\n\n6.4 EWMA covariance (time-decayed risk)\nEWMA weights recent returns more, capturing volatility clustering.\nwe set \\(\\lambda \\in (0,1)\\) as the decay (example: 0.94).\nDefine demeaned return vector \\(\\tilde{r}_{t-1} = r_{t-1} - \\bar{r}\\) and update: \\[\n\\Sigma_t = \\lambda \\Sigma_{t-1} + (1-\\lambda)\\tilde{r}_{t-1}\\tilde{r}_{t-1}^\\top\n\\]\nEWMA is popular because it reacts faster to regime changes than the sample covariance.\n\newma_lambda = 0.94\njitter, psd_eps = 1e-10, 1e-10\n\ndef make_psd(sigma, eps=1e-10):\n    sigma = 0.5 * (sigma + sigma.T)\n    vals, vecs = np.linalg.eigh(sigma)\n    vals = np.maximum(vals, eps)\n    out = (vecs * vals) @ vecs.T\n    return 0.5 * (out + out.T)\n\n\ndef ewma_covariance(x, lam=0.94):\n    x = x - x.mean(axis=0, keepdims=True)\n    T, N = x.shape\n    S = np.zeros((N, N), dtype=np.float64)\n    a = 1.0 - lam\n    for t in range(T):\n        xt = x[t][:, None]\n        S = lam * S + a * (xt @ xt.T)\n    scale = 1.0 - (lam ** max(T, 1))\n    if scale &gt; 1e-12:\n        S = S / scale\n    return S\n\ndef estimate_covariance(window):\n    x = window.values.astype(np.float64)\n    nn = x.shape[1]\n\n    cov_daily = {\n        \"SampleCov\": np.cov(x, rowvar=False, ddof=1).astype(np.float64),\n        \"LedoitWolf\": LedoitWolf().fit(x).covariance_.astype(np.float64),\n        \"OAS\": OAS().fit(x).covariance_.astype(np.float64),\n        \"EWMA\": ewma_covariance(x, lam=ewma_lambda).astype(np.float64),\n    }\n\n    out = {}\n    for k, c in cov_daily.items():\n        c = 0.5 * (c + c.T)\n        c += jitter * np.eye(nn)\n        out[k] = 252.0 * make_psd(c, psd_eps)\n    return out",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#portfolio-return-and-variance",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#portfolio-return-and-variance",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "7) Portfolio return and variance",
    "text": "7) Portfolio return and variance\nBefore we optimize anything, we need to know what:\n\nexpected return vector \\(\\mu\\)\n\ncovariance matrix \\(\\Sigma\\)\n\nis and we need to understand how they translate into portfolio return and portfolio risk.\n\n\n7.1 Portfolio weights\nWe suppose the portfolio weights of all the stocks we picked are \\[\nw =\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix},\n\\qquad\n\\mathbf{1}^\\top w = 1\n\\]\n\n\\(w_i\\) is the fraction of capital invested in asset \\(i\\)\n\\(\\mathbf{1}^\\top w = 1\\) means all the investment which is 1 because we don’t use leverage or short-selling.\nfor long-only portfolios we also require \\(w \\ge 0\\)\n\n\n\n\n7.2 Portfolio return\nall assets return vector for each period is \\[\nr =\n\\begin{bmatrix}\nr_1\\\\\nr_2\\\\\n\\vdots\\\\\nr_N\n\\end{bmatrix}\n\\]\nThen the portfolio return is the weighted sum of these returns: \\[\nr_p = w^\\top r\n\\]\nor \\[\nr_p =\n\\begin{bmatrix}\nw_1 & w_2 & \\cdots & w_N\n\\end{bmatrix}\n\\begin{bmatrix}\nr_1\\\\\nr_2\\\\\n\\vdots\\\\\nr_N\n\\end{bmatrix}\n= \\sum_{i=1}^{N} w_i r_i\n\\]\n\n\n\n7.3 Expected portfolio return\nThis ia what the optimizer targets We define the expected return vector for each period: \\[\nr_p=\n\\begin{bmatrix}\nr_{p,1}\\\\\nr_{p,2}\\\\\n\\vdots\\\\\nr_{p,T}\n\\end{bmatrix}=\n\\begin{bmatrix}\nr_{1,1} & r_{1,2} & \\cdots & r_{1,N}\\\\\nr_{2,1} & r_{2,2} & \\cdots & r_{2,N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nr_{T,1} & r_{T,2} & \\cdots & r_{T,N}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix}\n\\]\nexpectation of \\(r_p\\) is: \\[\n\\mathbb{E}[r_p] = \\mathbb{E}[w^\\top r] = w^\\top \\mathbb{E}[r] = w^\\top \\mu\n\\]\nExpanded: \\[\n\\mathbb{E}[r_p] = \\sum_{i=1}^{N} w_i \\mu_i\n\\]\nThis is what we optimize when we want to maximize the expected return of our portfolio\n\n\n\n7.4 Portfolio variance (risk)\nRisk in classical mean–variance is measured by variance.\nThe covariance matrix is: \\[\n\\Sigma=\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{N1} & \\sigma_{N2} & \\cdots & \\sigma_{NN}\n\\end{bmatrix},\n\\]\nThe portfolio variance is the quadratic form: \\[\n\\operatorname{Var}(r_p) = \\operatorname{Var}(w^\\top r) = w^\\top \\Sigma w\n\\]\nOr in expanded form:\n\\[\nw^\\top \\Sigma w =\n\\begin{bmatrix}\nw_1 & w_2 & \\cdots & w_N\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2N}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{N1} & \\sigma_{N2} & \\cdots & \\sigma_{NN}\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_N\n\\end{bmatrix}=\n\\sum_{i=1}^{N}\\sum_{j=1}^{N} w_i\\sigma_{ij}w_j\n\\]\n\nThe diagonal terms \\(w_i^2 \\sigma_{ii}\\) are the individual risk contributions (variances).\nThe off-diagonal terms \\(w_i w_j \\sigma_{ij}\\) capture correlations (diversification effects).\n\n\n\n\n7.5 Portfolio volatility\nOften we use volatility as standard deviation instead of variance for analyzing portfolio performance: \\[\n\\sigma_p = \\sqrt{w^\\top \\Sigma w}\n\\]\nVariance is mathematically convenient for optimization; volatility is easier to interpret.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#build-estimation-cache-at-rebalance-dates",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#build-estimation-cache-at-rebalance-dates",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "Build estimation cache at rebalance dates",
    "text": "Build estimation cache at rebalance dates\nFor each rebalance date we store: - active tickers (liquidity-selected) - return window - covariance maps - scaled annual excess returns \\(\\mu\\)\nThis speeds up the backtest.\n\ncache = {}\n\ndef rebalances_per_year(rebal_dates_index):\n    idx = pd.DatetimeIndex(rebal_dates_index)\n    if len(idx) &lt; 2:\n        return 1.0\n    per_year = pd.Series(1, index=idx).resample(\"YE\").sum()\n    return float(per_year.median())\n\n\nfor dt in rebal_dates:\n    pos = returns.index.get_loc(dt)\n    if isinstance(pos, slice):\n        pos = pos.stop - 1\n    if pos &lt; 252:\n        continue\n\n\n    liquid_tickers, avg_dollar_volume = select_liquid_universe(dt,close_prices, volumes, top_n=100, liq_lookback=252,\n    min_listing_days=252, min_obs=252,)\n\n    if len(liquid_tickers) &lt; 2:\n        continue\n\n    close_for_model = close_prices[liquid_tickers].iloc[pos - 252:pos]\n    if close_for_model.shape[0] &lt; 252:\n        continue\n\n   \n    window = close_for_model.pct_change(fill_method=None).iloc[1:]\n\n    window = window.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n    if window.shape[0] &lt; 251 or window.shape[1] &lt; 2:\n        continue\n\n    active_tickers = window.columns.tolist()\n    window = window.astype(np.float32)\n\n    cov_ann_map = estimate_covariance(window)\n    cov_ann = cov_ann_map[\"LedoitWolf\"]\n\n    score = momentum_score_from_returns(window, mode=\"6-1\")\n    z = winsorize_and_zscore(score, 0.05, 0.95)\n    mu_excess_ann = scale_mu_to_target_sharpe(z, cov_ann, 0.80, 0.30)\n\n    cache[dt] = {\n        \"R\": window,\n        \"mu_excess_ann\": mu_excess_ann,\n        \"cov_ann_map\": cov_ann_map,\n        \"tickers\": active_tickers,\n        \"avg_dollar_volume\": avg_dollar_volume.reindex(active_tickers).astype(np.float32),\n    }\n\nrebal_dates = [d for d in rebal_dates if d in cache]\nif len(rebal_dates) == 0:\n    raise ValueError(\"No rebalance dates\")\n\nuniverse_size = pd.Series({dt: len(cache[dt][\"tickers\"]) for dt in rebal_dates}, name=\"UniverseSize\")\nprint(f\"Universe size across rebalances: min={universe_size.min()}, max={universe_size.max()}, mean={universe_size.mean():.1f}\")\n\nrebal_per_year = rebalances_per_year(rebal_dates)\nrf_daily = (1.0 + rf) ** (1.0 / 252.0) - 1.0\nprint(\"Rebalances per year (estimated):\", round(rebal_per_year, 2))\n\nUniverse size across rebalances: min=100, max=100, mean=100.0\nRebalances per year (estimated): 12.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#optimization-problems-the-math-behind-each-model",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#optimization-problems-the-math-behind-each-model",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "8) Optimization problems (the math behind each model)",
    "text": "8) Optimization problems (the math behind each model)\nAt each rebalance date \\(t\\), we solve for target weights \\(w_t\\) using \\(\\mu_t\\) and \\(\\Sigma_t\\) for the filtered set of stocks \\(U_t\\).\nconstraints are: \\[\n\\mathbf{1}^\\top w_t = 1, \\quad w_t \\ge 0,\\quad w_{t,i} \\le w_{\\max}\n\\]\n\n8.1 Minimum Variance (MinVar)\nMinVar ignores expected returns and finds the lowest-risk portfolio: \\[\n\\min_{w} \\; w^\\top \\Sigma_t w\n\\]\nThis is a convex quadratic program (QP) under \\(\\Sigma_t \\succeq 0\\).\n\ncvx_cache = {}\nridge_mv_gamma = 12.0\ncost_bps = 10\nsolver_order = [\"OSQP\", \"CLARABEL\", \"ECOS\", \"SCS\"]\nturnover_penalty_bps = 10.0\nlong_only, w_min, w_max = True, 0.0, 0.25\n\nridge = 1e-4\n\ndef safe_normalize_weights(w, w_min, w_max, long_only):\n    w = np.asarray(w, dtype=np.float64).flatten()\n    if long_only:\n        w = np.maximum(w, 0.0)\n    if w_min is not None:\n        w = np.maximum(w, w_min)\n    if w_max is not None:\n        w = np.minimum(w, w_max)\n    s = w.sum()\n    if (not np.isfinite(s)) or s &lt;= 0:\n        return None\n    w = w / s\n    for _ in range(2):\n        if long_only:\n            w = np.maximum(w, 0.0)\n        if w_min is not None:\n            w = np.maximum(w, w_min)\n        if w_max is not None:\n            w = np.minimum(w, w_max)\n        s = w.sum()\n        if s &lt;= 0:\n            return None\n        w = w / s\n    return w\n\ndef kappa_annual(rebals_per_year_value):\n    k = 0.0\n    k += cost_bps / 10000.0\n    k += turnover_penalty_bps / 10000.0\n    return float(rebals_per_year_value * k)\n\n\ndef solve_cvx(prob, var):\n    for solver in solver_order:\n        try:\n            if solver == \"OSQP\":\n                kwargs = {\"max_iter\": 8000}\n            elif solver in (\"ECOS\", \"SCS\"):\n                kwargs = {\"max_iters\": 10000}\n            else:\n                kwargs = {}\n            prob.solve(solver=solver, warm_start=True, **kwargs)\n\n            if var.value is not None:\n                w = np.asarray(var.value, dtype=np.float64).flatten()\n                if np.all(np.isfinite(w)):\n                    return w\n        except Exception:\n            continue\n    return None\n\ndef blend_weights(w_star, w_prev, eta):\n    eta = float(np.clip(eta, 0.0, 1.0))\n    return (1.0 - eta) * np.asarray(w_star, dtype=np.float64) + eta * np.asarray(w_prev, dtype=np.float64)\n\ndef constraints_feasible(nn, w_min, w_max, long_only):\n    w_min_eff = 0.0 if long_only else (-np.inf if w_min is None else w_min)\n    w_max_eff = np.inf if w_max is None else w_max\n    if np.isfinite(w_max_eff) and w_max_eff * nn &lt; 1.0 - 1e-9:\n        return False\n    if np.isfinite(w_min_eff) and w_min_eff * nn &gt; 1.0 + 1e-9:\n        return False\n    return True\n\n\ndef get_minvar_solver(nn, ridge, kappa):\n    key = (\"minvar\", nn, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Minimize(cp.quad_form(w, cp.psd_wrap(S)) + kappa * 0.5 * cp.norm1(w - w_prev) + 0.5 * ridge * cp.sum_squares(w))\n    problem = cp.Problem(obj, cons)\n    cvx_cache[key] = (problem, w, S, w_prev)\n    return problem, w, S, w_prev\n\ndef minvar_weights(cov_ann, w_prev, rpy):\n    nn = cov_ann.shape[0]\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    prob, w_var, S_p, wprev_p = get_minvar_solver(nn, ridge, kappa_annual(rpy))\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\n\n\n8.2 Mean–Variance Utility (MV)\nIn this model we use both return and risk and try to maximize this object with our weights: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w\n\\]\nwhere: - \\(\\mu_t^\\top w\\) is for maximizing the return - \\(w^\\top \\Sigma_t w\\) is variance. we subtract it so the trade-off between risk and return happen and try to minimize risk for the cost of reducing return - \\(\\gamma &gt; 0\\) controls risk aversion. this parameter represents our importance of risk and how much we want to minimize it in respect to return.\n\n\n8.3 Regularized Mean–Variance (Ridge MV)\nWhen weights become unstable (high turnover, concentration),we can add an \\(L_2\\) penalty: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w - \\frac{\\eta}{2}\\lVert w\\rVert_2^2\n\\]\nThis discourages extreme weight vectors and improves stability. You can interpret ridge as adding a multiple of the identity to risk: \\[\nw^\\top \\Sigma_t w + \\frac{\\eta}{\\gamma}\\lVert w\\rVert_2^2 = w^\\top\\left(\\Sigma_t + \\frac{\\eta}{\\gamma}I\\right)w\n\\]\n\n\n8.4 Turnover-aware optimization (penalize trading)\nIf \\(w_{t^-}\\) is the portfolio just before rebalancing, turnover is: \\[\n\\operatorname{TO}_t = \\sum_{i=1}^{N}\\lvert w_{t,i} - w_{t^-,i}\\rvert\n\\]\nIt shows us how much weights have changed in rebalancing. the more the weights change, the more unstable the weights get and also the transaction cost would be huge.\nA simple way to reduce churn is to penalize turnover in the objective: \\[\n\\max_{w}\\; \\mu_t^\\top w - \\frac{\\gamma}{2} w^\\top \\Sigma_t w - \\kappa \\operatorname{TO}_t\n\\]\nThis is convex and works well with CVXPY.\n\nmv_lambda = 6.0\n\ndef get_mv_solver(nn, mv_lambda, ridge, kappa):\n    key = (\"mv\", nn, mv_lambda, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Maximize(mu @ w - 0.5 * mv_lambda * cp.quad_form(w, cp.psd_wrap(S))\n                      - kappa * 0.5 * cp.norm1(w - w_prev)\n                      - 0.5 * ridge * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev)\n    return prob, w, mu, S, w_prev\n\ndef get_reg_mv_solver(nn, mv_lambda, ridge, kappa, gamma_l2):\n    key = (\"ridge_mv\", nn, mv_lambda, ridge, kappa, gamma_l2, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    obj = cp.Maximize(mu @ w - 0.5 * mv_lambda * cp.quad_form(w, cp.psd_wrap(S)) - kappa * 0.5 * cp.norm1(w - w_prev) - 0.5 * (ridge + gamma_l2) * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev)\n    return prob, w, mu, S, w_prev\n\n\n\ndef mv_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    prob, w_var, mu_p, S_p, wprev_p = get_mv_solver(nn, mv_lambda, ridge, kappa_annual(rpy))\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\ndef ridge_mv_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    g2 = ridge_mv_gamma / max(nn, 1)\n    prob, w_var, mu_p, S_p, wprev_p = get_reg_mv_solver(nn, mv_lambda, ridge, kappa_annual(rpy), g2)\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n    w = solve_cvx(prob, w_var)\n    return None if w is None else safe_normalize_weights(w, w_min, w_max, long_only)\n\n\n\n8.5 Max-Sharpe portfolios\nThe Sharpe ratio of a portfolio is the excess return on risk ratio of portfolio (comparing to risk free rate) and it’s a main measure for comparing investments: \\[\n\\operatorname{SR}(w) = \\frac{\\mu_t^\\top w - r_f^{(d)}}{\\sqrt{w^\\top \\Sigma_t w}}\n\\]\nDirectly maximizing this fraction is not a simple QP and can be unstable. also we use SLSQP from scipy instead of considering convex optimization\n\n\n8.6 Max Sharpe with Frontier grid\nWe pick a grid of target returns \\(m\\) values and get min variance weights for each \\(m\\) and solve:\n\\[\n\\min_{w}\\; w^\\top \\Sigma_t w\n\\]\nsubject to: \\[\n\\mu_t^\\top w \\ge m,\\quad \\mathbf{1}^\\top w = 1,\\quad w \\ge 0,\\quad w \\le w_{\\max}\n\\]\nFor each solution \\(w(m)\\) we compute its Sharpe: \\[\n\\operatorname{SR}(w(m)) = \\frac{\\mu_t^\\top w(m) - r_f^{(d)}}{\\sqrt{w(m)^\\top \\Sigma_t w(m)}}\n\\]\nAnd choose the best: \\[\nw^\\star = \\arg\\max_{m} \\operatorname{SR}(w(m))\n\\]\nThis is slow but stable and avoids fragile non-convex solvers.\n\ndef max_sharpe_weights(mu_excess_ann, cov_ann, w_prev, rpy):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n    bounds = [(0.0 if long_only else (-1.0 if w_min is None else w_min), 1.0 if w_max is None else w_max) for _ in range(nn)]\n    x0 = np.ones(nn, dtype=np.float64) / nn\n    kappa = kappa_annual(rpy)\n    mu_use = np.asarray(mu_excess_ann, dtype=np.float64)\n\n    def neg_obj(w):\n        w = np.asarray(w, dtype=np.float64)\n        if np.any(~np.isfinite(w)):\n            return 1e6\n        ret = float(mu_use @ w)\n        vol = float(np.sqrt(w @ cov_ann @ w))\n        if vol &lt; 1e-12:\n            return 1e6\n        return -(ret / vol) + kappa * 0.5 * np.sum(np.abs(w - w_prev)) + 0.5 * ridge * np.sum(w**2)\n\n    result = minimize(neg_obj, x0, method=\"SLSQP\", bounds=bounds,\n                   constraints=({\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},),\n                   options={\"maxiter\": 8000})\n    if (not result.success) or np.any(~np.isfinite(result.x)):\n        return None\n    return safe_normalize_weights(result.x, w_min, w_max, long_only)\n\n\ndef sharpe_from_w(mu_excess_ann, cov_ann, w):\n    w = np.asarray(w, dtype=np.float64).flatten()\n    r = float(np.dot(mu_excess_ann, w))\n    v = float(np.sqrt(max(w @ cov_ann @ w, 1e-18)))\n    return r / v if v &gt; 1e-12 else -np.inf\n\n\n\ndef get_frontier_solver(nn, ridge, kappa):\n    key = (\"frontier\", nn, ridge, kappa, w_min, w_max, long_only)\n    if key in cvx_cache:\n        return cvx_cache[key]\n\n    w = cp.Variable(nn)\n    mu = cp.Parameter(nn)\n    S = cp.Parameter((nn, nn), symmetric=True)\n    w_prev = cp.Parameter(nn)\n    r_target = cp.Parameter(nonneg=False)\n\n    cons = []\n    if long_only:\n        cons.append(w &gt;= 0)\n    if w_min is not None:\n        cons.append(w &gt;= w_min)\n    if w_max is not None:\n        cons.append(w &lt;= w_max)\n    cons.append(cp.sum(w) == 1)\n    cons.append(mu @ w &gt;= r_target)\n\n    obj = cp.Minimize(cp.quad_form(w, cp.psd_wrap(S)) + kappa * cp.norm1(w - w_prev) + 0.5 * ridge * cp.sum_squares(w))\n    prob = cp.Problem(obj, cons)\n    cvx_cache[key] = (prob, w, mu, S, w_prev, r_target)\n    return prob, w, mu, S, w_prev, r_target\n\n\ndef greedy_max_return_weight(mu, w_max):\n    mu = np.asarray(mu, dtype=np.float64).flatten()\n    n = len(mu)\n    order = np.argsort(mu)[::-1]\n    w = np.zeros(n, dtype=np.float64)\n    cap = np.inf if w_max is None else float(w_max)\n    remaining = 1.0\n    for i in order:\n        if remaining &lt;= 1e-12:\n            break\n        add = min(cap, remaining)\n        w[i] = add\n        remaining -= add\n    if remaining &gt; 1e-6:\n        return None\n    return w\n\n\ndef max_sharpe_frontier_grid_weights(mu_excess_ann, cov_ann, w_prev, rpy, grid_n=20):\n    nn = len(mu_excess_ann)\n    if not constraints_feasible(nn, w_min, w_max, long_only):\n        return None\n\n    w_minv = minvar_weights(cov_ann, w_prev, rpy)\n    if w_minv is None:\n        w_minv = np.ones(nn, dtype=np.float64) / nn\n\n    w_maxr = greedy_max_return_weight(mu_excess_ann, w_max)\n    if w_maxr is None:\n        return None\n\n    r_lo = float(np.dot(mu_excess_ann, w_minv))\n    r_hi = float(np.dot(mu_excess_ann, w_maxr))\n    if not np.isfinite(r_lo) or not np.isfinite(r_hi) or r_hi &lt;= r_lo + 1e-12:\n        return None\n\n    targets = np.linspace(r_lo, r_hi, int(grid_n))\n\n    prob, w_var, mu_p, S_p, wprev_p, r_p = get_frontier_solver(nn, ridge, kappa_annual(rpy))\n    mu_p.value = np.asarray(mu_excess_ann, dtype=np.float64)\n    S_p.value = np.asarray(cov_ann, dtype=np.float64)\n    wprev_p.value = np.asarray(w_prev, dtype=np.float64)\n\n    best_w, best_s = None, -np.inf\n    for rt in targets:\n        r_p.value = float(rt)\n        w = solve_cvx(prob, w_var)\n        if w is None:\n            continue\n        w = safe_normalize_weights(w, w_min, w_max, long_only)\n        if w is None:\n            continue\n        s = sharpe_from_w(mu_excess_ann, cov_ann, w)\n        if s &gt; best_s:\n            best_s, best_w = s, w\n\n    return best_w",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#trading-transaction-costs-and-real-market-simulation",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#trading-transaction-costs-and-real-market-simulation",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "9) Trading, transaction costs, and real market simulation",
    "text": "9) Trading, transaction costs, and real market simulation\nWe talked about turnover and how to add it to our optimization problem as a penalty. At a rebalance date, trading changes weights from \\(w_{t^-}\\) to \\(w_t\\). Turnover: \\[\n\\operatorname{TO}_t = \\sum_{i=1}^{N}\\lvert w_{t,i} - w_{t^-,i}\\rvert\n\\]\nIf costs are \\(c\\) in decimal per unit turnover (for example 1 bps means \\(c=0.001\\)), then cost paid is: \\[\nC_t = c\\,\\operatorname{TO}_t\\,W_{t-1}\n\\]\nwhich \\(W_{t-1}\\) is the wealth in the last date\nNet wealth right after rebalancing becomes: \\[\nW_{t-1}^{(net)} = W_{t-1} - C_t\n\\]\nThen apply the day-\\(t\\) return: \\[\nW_t = W_{t-1}^{(net)}(1 + r_{p,t})\n\\]\nThis model is simple but captures the key reality: higher turnover causes more costs and reduces long-run performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#backtest-engine",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#backtest-engine",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "10) Backtest engine",
    "text": "10) Backtest engine\nDaily drift: \\[\n\\tilde{w}_{t,i} = \\frac{w_{t-1,i}(1+r_{t,i})}{\\sum_j w_{t-1,j}(1+r_{t,j})}\n\\]\nIn each rebalance we: - compute w_pre (drifted weights in active set of selected stocks) - compute w_tar from strategy - blend, normalize, apply costs - analyze performance",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#performance-metrics-what-we-report",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#performance-metrics-what-we-report",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "11) Performance metrics (what we report)",
    "text": "11) Performance metrics (what we report)\nAssume we have daily portfolio returns \\((r_{p,t})_{t=1}^T\\) and wealth series \\(\\{W_t\\}\\).\n\n9.1 CAGR\nif \\(T\\) is the number of trading days. The compounded annual growth rate is:\n\\(\\operatorname{CAGR} = \\left(\\frac{W_T}{W_0}\\right)^{252/T} - 1\\)\n\n\n9.2 Annualized volatility\nif \\(\\sigma_d = \\operatorname{std}(r_{p,t})\\). Then:\n\\(\\sigma_{ann} = \\sqrt{252}\\,\\sigma_d\\)\n\n\n9.3 Sharpe ratio\nWe’ve already talked about this. \\(\\bar{r}_p\\) is the mean daily portfolio return. Using daily risk-free \\(r_f^{(d)}\\):\n\\(\\operatorname{SR} = \\frac{\\bar{r}_p - r_f^{(d)}}{\\sigma_d}\\sqrt{252}\\)\n\n\n9.4 Drawdown and max drawdown\nDefine running peak (the highest point in our wealth):\n\\(M_t = \\max_{s \\le t} W_s\\)\nDrawdown (how much we go down after we hit the peak):\n\\(DD_t = 1 - \\frac{W_t}{M_t}\\)\nMax drawdown:\n\\(\\operatorname{MaxDD} = \\max_t DD_t\\)\n\n\n9.5 Turnover diagnostics\nAverage turnover per rebalance:\n\\(\\overline{\\operatorname{TO}} = \\frac{1}{|\\mathcal{T}|}\\sum_{t \\in \\mathcal{T}}\\operatorname{TO}_t\\)\nApproximate annual turnover if rebalances occur \\(B\\) times per year:\n\\(\\operatorname{TO}_{ann} \\approx B\\,\\overline{\\operatorname{TO}}\\)\n\nfixed_fee = 0.0\n\n\ndef calc_drawdown(series):\n    return series / series.cummax() - 1.0\n\ndef backtest_strategy(name, cov_key):\n    all_dates = returns.loc[rebal_dates[0]:].index\n    rebal_set = set(rebal_dates)\n\n    w = pd.Series(dtype=np.float64)\n    gross_value, net_value = 1.0, 1.0\n\n    gross_values, net_values, gross_returns = [], [], []\n    weights_rebal = {}\n    turnover_list, cost_list = [], []\n    fallback_count = 0\n\n    for dt in all_dates:\n        if dt in rebal_set:\n            st = cache[dt]\n            mu_excess_ann = st[\"mu_excess_ann\"]\n            cov_ann = st[\"cov_ann_map\"][cov_key]\n            active_tickers = st[\"tickers\"]\n            nn = len(active_tickers)\n            if nn &gt;= 2:\n\n                w_pre = w.reindex(active_tickers).fillna(0.0).astype(np.float64)\n                s = float(w_pre.sum())\n                w_pre = (w_pre / s) if s &gt; 0 else pd.Series(np.ones(nn, dtype=np.float64) / nn, index=active_tickers)\n\n                if name == \"EW\":\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                elif name.startswith(\"MinVar\"):\n                    w_tar = minvar_weights(cov_ann, w_pre.values, rebal_per_year)\n                elif name.startswith(\"MV\"):\n                    w_tar = mv_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"Ridge MV\":\n                    w_tar = ridge_mv_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"MaxSharpe\":\n                    w_tar = max_sharpe_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                elif name == \"MaxSharpe (FrontierGrid)\":\n                    w_tar = max_sharpe_frontier_grid_weights(mu_excess_ann, cov_ann, w_pre.values, rebal_per_year)\n                else:\n                    w_tar = None\n\n                if w_tar is None or np.any(~np.isfinite(w_tar)):\n                    print_warn(f\"{name} failed on {dt.date()}, fallback to EW\")\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                    fallback_count += 1\n\n                w_tar = blend_weights(w_tar, w_pre.values, float(blend.get(name, 0.0)))\n                w_tar = safe_normalize_weights(w_tar, w_min, w_max, long_only)\n                if w_tar is None:\n                    print_warn(f\"{name} infeasible on {dt.date()}, fallback to EW\")\n                    w_tar = np.ones(nn, dtype=np.float64) / nn\n                    fallback_count += 1\n\n                delta = w_tar - w_pre.values\n                turnover = 0.5 * np.sum(np.abs(delta))\n                cost_value = 0.0\n                cost_rate = float((cost_bps / 10000.0) * 0.5 * np.sum(np.abs(delta)))\n                cost_value = net_value * cost_rate\n                net_value = max(net_value - cost_value, 1e-12)\n                if fixed_fee &gt; 0:\n                    fee = fixed_fee * np.count_nonzero(np.abs(delta) &gt; 1e-12)\n                    net_value = max(net_value - fee, 1e-12)\n                    cost_value += fee\n\n                turnover_list.append(turnover)\n                cost_list.append(cost_value)\n\n                weights_rebal[dt] = pd.Series(w_tar.astype(np.float32), index=active_tickers)\n                w = pd.Series(w_tar, index=active_tickers, dtype=np.float64)\n\n        if w.empty:\n            port_ret = 0.0\n            w_close = pd.Series(dtype=np.float64)\n        else:\n            r_today = returns.loc[dt].reindex(w.index).fillna(0.0).astype(np.float64)\n            port_ret = float(np.dot(w.values, r_today.values))\n\n            gross_value *= (1.0 + port_ret)\n            net_value *= (1.0 + port_ret)\n\n            grossed = w.values * (1.0 + r_today.values)\n            gs = float(grossed.sum())\n            w_close = pd.Series(grossed / gs, index=w.index, dtype=np.float64) if gs &gt; 0 and np.isfinite(gs) else pd.Series(dtype=np.float64)\n\n        gross_values.append(gross_value)\n        net_values.append(net_value)\n        gross_returns.append(port_ret)\n        w = w_close\n\n    gross_values = pd.Series(gross_values, index=all_dates, name=f\"{name}_gross\")\n    net_values = pd.Series(net_values, index=all_dates, name=f\"{name}_net\")\n    gross_returns = pd.Series(gross_returns, index=all_dates, name=f\"{name}_gross_ret\")\n    net_returns = net_values.pct_change().fillna(0.0)\n\n    wdf = pd.DataFrame.from_dict(weights_rebal, orient=\"index\")\n    if not wdf.empty:\n        wdf = wdf.fillna(0.0)\n\n    return {\n        \"gross_values\": gross_values,\n        \"net_values\": net_values,\n        \"gross_returns\": gross_returns,\n        \"net_returns\": net_returns,\n        \"weights\": wdf,\n        \"turnover\": pd.Series(turnover_list, index=wdf.index) if len(wdf) else pd.Series([], dtype=float),\n        \"costs\": pd.Series(cost_list, index=wdf.index) if len(wdf) else pd.Series([], dtype=float),\n        \"fallbacks\": fallback_count,\n    }",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#strategy-dashboards",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#strategy-dashboards",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "12) Strategy dashboards",
    "text": "12) Strategy dashboards\nWe report the performance of each model and top weights and top risk contribution (to volatility) at date \\(t\\): if portfolio variance \\(\\sigma_p^2 = w^T\\Sigma w\\), and marginal risk is \\(m = \\Sigma w\\). Contribution to variance: \\(RC^{var}_i = w_i m_i\\). Contribution to volatility: \\(RC^{vol}_i = RC^{var}_i / \\sigma_p\\).\n\ndef format_date_axis(ax):\n    ax.xaxis.set_major_formatter(DateFormatter(\"%Y-%m\"))\n    ax.figure.autofmt_xdate()\n\ndef plot_strategy_dashboard(name, res, cov_key):\n    if res[\"net_values\"].empty:\n        print_warn(f\"{name}: empty results\")\n        return\n\n    fig, axes = plt.subplots(2, 2, figsize=(9, 6))\n\n    ax = axes[0, 0]\n    ax.plot(res[\"net_values\"].index, res[\"net_values\"].values, color=strategy_colors[name])\n    ax.set_title(f\"{name} — Net Equity\")\n    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Growth of $1\")\n    format_date_axis(ax)\n\n    ax = axes[0, 1]\n    dd = calc_drawdown(res[\"net_values\"])\n    ax.plot(dd.index, dd.values, color=strategy_colors[name])\n    ax.set_title(f\"{name} — Net Drawdown\")\n    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Drawdown\")\n    format_date_axis(ax)\n\n    wdf = res[\"weights\"]\n    if wdf.empty:\n        axes[1, 0].set_axis_off()\n        axes[1, 1].set_axis_off()\n        plt.tight_layout()\n        plt.show()\n        return\n\n    last_dt = wdf.index[-1]\n    w_last = wdf.loc[last_dt].astype(float)\n    w_last = w_last[w_last &gt; 0].sort_values(ascending=False)\n\n    ax = axes[1, 0]\n    topw = w_last.head(10).sort_values()\n    ax.barh(topw.index, topw.values, color=strategy_colors[name])\n    ax.set_title(f\"{name} — Top-10 Weights ({last_dt.date()})\")\n    ax.set_xlabel(\"Weight\")\n\n    ax = axes[1, 1]\n    cov = cache[last_dt][\"cov_ann_map\"][cov_key]\n    tickers = cache[last_dt][\"tickers\"]\n    w_vec = wdf.loc[last_dt].reindex(tickers).fillna(0.0).values.astype(np.float64)\n    Sigma_w = cov @ w_vec\n    port_var = float(w_vec @ Sigma_w)\n    port_vol = np.sqrt(max(port_var, 1e-18))\n    rc_var = w_vec * Sigma_w\n    rc_vol = rc_var / port_vol\n\n    rc = pd.Series(rc_vol, index=tickers)\n    top_rc = rc.abs().sort_values(ascending=False).head(10).index\n    plot_rc = rc.loc[top_rc].sort_values()\n\n    ax.barh(plot_rc.index, plot_rc.values, color=strategy_colors[name])\n    ax.set_title(f\"{name} — Top-10 Risk Contributions ({last_dt.date()})\")\n    ax.set_xlabel(\"Contribution to vol\")\n\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#running-strategies",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#running-strategies",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "13) Running strategies",
    "text": "13) Running strategies\n\nresults = {}\n\nname = \"EW\"\nresults[name] = backtest_strategy(name, strategy_cov_key[name])\nplot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\nfor name in [\"MinVar (SampleCov)\", \"MinVar (LedoitWolf)\", \"MinVar (OAS)\", \"MinVar (EWMA)\"]:\n    results[name] = backtest_strategy(name, strategy_cov_key[name])\n    plot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\nfor name in [\"MV (SampleCov)\", \"MV (LedoitWolf)\", \"MV (OAS)\", \"MV (EWMA)\"]:\n    results[name] = backtest_strategy(name, strategy_cov_key[name])\n    plot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\nname = \"Ridge MV\"\nresults[name] = backtest_strategy(name, strategy_cov_key[name])\nplot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\nname = \"MaxSharpe\"\nresults[name] = backtest_strategy(name, strategy_cov_key[name])\nplot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\n\nname = \"MaxSharpe (FrontierGrid)\"\nresults[name] = backtest_strategy(name, strategy_cov_key[name])\nplot_strategy_dashboard(name, results[name], strategy_cov_key[name])\n\n\nstrategy_names = list(results.keys())\nprint(\"Computed strategies:\", strategy_names)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputed strategies: ['EW', 'MinVar (SampleCov)', 'MinVar (LedoitWolf)', 'MinVar (OAS)', 'MinVar (EWMA)', 'MV (SampleCov)', 'MV (LedoitWolf)', 'MV (OAS)', 'MV (EWMA)', 'Ridge MV', 'MaxSharpe', 'MaxSharpe (FrontierGrid)']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#summary-tables-and-overall-plots-style-preserved",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#summary-tables-and-overall-plots-style-preserved",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "14) summary tables and overall plots (style preserved)",
    "text": "14) summary tables and overall plots (style preserved)\n\nTrading diagnostics (turnover, costs, concentration)\nWe compute: - average turnover per rebalance - total turnover - total costs - average HHI and effective number of holdings\n\\[\nHHI_t = \\sum_i w_{t,i}^2\n\\quad,\\quad\nN_{eff} = \\frac{1}{E[HHI]}\n\\]\n\ndef calc_drawdown(series):\n    return series / series.cummax() - 1.0\n\ndef performance_metrics(net_returns, net_values):\n    years = len(net_returns) / 252.0\n    cagr = (net_values.iloc[-1] ** (1.0 / years) - 1.0) if years &gt; 0 else 0.0\n    vol = net_returns.std() * np.sqrt(252.0)\n    excess = net_returns - rf_daily\n    sharpe = (excess.mean() / net_returns.std()) * np.sqrt(252.0) if net_returns.std() &gt; 0 else np.nan\n    dd = calc_drawdown(net_values)\n    max_dd = dd.min()\n    calmar = cagr / abs(max_dd) if max_dd &lt; 0 else np.nan\n    downside = net_returns[net_returns &lt; 0]\n    sortino = (excess.mean() / downside.std()) * np.sqrt(252.0) if downside.std() &gt; 0 else np.nan\n    return cagr, vol, sharpe, max_dd, calmar, sortino\n\nmetrics_rows = []\nfor name, res in results.items():\n    metrics_rows.append([name, *performance_metrics(res[\"net_returns\"], res[\"net_values\"])])\n\nmetrics_df = pd.DataFrame(metrics_rows, columns=[\"Strategy\", \"CAGR\", \"AnnVol\", \"Sharpe\", \"MaxDD\", \"Calmar\", \"Sortino\"]).set_index(\"Strategy\")\nprint(\"Risk/Return Summary (Net)\")\ndisplay(metrics_df)\n\ntrade_rows = []\nfor name, res in results.items():\n    turnover, costs, wdf = res[\"turnover\"], res[\"costs\"], res[\"weights\"]\n    if len(wdf) &gt; 0:\n        hhi = (wdf ** 2).sum(axis=1)\n        avg_hhi = float(hhi.mean())\n        eff_n = 1.0 / avg_hhi if avg_hhi &gt; 0 else np.nan\n    else:\n        avg_hhi, eff_n = np.nan, np.nan\n    trade_rows.append([\n        name,\n        float(turnover.mean()) if len(turnover) else 0.0,\n        float(turnover.sum()) if len(turnover) else 0.0,\n        float(costs.sum()) if len(costs) else 0.0,\n        float(costs.sum() / res[\"net_values\"].iloc[-1]) if len(costs) else 0.0,\n        avg_hhi,\n        eff_n,\n    ])\n\ntrade_df = pd.DataFrame(trade_rows, columns=[\"Strategy\", \"Avg Turnover\", \"Total Turnover\", \"Total Costs\", \"Cost % Final Value\", \"Avg HHI\", \"Effective N\"]).set_index(\"Strategy\")\nprint(\"Trading & Stability Summary\")\ndisplay(trade_df)\n\nprint(\"Fallback counts per strategy:\")\nfor name in strategy_names:\n    print(f\"{name}: {results[name]['fallbacks']}\")\n\ndef plot_equity_curves(results_dict, key, title):\n    plt.figure(figsize=(12, 6))\n    for name, res in results_dict.items():\n        s = res[key]\n        plt.plot(s.index, s.values, label=name, color=strategy_colors[name])\n    plt.title(title)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Growth of $1\")\n    plt.grid(True, alpha=0.3)\n    plt.legend()\n    format_date_axis(plt.gca())\n    plt.tight_layout()\n    plt.show()\n\nplot_equity_curves(results, \"gross_values\", \"Equity Curves (Gross)\")\nplot_equity_curves(results, \"net_values\", \"Equity Curves (Net)\")\n\nplt.figure(figsize=(12, 6))\nfor name, res in results.items():\n    dd = calc_drawdown(res[\"net_values\"])\n    plt.plot(dd.index, dd.values, label=name, color=strategy_colors[name])\nplt.title(\"Drawdowns (Net)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Drawdown\")\nplt.grid(True, alpha=0.3)\nplt.legend()\nformat_date_axis(plt.gca())\nplt.tight_layout()\nplt.show()\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(metrics_df[\"AnnVol\"], metrics_df[\"CAGR\"])\nfor s in metrics_df.index:\n    plt.annotate(s, (metrics_df.loc[s, \"AnnVol\"], metrics_df.loc[s, \"CAGR\"]), fontsize=9, alpha=0.9)\nplt.title(\"Realized Risk-Return (Net): CAGR vs Annualized Volatility\")\nplt.xlabel(\"Annualized Volatility\")\nplt.ylabel(\"CAGR\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 6))\nmetrics_df[\"Sharpe\"].sort_values().plot(kind=\"barh\")\nplt.title(\"Realized Sharpe (Net)\")\nplt.xlabel(\"Sharpe\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nret_mat = pd.concat({k: v[\"net_returns\"] for k, v in results.items()}, axis=1).dropna(how=\"any\")\ncorr = ret_mat.corr()\n\nplt.figure(figsize=(10, 8))\nim = plt.imshow(corr.values, aspect=\"auto\")\nplt.colorbar(im, fraction=0.046, pad=0.04)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation=90, fontsize=8)\nplt.yticks(range(len(corr.index)), corr.index, fontsize=8)\nplt.title(\"Correlation of Strategy Net Returns\")\nplt.tight_layout()\nplt.show()\n\nRisk/Return Summary (Net)\n\n\n\n\n\n\n\n\n\nCAGR\nAnnVol\nSharpe\nMaxDD\nCalmar\nSortino\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\nEW\n0.146453\n0.253005\n0.511901\n-0.460750\n0.317857\n0.672864\n\n\nMinVar (SampleCov)\n0.153991\n0.160920\n0.725942\n-0.300514\n0.512424\n0.889201\n\n\nMinVar (LedoitWolf)\n0.130693\n0.161293\n0.598409\n-0.297730\n0.438966\n0.730984\n\n\nMinVar (OAS)\n0.132298\n0.161460\n0.606626\n-0.297890\n0.444118\n0.742014\n\n\nMinVar (EWMA)\n0.166994\n0.154277\n0.823995\n-0.288007\n0.579826\n1.034746\n\n\nMV (SampleCov)\n0.170766\n0.170585\n0.779640\n-0.261325\n0.653462\n1.008728\n\n\nMV (LedoitWolf)\n0.160197\n0.172680\n0.720231\n-0.260224\n0.615611\n0.923250\n\n\nMV (OAS)\n0.168002\n0.171487\n0.762955\n-0.261363\n0.642792\n0.979051\n\n\nMV (EWMA)\n0.185129\n0.172926\n0.844145\n-0.253481\n0.730344\n1.071225\n\n\nRidge MV\n0.157800\n0.171445\n0.712393\n-0.256076\n0.616224\n0.908904\n\n\nMaxSharpe\n0.213863\n0.290424\n0.681396\n-0.458022\n0.466928\n0.877095\n\n\nMaxSharpe (FrontierGrid)\n0.271910\n0.315597\n0.799863\n-0.463704\n0.586387\n1.044933\n\n\n\n\n\n\n\nTrading & Stability Summary\n\n\n\n\n\n\n\n\n\nAvg Turnover\nTotal Turnover\nTotal Costs\nCost % Final Value\nAvg HHI\nEffective N\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\nEW\n0.048705\n5.308806\n0.010946\n0.003211\n0.010000\n100.000004\n\n\nMinVar (SampleCov)\n0.024229\n2.640967\n0.004906\n0.001357\n0.098816\n10.119779\n\n\nMinVar (LedoitWolf)\n0.030512\n3.325809\n0.006660\n0.002212\n0.060997\n16.394131\n\n\nMinVar (OAS)\n0.030349\n3.308053\n0.006694\n0.002196\n0.075948\n13.166957\n\n\nMinVar (EWMA)\n0.080659\n8.791883\n0.019071\n0.004771\n0.103168\n9.692920\n\n\nMV (SampleCov)\n0.130850\n14.262650\n0.034118\n0.008292\n0.111507\n8.968033\n\n\nMV (LedoitWolf)\n0.133851\n14.589796\n0.032932\n0.008682\n0.081683\n12.242522\n\n\nMV (OAS)\n0.131115\n14.291580\n0.033921\n0.008421\n0.090959\n10.993921\n\n\nMV (EWMA)\n0.295051\n32.160563\n0.084430\n0.018393\n0.114862\n8.706068\n\n\nRidge MV\n0.125177\n13.644327\n0.030492\n0.008189\n0.064969\n15.392010\n\n\nMaxSharpe\n0.391773\n42.703308\n0.119849\n0.021059\n0.137056\n7.296314\n\n\nMaxSharpe (FrontierGrid)\n0.336119\n36.637021\n0.129177\n0.014927\n0.151004\n6.622350\n\n\n\n\n\n\n\nFallback counts per strategy:\nEW: 0\nMinVar (SampleCov): 0\nMinVar (LedoitWolf): 0\nMinVar (OAS): 0\nMinVar (EWMA): 0\nMV (SampleCov): 0\nMV (LedoitWolf): 0\nMV (OAS): 0\nMV (EWMA): 0\nRidge MV: 0\nMaxSharpe: 0\nMaxSharpe (FrontierGrid): 0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  },
  {
    "objectID": "notebooks/02_portfolio_optimization_MV_models.html#implementation-on-hong-kong-stock-market-with-quantfinlab",
    "href": "notebooks/02_portfolio_optimization_MV_models.html#implementation-on-hong-kong-stock-market-with-quantfinlab",
    "title": "2. Portfolio Optimization with Mean–Variance Models",
    "section": "implementation on Hong Kong stock market with quantfinlab",
    "text": "implementation on Hong Kong stock market with quantfinlab\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport quantfinlab.portfolio as pf\nimport quantfinlab.plots as pl\nfrom quantfinlab import PortfolioState\n\n\nrf_annual = 0.04\nrf_daily = (1.0 + rf_annual) ** (1.0 / 252.0) - 1.0\n\n\n\nraw = pd.read_csv(\"../data/hkex_selected_close_volume.csv\", header=[0, 1], low_memory=False)\nraw.columns = pd.MultiIndex.from_tuples(\n    [(str(a).strip(), str(b).strip()) for a, b in raw.columns]\n)\n\ndate_cols = [c for c in raw.columns if c[0].lower() == \"date\"]\nclose_cols = [c for c in raw.columns if c[1].lower() == \"close\"]\nvolume_cols = [c for c in raw.columns if c[1].lower() == \"volume\"]\n\nif len(date_cols) == 0:\n    raise ValueError(\"HKEX CSV is missing a Date column.\")\nif len(close_cols) == 0 or len(volume_cols) == 0:\n    raise ValueError(\"HKEX CSV must contain both Close and Volume fields.\")\n\ndates = pd.to_datetime(raw[date_cols[0]], errors=\"coerce\")\n\nclose_prices = raw.loc[:, close_cols].copy()\nvolumes = raw.loc[:, volume_cols].copy()\nclose_prices.columns = [str(c[0]).strip() for c in close_cols]\nvolumes.columns = [str(c[0]).strip() for c in volume_cols]\n\nif close_prices.columns.duplicated().any():\n    close_prices = close_prices.T.groupby(level=0).last().T\nif volumes.columns.duplicated().any():\n    volumes = volumes.T.groupby(level=0).last().T\n\nclose_prices.index = dates\nvolumes.index = dates\n\nclose_prices = (\n    close_prices.apply(pd.to_numeric, errors=\"coerce\")\n    .replace([np.inf, -np.inf], np.nan)\n    .astype(np.float32)\n)\nvolumes = (\n    volumes.apply(pd.to_numeric, errors=\"coerce\")\n    .replace([np.inf, -np.inf], np.nan)\n    .astype(np.float32)\n)\n\nclose_prices = close_prices.where(close_prices &gt; 0)\nvolumes = volumes.where(volumes &gt;= 0)\n\nclose_prices = close_prices[~close_prices.index.isna()].sort_index()\nvolumes = volumes[~volumes.index.isna()].sort_index()\n\nif close_prices.index.has_duplicates:\n    close_prices = close_prices[~close_prices.index.duplicated(keep=\"last\")]\nif volumes.index.has_duplicates:\n    volumes = volumes[~volumes.index.duplicated(keep=\"last\")]\n\nstart = pd.Timestamp(\"2016-01-01\")\nclose_prices = close_prices.loc[close_prices.index &gt;= start]\nvolumes = volumes.loc[volumes.index &gt;= start]\n\nidx = close_prices.index.intersection(volumes.index)\ncols = close_prices.columns.intersection(volumes.columns)\nclose_prices = close_prices.loc[idx, cols]\nvolumes = volumes.loc[idx, cols]\n\nvalid_cols = close_prices.notna().any(axis=0) & volumes.notna().any(axis=0)\nclose_prices = close_prices.loc[:, valid_cols]\nvolumes = volumes.loc[:, valid_cols]\n\nprices = close_prices.copy()\n\n\nprint(\"close_prices:\", close_prices.shape, \"| volumes:\", volumes.shape)\nprint(\"Date range:\", close_prices.index.min().date(), \"to\", close_prices.index.max().date())\n\n\nreturns = pf.prices_to_returns(prices)\nrebal_dates = pf.make_rebalance_dates(returns.index, freq=\"M\", min_history_days=252)\n\n\ncache: dict[pd.Timestamp, PortfolioState] = {}\n\n\ndef build_state(dt: pd.Timestamp) -&gt; PortfolioState | None:\n    tickers, avg_dv = pf.select_liquid_universe(\n        dt,\n        close_prices=prices,\n        volumes=volumes,\n        top_n=100,\n        liq_lookback=200,\n        min_listing_days=200,\n        min_obs=200,\n    )\n    if len(tickers) &lt; 2:\n        return None\n\n    window = returns.loc[:dt, tickers].tail(252).dropna(axis=1, how=\"any\")\n    if window.shape[0] &lt; 200 or window.shape[1] &lt; 2:\n        return None\n\n    mu_excess_ann = pf.mu_momentum(window, mode=\"6-1\", rf=rf_annual, \n                                   target_sharpe=0.80, mu_cap=0.30, winsor=0.05, \n                                   zscore=True,return_series=True,)\n\n    cov_ann_map = {\n        \"sample\": pf.cov_estimate(window, method=\"samplecov\", psd=True, ridge=1e-10),\n        \"lw\": pf.cov_estimate(window, method=\"ledoitwolf\", psd=True, ridge=1e-10),\n        \"oas\": pf.cov_estimate(window, method=\"oas\", psd=True, ridge=1e-10),\n        \"ewma\": pf.cov_estimate(window, method=\"ewma\", ewma_lambda=0.94, psd=True, ridge=1e-10),\n    }\n\n    return PortfolioState(\n        tickers=list(window.columns),\n        mu_excess_ann=mu_excess_ann.reindex(window.columns).astype(float),\n        cov_ann_map=cov_ann_map,\n        avg_dollar_volume=avg_dv.reindex(window.columns).astype(float),\n    )\n\n\nfor dt in rebal_dates:\n    st = build_state(pd.Timestamp(dt))\n    if st is not None:\n        cache[pd.Timestamp(dt)] = st\n\nrebal_dates = [pd.Timestamp(d) for d in rebal_dates if pd.Timestamp(d) in cache]\nif len(rebal_dates) == 0:\n    raise ValueError(\"No valid rebalance dates after state construction.\")\n\nprint(f\"usable rebalance dates: {len(rebal_dates)}\")\nprint(f\"avg universe size: {np.mean([len(cache[d].tickers) for d in rebal_dates]):.1f}\")\n\n\ndef ew(dt, st, w_prev):\n    return pf.weights_equal(st[\"tickers\"], w_min=0.0, w_max=0.25, long_only=True)\n\n\ndef make_minvar(cov_key: str):\n    def _fn(dt, st, w_prev):\n        return pf.weights_minvar(cov_ann=st[\"cov_ann_map\"][cov_key], w_prev=w_prev,\n                                w_min=0.0, w_max=0.25, long_only=True,\n                                turnover_penalty_bps=10.0,solver_order=[\"osqp\", \"ecos\", \"scs\"])\n    return _fn\n\n\ndef make_mv(cov_key: str):\n    def _fn(dt, st, w_prev):\n        return pf.weights_mv(\n            mu_excess_ann=st[\"mu_excess_ann\"].values,\n            cov_ann=st[\"cov_ann_map\"][cov_key],\n            w_prev=w_prev,\n            mv_lambda=4.0,\n            kappa_target_annual=0.20,\n            w_min=0.0,\n            w_max=0.25,\n            long_only=True,\n            turnover_penalty_bps=10.0,\n            solver_order=[\"osqp\", \"ecos\", \"scs\"],\n        )\n\n    return _fn\n\n\ndef ridge_mv_lw(dt, st, w_prev):\n    return pf.weights_ridge_mv(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"],\n        w_prev=w_prev,\n        ridge=1e-4,\n        mv_lambda=6.0,\n        kappa_target_annual=0.30,\n        w_min=0.0,\n        w_max=0.25,\n        long_only=True,\n        turnover_penalty_bps=10.0,\n        solver_order=[\"osqp\", \"ecos\", \"scs\"],\n    )\n\n\ndef maxsharpe_slsqp_lw(dt, st, w_prev):\n    return pf.weights_maxsharpe_slsqp(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"],\n        w_prev=w_prev,\n        w_min=0.0,\n        w_max=0.25,\n        long_only=True,\n        turnover_penalty_bps=10.0,\n        kappa_target_annual=0.30,\n    )\n\n\n\ndef maxsharpe_frontier_lw(dt, st, w_prev):\n    return pf.weights_maxsharpe_frontier_grid(\n        mu_excess_ann=st[\"mu_excess_ann\"].values,\n        cov_ann=st[\"cov_ann_map\"][\"lw\"],\n        w_prev=w_prev,\n        grid_n=25,\n        w_min=0.0,\n        w_max=0.25,\n        long_only=True,\n        turnover_penalty_bps=10.0,\n        solver_order=[\"osqp\", \"ecos\", \"scs\"],\n    )\n\n\nstrategy_fns = {\n    \"ew\": ew,\n    \"minvar_sample\": make_minvar(\"sample\"),\n    \"minvar_lw\": make_minvar(\"lw\"),\n    \"minvar_oas\": make_minvar(\"oas\"),\n    \"minvar_ewma\": make_minvar(\"ewma\"),\n    \"mv_sample\": make_mv(\"sample\"),\n    \"mv_lw\": make_mv(\"lw\"),\n    \"mv_oas\": make_mv(\"oas\"),\n    \"mv_ewma\": make_mv(\"ewma\"),\n    \"ridge_mv\": ridge_mv_lw,\n    \"maxsharpe_slsqp\": maxsharpe_slsqp_lw,\n    \"maxsharpe_frontier\": maxsharpe_frontier_lw,\n}\n\ncov_key_for_rc = {\n    \"ew\": \"lw\",\n    \"minvar_sample\": \"sample\",\n    \"minvar_lw\": \"lw\",\n    \"minvar_oas\": \"oas\",\n    \"minvar_ewma\": \"ewma\",\n    \"mv_sample\": \"sample\",\n    \"mv_lw\": \"lw\",\n    \"mv_oas\": \"oas\",\n    \"mv_ewma\": \"ewma\",\n    \"ridge_mv\": \"lw\",\n    \"maxsharpe_slsqp\": \"lw\",\n    \"maxsharpe_frontier\": \"lw\",\n}\n\n\nresults = {\n    name: pf.backtest(\n        returns=returns,\n        rebal_dates=rebal_dates,\n        cache=cache,\n        weight_fn=fn,\n        cost_bps=10.0,\n        rf_daily=rf_daily,\n    )\n    for name, fn in strategy_fns.items()\n}\n\nprint(f\"computed strategies: {len(results)}\")\nprint(sorted(results.keys()))\n\n\nbest_name, sharpes = pf.best_strategy_by_sharpe(results, rf_daily=rf_daily, annualization=252.0)\n\n\ncolors = pl.make_color_map(results.keys(), pl.LAB_COLORS)\nbest_color = colors[best_name]\n\nfig, axes = plt.subplots(4, 2, figsize=(16, 22))\n\npl.plot_net_equity(axes[0, 0], results[best_name], name=best_name, color=best_color)\npl.plot_drawdown(axes[0, 1], results[best_name], name=best_name, color=best_color)\npl.plot_top_weights(axes[1, 0], results[best_name], name=best_name, color=best_color, k=10)\npl.plot_top_risk_contrib(\n    axes[1, 1],\n    results[best_name],\n    cache,\n    cov_key=cov_key_for_rc[best_name],\n    name=best_name,\n    color=best_color,\n    k=10,\n)\n\npl.plot_net_equity_compare(axes[2, 0], results, colors=colors, title=\"Net Equity (Comparison)\")\npl.plot_drawdown_compare(axes[2, 1], results, colors=colors, title=\"Drawdown (Comparison)\")\npl.plot_risk_return_scatter(\n    axes[3, 0],\n    results,\n    rf_daily=rf_daily,\n    annualization=252.0,\n    colors=colors,\n    title=\"Realized Risk-Return (Net)\",\n)\npl.plot_sharpe_compare(\n    axes[3, 1],\n    results,\n    rf_daily=rf_daily,\n    annualization=252.0,\n    colors=colors,\n    title=\"Realized Sharpe (Net)\",\n)\n\nplt.tight_layout()\nplt.show()\n\nmetrics_df, trade_df = pf.summarize_results(results, rf_daily=rf_daily, annualization=252.0)\n\nprint(\"Risk/Return Summary (Net)\")\ndisplay(metrics_df)\n\nprint(\"Trading & Stability Summary\")\ndisplay(trade_df)\n\nprint(f\"best sharpe: {best_name} | sharpe: {sharpes[best_name]:.4f}\")\n\n\n\n\n\nclose_prices: (2478, 146) | volumes: (2478, 146)\nDate range: 2016-01-04 to 2026-01-28\nusable rebalance dates: 109\navg universe size: 85.4\ncomputed strategies: 12\n['ew', 'maxsharpe_frontier', 'maxsharpe_slsqp', 'minvar_ewma', 'minvar_lw', 'minvar_oas', 'minvar_sample', 'mv_ewma', 'mv_lw', 'mv_oas', 'mv_sample', 'ridge_mv']\n\n\n\n\n\n\n\n\n\nRisk/Return Summary (Net)\n\n\n\n\n\n\n\n\n\nCAGR\nAnnVol\nSharpe\nMaxDD\nCalmar\nSortino\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\new\n0.105982\n0.219791\n0.390422\n-0.387785\n0.273300\n0.544422\n\n\nmaxsharpe_frontier\n0.237517\n0.251844\n0.818148\n-0.358117\n0.663237\n1.023069\n\n\nmaxsharpe_slsqp\n0.235413\n0.286673\n0.737755\n-0.400855\n0.587277\n1.102906\n\n\nminvar_ewma\n0.088852\n0.126843\n0.425015\n-0.285126\n0.311625\n0.541686\n\n\nminvar_lw\n0.043738\n0.126391\n0.091282\n-0.340885\n0.128307\n0.116274\n\n\nminvar_oas\n0.045013\n0.124523\n0.100600\n-0.345664\n0.130223\n0.127549\n\n\nminvar_sample\n0.043852\n0.124010\n0.091446\n-0.358684\n0.122259\n0.116092\n\n\nmv_ewma\n0.033584\n0.160818\n0.043576\n-0.466487\n0.071994\n0.056855\n\n\nmv_lw\n0.064596\n0.169217\n0.224286\n-0.360666\n0.179102\n0.299885\n\n\nmv_oas\n0.068789\n0.167429\n0.248372\n-0.360432\n0.190852\n0.331193\n\n\nmv_sample\n0.066366\n0.165987\n0.235358\n-0.363971\n0.182338\n0.313857\n\n\nridge_mv\n0.066379\n0.171982\n0.233142\n-0.367630\n0.180558\n0.311168\n\n\n\n\n\n\n\nTrading & Stability Summary\n\n\n\n\n\n\n\n\n\nAvg Turnover\nTotal Turnover\nTotal Costs\nCost % Final Value\nAvg HHI\nEffective N\nFallbacks\n\n\nStrategy\n\n\n\n\n\n\n\n\n\n\n\new\n0.041872\n4.564092\n0.006441\n0.002657\n0.012005\n83.300144\n0\n\n\nmaxsharpe_frontier\n0.426340\n46.471081\n0.134328\n0.020638\n0.146748\n6.814389\n0\n\n\nmaxsharpe_slsqp\n0.080928\n8.821199\n0.019806\n0.003089\n0.122346\n8.173522\n0\n\n\nminvar_ewma\n0.425926\n46.425963\n0.069148\n0.032722\n0.131061\n7.630059\n0\n\n\nminvar_lw\n0.109285\n11.912089\n0.014329\n0.009836\n0.084946\n11.772124\n0\n\n\nminvar_oas\n0.099307\n10.824511\n0.012957\n0.008799\n0.105025\n9.521534\n0\n\n\nminvar_sample\n0.104921\n11.436394\n0.013753\n0.009431\n0.132236\n7.562213\n0\n\n\nmv_ewma\n0.029402\n3.204796\n0.003775\n0.002823\n0.070458\n14.192867\n0\n\n\nmv_lw\n0.011909\n1.298131\n0.001828\n0.001054\n0.035090\n28.498301\n0\n\n\nmv_oas\n0.010871\n1.184981\n0.001651\n0.000920\n0.037956\n26.346094\n0\n\n\nmv_sample\n0.011250\n1.226279\n0.001688\n0.000960\n0.039383\n25.391786\n0\n\n\nridge_mv\n0.009402\n1.024805\n0.001501\n0.000853\n0.032039\n31.211513\n0\n\n\n\n\n\n\n\nbest sharpe: maxsharpe_frontier | sharpe: 0.8181",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2. Portfolio Optimization with Mean–Variance Models</span>"
    ]
  }
]