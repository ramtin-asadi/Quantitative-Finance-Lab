[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Finance Lab",
    "section": "",
    "text": "1 Quantitative Finance Lab\nWelcome to the quantitative finance lab. This book documents methods, experiments, and results.\n\nNotebooks will be added later under /notebooks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quantitative Finance Lab</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "2  Overview",
    "section": "",
    "text": "This project is a workspace for quantitative finance experiments and reproducible research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html",
    "href": "../notebooks/01_yield_curve_construction.html",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "",
    "text": "3.1 1) Bonds, Coupons, Treasuries, and Par Yields\nin this project we cover: - Treasury/par yields and coupon bonds\n- time conversion (tenors → tensors)\n- discount factors (DF), zero rates, forward rates\n- yield-curve construction via bootstrapping\n- alternative curve models: log-linear DF, PCHIP, Nelson–Siegel–Svensson, QP curve - synthetic bond issuance and rolling books - scenario shocks and pricing under scenarios - risk measures: PV01/DV01, convexity, key-rate duration (KRD), key-rate convexity - carry, rolldown, PnL, PnL attribution - hedging via DV01 and KRD —",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#bonds-coupons-treasuries-and-par-yields",
    "href": "../notebooks/01_yield_curve_construction.html#bonds-coupons-treasuries-and-par-yields",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "",
    "text": "3.1.1 1.1 Fixed-coupon bond cashflows\nA standard coupon bond is defined by: - notional \\(N\\) (principal. the money that you get back) - annual coupon rate \\(c\\) (the interest rate of the bond) - maturity \\(T\\) (the time that bond ends and you get back principal and interest) - coupon frequency \\(f\\) (the amount of payments per year)\nCoupon payment each period is \\(\\dfrac{c}{f}N\\).\nPayment times are \\(t_i=\\dfrac{i}{f}\\) for \\(i=1,2,\\dots,n\\)      where \\(n=fT\\)\nCashflows for each period \\(i\\) is \\(CF_i=\\dfrac{c}{f}N\\)         \\(i=1,\\dots,n-1\\)\nfinal cash flow: \\(CF_n=\\dfrac{c}{f}N+N\\)\n\n\n3.1.2 1.2 Price from discount factors\nFor calculating price of a bond we need to discount all the cash flows to today value to compute the present value of the bond. for that we need to have a discount rate for every \\(t\\) that a cash flow accurs. this comes from a continous function of time which is discount curve \\(D(t)\\), the price is \\(P=\\sum_{i=1}^{n} CF_i\\,D(t_i)\\)\n\n\n3.1.3 1.3 Year fraction from dates\nGiven dates \\(d_0\\) and \\(d_1\\), a day-count year fraction is \\(\\tau(d_0,d_1)=\\dfrac{\\text{days}(d_0,d_1)}{365}\\)\n\n\n3.1.4 1.4 Tenor mapping\nTenor labels map to maturities: - \\(k\\) M → \\(T=k/12\\) - \\(k\\) Y → \\(T=k\\)\nCollect maturities into a numeric vector: \\(\\mathbf{T}=(T_1,T_2,\\dots,T_m)\\)\nObserved market par yields: \\(\\mathbf{y}=(y_1,y_2,\\dots,y_m)\\) with \\(y_j=y(T_j)\\)\n\n\n3.1.5 1.5 What “Treasury par yield” means\nA par yield at maturity \\(T\\) is the coupon rate \\(c(T)\\) that makes a standard coupon bond price equal to par: \\(P(T,c(T))=N\\) If we normalize \\(N=1\\), then par means \\(P=1\\).\nFor discounting, we use these yields but the problem is we have some of the standard maturities which may not be the same as other bonds. so that’s why we have to make this yield curve from the treasury maturities to be able to discount any time to present.\nyou can download the data used in this notebook here (treasury par yields from 1990 to 2026)\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nimport math\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom cycler import cycler\ncolors = [\"#069AF3\",\"#FE420F\", \"#00008B\", \"#008080\", \"#7BC8F6\",\"#800080\",\"#0072B2\",\"#008000\",\"#CC79A7\", \"#DC143C\", \"#04D8B2\"]\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)\nplt.rcParams.update({\n    \"figure.figsize\": (9, 4),\n    \"figure.dpi\": 300,\n    \"savefig.dpi\": 300,\n    \"axes.grid\": True,\n    \"grid.alpha\": 0.20,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 5,\n})\n\n\ndf = pd.read_csv(r\"E:\\daneshgah\\quantitative-finance-lab\\data\\par-yield-curve-rates-1990-2026.csv\")\n\ncol_map = {\"date\": \"Date\",\"1 mo\": \"1M\",\"2 mo\": \"2M\",\"3 mo\": \"3M\",\"4 mo\": \"4M\",\"6 mo\": \"6M\",\"1 yr\": \"1Y\",\"2 yr\": \"2Y\",\"3 yr\": \"3Y\",\"5 yr\": \"5Y\",\"7 yr\": \"7Y\",\"10 yr\": \"10Y\",\"20 yr\": \"20Y\",\"30 yr\": \"30Y\"}\n\ndf = df.rename(columns={k.lower(): v for k, v in col_map.items()})\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"Date\"], ).set_index(\"Date\").sort_index()\n\ndf\n\n\n\n\n\n\n\n\n1M\n2M\n3M\n4M\n6M\n1Y\n2Y\n3Y\n5Y\n7Y\n10Y\n20Y\n30Y\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-02\nNaN\nNaN\n7.83\nNaN\n7.89\n7.81\n7.87\n7.90\n7.87\n7.98\n7.94\nNaN\n8.00\n\n\n1990-01-03\nNaN\nNaN\n7.89\nNaN\n7.94\n7.85\n7.94\n7.96\n7.92\n8.04\n7.99\nNaN\n8.04\n\n\n1990-01-04\nNaN\nNaN\n7.84\nNaN\n7.90\n7.82\n7.92\n7.93\n7.91\n8.02\n7.98\nNaN\n8.04\n\n\n1990-01-05\nNaN\nNaN\n7.79\nNaN\n7.85\n7.79\n7.90\n7.94\n7.92\n8.03\n7.99\nNaN\n8.06\n\n\n1990-01-08\nNaN\nNaN\n7.79\nNaN\n7.88\n7.81\n7.90\n7.95\n7.92\n8.05\n8.02\nNaN\n8.09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2026-01-22\n3.79\n3.72\n3.71\n3.67\n3.61\n3.53\n3.61\n3.68\n3.85\n4.05\n4.26\n4.79\n4.84\n\n\n2026-01-23\n3.78\n3.72\n3.70\n3.67\n3.61\n3.53\n3.60\n3.67\n3.84\n4.03\n4.24\n4.78\n4.82\n\n\n2026-01-26\n3.77\n3.70\n3.67\n3.67\n3.62\n3.52\n3.56\n3.66\n3.82\n4.02\n4.22\n4.75\n4.80\n\n\n2026-01-27\n3.77\n3.70\n3.67\n3.66\n3.61\n3.50\n3.53\n3.65\n3.81\n4.03\n4.24\n4.79\n4.83\n\n\n2026-01-28\n3.76\n3.71\n3.68\n3.70\n3.63\n3.52\n3.56\n3.66\n3.83\n4.05\n4.26\n4.81\n4.85\n\n\n\n\n9024 rows × 13 columns\n\n\n\n\ntenor_cols = [\"1M\",\"2M\",\"3M\",\"4M\",\"6M\",\"1Y\",\"2Y\",\"3Y\",\"5Y\",\"7Y\",\"10Y\",\"20Y\",\"30Y\"]\nfor c in tenor_cols:\n    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\nfirst_valid = df[tenor_cols].apply(lambda s: s.first_valid_index())\navailability = pd.DataFrame({\n    \"tenor\": tenor_cols,\n    \"first_valid_date\": [first_valid[t] for t in tenor_cols],\n})\navailability[\"first_valid_date\"] = pd.to_datetime(availability[\"first_valid_date\"])\navailability = availability.sort_values(\"first_valid_date\")\n\n\n\nprint(\"\\nData shape:\", df.shape)\nprint(\"Date range:\", df.index.min().date(), \"to\", df.index.max().date())\ndisplay(df[tenor_cols].describe().T)\n\n\nplt.figure()\nfor c in df.columns:\n    plt.plot(df.index, df[c])\nplt.title(\"Par Yields Over Time\")\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Date\")\nplt.legend()\nplt.show()\n\n\nprint(\"First available date per tenor:\")\ndisplay(availability)\n\n\nData shape: (9024, 13)\nDate range: 1990-01-02 to 2026-01-28\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n1M\n6124.0\n1.669061\n1.834318\n0.00\n0.08\n0.98\n2.700\n6.02\n\n\n2M\n1819.0\n2.731045\n2.092637\n0.00\n0.14\n2.43\n4.635\n5.61\n\n\n3M\n9020.0\n2.793844\n2.279473\n0.00\n0.23\n2.74\n5.010\n8.26\n\n\n4M\n817.0\n4.826524\n0.600693\n3.58\n4.35\n4.77\n5.440\n5.64\n\n\n6M\n9023.0\n2.908257\n2.296244\n0.02\n0.41\n3.00\n5.100\n8.49\n\n\n1Y\n9023.0\n3.004545\n2.274365\n0.04\n0.56\n3.12\n5.025\n8.64\n\n\n2Y\n9023.0\n3.243374\n2.267325\n0.09\n0.94\n3.35\n4.990\n9.05\n\n\n3Y\n9023.0\n3.422187\n2.212582\n0.10\n1.37\n3.55\n5.050\n9.11\n\n\n5Y\n9023.0\n3.761102\n2.107014\n0.19\n1.81\n3.73\n5.380\n9.10\n\n\n7Y\n9023.0\n4.037397\n2.028457\n0.36\n2.22\n3.93\n5.600\n9.12\n\n\n10Y\n9023.0\n4.251091\n1.937830\n0.52\n2.61\n4.17\n5.710\n9.09\n\n\n20Y\n8084.0\n4.377290\n1.629305\n0.87\n2.89\n4.53\n5.540\n8.30\n\n\n30Y\n8029.0\n4.735390\n1.879145\n0.99\n3.08\n4.56\n6.130\n9.18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst available date per tenor:\n\n\n\n\n\n\n\n\n\ntenor\nfirst_valid_date\n\n\n\n\n2\n3M\n1990-01-02\n\n\n4\n6M\n1990-01-02\n\n\n5\n1Y\n1990-01-02\n\n\n6\n2Y\n1990-01-02\n\n\n7\n3Y\n1990-01-02\n\n\n8\n5Y\n1990-01-02\n\n\n9\n7Y\n1990-01-02\n\n\n10\n10Y\n1990-01-02\n\n\n12\n30Y\n1990-01-02\n\n\n11\n20Y\n1993-10-01\n\n\n0\n1M\n2001-07-31\n\n\n1\n2M\n2018-10-16\n\n\n3\n4M\n2022-10-19\n\n\n\n\n\n\n\n\nsample_dates = [\n    df.index[0],\n    df.index[len(df)//2],\n    df.index[-252],\n    df.index[df.index &lt;= pd.Timestamp(\"2007-03-01\")][-1],\n    df.index[-1]\n]\n\nx = np.arange(len(tenor_cols))  \n\nplt.figure()\nfor d in sample_dates:\n    y = df.loc[d, tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    plt.plot(x[mask], y.values[mask], marker=\"o\", label=d.strftime(\"%Y-%m-%d\"))\n\nplt.title(\"Yield Curve Snapshots (Par Yields)\")\nplt.xticks(x, tenor_cols)\nplt.ylabel(\"Yield (%)\")\nplt.xlabel(\"Tenor\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#discount-factors-zero-rates-and-forward-rates",
    "href": "../notebooks/01_yield_curve_construction.html#discount-factors-zero-rates-and-forward-rates",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.2 3) Discount Factors, Zero Rates, and Forward Rates",
    "text": "3.2 3) Discount Factors, Zero Rates, and Forward Rates\n\n3.2.1 3.1 Discount factor\n\\(D(t)\\) is the present value of receiving 1 unit of currency at time \\(t\\). For example what does 1 dollar in 20 years worth now.\n\n\n3.2.2 3.2 Zero rate (continuous compounding)\n\\(z(t)\\) is the constant rate that discounts a payment in \\(t\\) to present. for example, if we want to know discount factor of 1 dollar in 20 years we need an annual rate to compute the present value. that’s zero rate.\nDefine \\(z(t)\\) by \\(D(t)=e^{-z(t)t}\\)            \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\n\n3.2.3 3.3 Instantaneous forward rate\n\\(f(t)\\) is the slope of \\(z(t)\\) which tells us how much the rate of return is very close to the time of maturity. it is used because zero rate is smooth and in instant time we need to have exact forward rate\n\\(f(t)=-\\dfrac{d}{dt}\\ln D(t)\\)            \\(D(t)=\\exp\\left(-\\int_0^t f(u)\\,du\\right)\\)\n\n\n3.2.4 3.4 Discrete forward over an interval\nFor \\(t_1&lt;t_2\\), the continuously-compounded forward rate for the interval is \\(F(t_1,t_2)=\\dfrac{\\ln D(t_1)-\\ln D(t_2)}{t_2-t_1}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#par-yield-implied-by-a-curve",
    "href": "../notebooks/01_yield_curve_construction.html#par-yield-implied-by-a-curve",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.3 4) Par Yield Implied by a Curve",
    "text": "3.3 4) Par Yield Implied by a Curve\nThis is used for building yield curve and validation to see if the predicted rate is close to real rate based on curve. what is the rate that makes the price of bond (PV of cashflows) equal to 1?\nGiven a curve \\(D(t)\\), the par coupon rate for maturity \\(T\\) and frequency \\(f\\) solves\n\\(1=\\sum_{i=1}^{n}\\dfrac{c}{f}D(t_i)+D(T)\\)            \\(t_i=i/f\\), \\(n=fT\\).\n\\(1=\\dfrac{c}{f}\\sum_{i=1}^{n}D(t_i)+D(T)\\)\nand finally we get to \\(c=f\\,\\dfrac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nShort maturities (&lt;1Y) often use money-market conventions because they are mostly single payment. Two common ones: - continuous: \\(y=-\\dfrac{\\ln D(T)}{T}\\) - simple: \\(y=\\dfrac{1/D(T)-1}{T}\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#bootstrapping-discount-factors-from-par-yields",
    "href": "../notebooks/01_yield_curve_construction.html#bootstrapping-discount-factors-from-par-yields",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.4 5) Bootstrapping Discount Factors from Par Yields",
    "text": "3.4 5) Bootstrapping Discount Factors from Par Yields\nBootstrapping constructs \\(D(T)\\) at market tenors from observed par yields. in this way we can have a function of time to discount a payment in any maturity based on the real yields that we have.\n\n3.4.1 5.1 Short end (&lt;1Y) convention\nFor \\(T&lt;1\\) we use the money market convention again. for calculating discount factor:\n\ncontinuous convention: \\(D(T)=e^{-y(T)T}\\)\nsimple convention: \\(D(T)=\\dfrac{1}{1+y(T)T}\\)\n\n\n\n3.4.2 5.2 Bootstrapping for coupon tenors (T ≥ 1)\nLet \\(c=y(T)\\) be the market par yield at maturity \\(T\\) (used as coupon rate). With frequency \\(f\\) and cashflow times \\(t_i=i/f\\):\nPar condition (normalized notional 1): \\(1=\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)+\\left(1+\\dfrac{c}{f}\\right)D(T)\\)\nSolve for the new unknown \\(D(T)\\): \\(D(T)=\\dfrac{1-\\sum_{i=1}^{n-1}\\dfrac{c}{f}D(t_i)}{1+\\dfrac{c}{f}}\\)\nif we have the earlier coupons discount factor, we know everything on the right side of equation. so we can solve it and get to \\(D(T)\\)\nit’s called bootstrapping because we first compute the \\(D(T&lt;1Y)\\) with short end convention, then we use that to compute \\(D(1Y)\\) and then use them for \\(D(2Y)\\) until the last maturity (30Y)\n\n\n3.4.3 5.3 Interpolating discount factors at coupon dates\nBootstrapping needs \\(D(t_i)\\) at coupon dates, but you often only have DFs at pillar maturities.\nA robust choice is log-linear interpolation: If \\(T_a&lt;t&lt;T_b\\), then\n\\(\\ln D(t)=\\ln D(T_a)+\\dfrac{t-T_a}{T_b-T_a}\\left(\\ln D(T_b)-\\ln D(T_a)\\right)\\)\nSo \\(D(t)=\\exp\\left(\\ln D(t)\\right)\\)\n\ndf_dec = df.copy()\ndf_dec[tenor_cols] = df_dec[tenor_cols] / 100.0\n\nshort_end_convention = \"continuous\"\nf = 2\nmin_d = 1e-12\n\n\ndef labels_to_T(labels):\n    T = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        if labu.endswith(\"M\"):\n            T.append(int(labu[:-1]) / 12.0)\n        else:\n            T.append(float(int(labu[:-1])))\n    return np.array(T, dtype=float)\n\n\ndef get_par_from_row(row):\n    y = row[tenor_cols].astype(float)\n    mask = np.isfinite(y.values)\n    labels = [tenor_cols[i] for i in range(len(tenor_cols)) if mask[i]]\n    if len(labels) == 0:\n        return None\n\n    par = y.values[mask].astype(float)\n    T = labels_to_T(labels)\n\n    i = np.argsort(T)\n    T = T[i]\n    par = par[i]\n    labels = [labels[i] for i in i]\n\n    return T, par, labels\n\n\ndef get_par_for_date(date):\n    row = df_dec.loc[date]\n    return get_par_from_row(row)\n\n\ndef short_end_df(Ti, ri):\n    if short_end_convention == \"continuous\":\n        return math.exp(-ri * Ti)\n    return 1.0 / (1.0 + ri * Ti)\n\n\ndef price_error_loglinear(d_T, Ti, t_prev, d_prev, times_interp, c, pv_known):\n    d_T = max(float(d_T), min_d)\n    pv_interp = 0.0\n    if len(times_interp) &gt; 0:\n        w = (times_interp - t_prev) / (Ti - t_prev)\n        log_d = (1 - w) * np.log(d_prev) + w * np.log(d_T)\n        d_interp = np.exp(log_d)\n        pv_interp = np.sum((c / f) * d_interp)\n    return pv_known + pv_interp + d_T - 1.0\n\n\ndef solve_df_long_end(Ti, ri, d_map):\n    c = float(ri)\n    n = int(round(Ti * f))\n    times = np.array([k / f for k in range(1, n + 1)], dtype=float)\n\n\n    known_T = np.array(sorted(d_map.keys()), dtype=float)\n    known_D = np.array([d_map[t] for t in known_T], dtype=float)\n    known_D = np.clip(known_D, min_d, None)\n\n\n    t_prev = known_T[-1]\n    d_prev = known_D[-1]\n\n    times_known = times[times &lt;= t_prev + 1e-12]\n    times_interp = times[times &gt; t_prev + 1e-12]\n\n    pv_known = 0.0\n    if len(times_known) &gt; 0:\n        log_known_D = np.log(known_D)\n        log_df_known = np.interp(times_known, known_T, log_known_D)\n        d_known = np.exp(log_df_known)\n        pv_known = np.sum((c / f) * d_known)\n\n    lo = min_d\n    hi = d_prev\n    f_lo = price_error_loglinear(lo, Ti, t_prev, d_prev, times_interp, c, pv_known)\n    f_hi = price_error_loglinear(hi, Ti, t_prev, d_prev, times_interp, c, pv_known)\n\n    if f_lo * f_hi &gt; 0:\n    \n        log_known_D = np.log(known_D)\n        log_df_cpn = np.interp(\n            times[:-1],\n            known_T,\n            log_known_D,\n            left=log_known_D[0],\n            right=log_known_D[-1],\n        )\n        d_cpn = np.exp(log_df_cpn)\n        pv_coupons = np.sum((c / f) * d_cpn)\n        d_T = (1.0 - pv_coupons) / (1.0 + c / f)\n    else:\n        for _ in range(100):\n            mid = 0.5 * (lo + hi)\n            f_mid = price_error_loglinear(mid, Ti, t_prev, d_prev, times_interp, c, pv_known)\n            if f_lo * f_mid &lt;= 0:\n                hi = mid\n                f_hi = f_mid\n            else:\n                lo = mid\n                f_lo = f_mid\n            if abs(hi - lo) &lt; 1e-12:\n                break\n        d_T = 0.5 * (lo + hi)\n\n    return d_T\n\n\ndef bootstrap_from_inputs(T, par, labels, date=None):\n    d_map = {}\n\n    # short convention\n    for Ti, ri in zip(T, par):\n        if Ti &lt; 1.0:\n            d_T = short_end_df(Ti, ri)\n            d_map[Ti] = max(float(d_T), min_d)\n            continue\n\n        d_T = solve_df_long_end(Ti, ri, d_map)\n        if (not np.isfinite(d_T)) or (d_T &lt;= 0):\n            d_T = min_d\n        d_map[Ti] = max(float(d_T), min_d)\n\n    dfs = np.array([d_map[t] for t in T], dtype=float)\n\n    return {\n        \"date\": date,\n        \"T\": T,\n        \"par\": par,\n        \"labels\": labels,\n        \"dfs\": dfs,\n    }\n\n\ndef bootstrap_pillars(date):\n    result = get_par_for_date(date)\n    if result is None:\n        return None\n\n    T, par, labels = result\n    return bootstrap_from_inputs(T, par, labels, date=date)\n\n\n# we bootstrap discount factors at the last available date for now\nbase_date = df_dec.index[-1]\npillars = bootstrap_pillars(base_date)\n\nT = pillars[\"T\"]\npar = pillars[\"par\"]\nlabels = pillars[\"labels\"]\ndfs = pillars[\"dfs\"]\n\nprint(\"Base date:\", base_date.date())\nprint(\"Tenors used:\", labels)\nprint(\"First 5 pillar DFs:\", dfs)\n\nplt.figure()\nplt.plot(T, dfs, marker=\"o\")\nplt.title(\"Bootstrapped Discount Factors\")\nplt.xlabel(\"Maturity T\")\nplt.ylabel(\"Discount Factor D(t)\")\nplt.show()\n\nBase date: 2026-01-28\nTenors used: ['1M', '2M', '3M', '4M', '6M', '1Y', '2Y', '3Y', '5Y', '7Y', '10Y', '20Y', '30Y']\nFirst 5 pillar DFs: [0.99687157 0.99383574 0.99084219 0.98774241 0.98201372 0.96571989\n 0.93185753 0.89680276 0.82670445 0.75353422 0.65216175 0.37090153\n 0.22566195]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#turning-bootstrapped-pillars-into-a-full-curve",
    "href": "../notebooks/01_yield_curve_construction.html#turning-bootstrapped-pillars-into-a-full-curve",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.5 6) Turning Bootstrapped Pillars into a Full Curve",
    "text": "3.5 6) Turning Bootstrapped Pillars into a Full Curve\nAfter bootstrapping we have pillars \\((T_j, D(T_j))\\) or \\((T_j, z(T_j))\\). Now we want to efine continuous functions \\(D(t)\\) and \\(z(t)\\) for all \\(t\\).\n\n3.5.1 6.1 Method A: Log-linear discount factors\nInterpolate \\(\\ln D(t)\\) linearly between pillars (just like for the T we had. we do the same thing between them):\n\\(\\ln D(t)=\\text{linear interp of } \\{\\ln D(T_j)\\}\\)\nThen \\(D(t)=\\exp(\\ln D(t))\\)\nand \\(z(t)=-\\dfrac{\\ln D(t)}{t}\\)\n\ndef loglinear_curve(T, dfs):\n    loglinear_log_dfs = np.log(dfs)\n\n    loglinear_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    loglinear_log_df_grid = np.interp(\n        loglinear_grid,\n        T,\n        loglinear_log_dfs,\n        left=loglinear_log_dfs[0],\n        right=loglinear_log_dfs[-1],\n    )\n    loglinear_df_grid = np.exp(loglinear_log_df_grid)\n\n    # zero rate\n    loglinear_z_grid = -np.log(loglinear_df_grid) / loglinear_grid\n\n    # instantaneous forward rate\n    loglinear_fwd_grid = -np.gradient(np.log(loglinear_df_grid), loglinear_grid)\n\n    def loglinear_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(\n            t,\n            T,\n            loglinear_log_dfs,\n            left=loglinear_log_dfs[0],\n            right=loglinear_log_dfs[-1],\n        )\n        return np.exp(log_df)\n\n    return {\n        \"name\": \"Log-linear DF\",\n        \"grid\": loglinear_grid,\n        \"df_grid\": loglinear_df_grid,\n        \"z_grid\": loglinear_z_grid,\n        \"fwd_grid\": loglinear_fwd_grid,\n        \"df_func\": loglinear_df_func,\n        \"log_dfs\": loglinear_log_dfs,\n    }\n\n\nloglinear_curve_data = loglinear_curve(T, dfs)\nloglinear_grid = loglinear_curve_data[\"grid\"]\nloglinear_df_grid = loglinear_curve_data[\"df_grid\"]\nloglinear_z_grid = loglinear_curve_data[\"z_grid\"]\nloglinear_fwd_grid = loglinear_curve_data[\"fwd_grid\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"loglinear\"] = loglinear_curve_data\n\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_z_grid * 100.0)\nplt.title(\"Zero Curve (log-linear)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Zero Rate (%)\")\nplt.show()\n\nplt.figure()\nplt.plot(loglinear_grid, loglinear_fwd_grid * 100.0)\nplt.title(\"Instantaneous Forward Rate (derivative of ln(DF))\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Forward Rate (%)\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#curve-models-using-zero-rate-smoothing",
    "href": "../notebooks/01_yield_curve_construction.html#curve-models-using-zero-rate-smoothing",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.6 7) Curve Models Using Zero-Rate Smoothing",
    "text": "3.6 7) Curve Models Using Zero-Rate Smoothing\nInstead of interpolating \\(D\\), we can interpolate \\(z\\) and recover \\(D(t)=e^{-z(t)t}\\).\n\n3.6.1 7.1 PCHIP on zero rates (Piecewise Cubic Hermite Interpolating Polynomial)\nGiven nodes \\(x_j=T_j\\) and \\(y_j=z(T_j)\\), PCHIP builds a piecewise cubic polynomial on each interval:\n\\(p_j(t)=a_j(t-x_j)^3+b_j(t-x_j)^2+c_j(t-x_j)+d_j\\) for \\(t\\in[x_j,x_{j+1}]\\)\nConstraints include: - \\(p_j(x_j)=y_j\\) and \\(p_j(x_{j+1})=y_{j+1}\\) - first derivatives are chosen by shape-preserving slope rules to reduce overshoot\nwe define \\(z(t)=p_j(t)\\)\nthen \\(D(t)=e^{-z(t)t}\\)\n\nfrom scipy.interpolate import PchipInterpolator\n\n\ndef pchip_curve(T, dfs):\n    pchip_zeros = -np.log(np.clip(dfs, min_d, None)) / T\n\n    pchip_z = PchipInterpolator(T, pchip_zeros, extrapolate=True)\n    pchip_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    pchip_z_grid = pchip_z(pchip_grid)\n    pchip_df_grid = np.exp(-pchip_z_grid * pchip_grid)\n\n    pchip_fwd_grid = -np.gradient(\n        np.log(np.clip(pchip_df_grid, min_d, None)), pchip_grid\n    )\n\n    def pchip_df_func(t):\n        t = np.array(t, dtype=float)\n        z = pchip_z(t)\n        return np.exp(-z * t)\n\n    return {\n        \"name\": \"PCHIP zero\",\n        \"grid\": pchip_grid,\n        \"df_grid\": pchip_df_grid,\n        \"z_grid\": pchip_z_grid,\n        \"fwd_grid\": pchip_fwd_grid,\n        \"df_func\": pchip_df_func,\n        \"pillar_zeros\": pchip_zeros,\n    }\n\n\npchip_curve_data = pchip_curve(T, dfs)\ncurves[\"pchip\"] = pchip_curve_data\n\n\nplt.figure()\nplt.plot(T, pchip_curve_data[\"pillar_zeros\"] * 100.0, \"o\", label=\"pillar zeros\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"z_grid\"] * 100.0, \"-\", label=\"PCHIP zero\")\nplt.title(\"Zero Curve (PCHIP)\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(T, dfs, \"o\", label=\"Pillar DFs\")\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"df_grid\"], \"-\", label=\"DF from PCHIP\")\nplt.title(\"Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(pchip_curve_data[\"grid\"], pchip_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"Instantaneous Forward Rate\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#nelsonsiegelsvensson-nss-yield-curve",
    "href": "../notebooks/01_yield_curve_construction.html#nelsonsiegelsvensson-nss-yield-curve",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.7 8) Nelson–Siegel–Svensson (NSS) yield curve",
    "text": "3.7 8) Nelson–Siegel–Svensson (NSS) yield curve\nwe represent the continuous-compounded zero rate curve \\(z(t)\\) with a small number of parameters, then derive discount factors, par yields and forwards\n\n3.7.1 8.1 NSS zero-rate function\nFor maturity \\(t&gt;0\\), the NSS zero rate is:\n\\(z(t)=\\beta_0 +\\beta_1\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}\\right) +\\beta_2\\left(\\frac{1-e^{-t/\\tau_1}}{t/\\tau_1}-e^{-t/\\tau_1}\\right) +\\beta_3\\left(\\frac{1-e^{-t/\\tau_2}}{t/\\tau_2}-e^{-t/\\tau_2}\\right)\\)\nParameters:\n\n\\(\\beta_0\\) = long-run “level”\n\\(\\beta_1\\) = “slope” (short-end effect)\n\\(\\beta_2\\) = medium-term “curvature” (first hump)\n\\(\\beta_3\\) = additional curvature (second hump)\n\\(\\tau_1,\\tau_2&gt;0\\) control where humps occur\n\n\ndef nss_zero(t, b0,b1,b2,b3,tau1,tau2):\n    t = np.array(t, dtype=float)\n    x1 = t / tau1\n    x2 = t / tau2\n    L1 = (1.0 - np.exp(-x1)) / x1\n    C1 = L1 - np.exp(-x1)\n    C2 = (1.0 - np.exp(-x2)) / x2 - np.exp(-x2)\n    return b0 + b1*L1 + b2*C1 + b3*C2\n\n\n\n3.7.2 8.3 Par yield implied by NSS\nFor a coupon bond with maturity \\(T\\) and coupon frequency \\(f\\), coupon rate \\(c(T)\\) is the rate that makes the bond price equal to par (normalize notional to 1):\n\\(1=\\sum_{i=1}^{n}\\frac{c(T)}{f}D(t_i)+D(T)\\)\nSolve for \\(c(T)\\):\n\\(c(T)=f\\cdot\\frac{1-D(T)}{\\sum_{i=1}^{n}D(t_i)}\\)\nFor short maturities (money-market style), a common mapping is:\ncontinuous: \\(y(T)=-\\ln D(T)/T\\)\nsimple: \\(y(T)=(1/D(T)-1)/T\\)\nbut first we have to have \\(D(t)\\)\n\ndef par_from_d(df_func, T_list):\n    output = np.zeros_like(T_list, dtype=float)\n    for k, Tk in enumerate(T_list):\n        if Tk &lt; 1.0:\n            D = df_func(np.array([Tk], float))[0]\n            if short_end_convention == \"simple\":\n                output[k] = (1.0 / D - 1.0) / Tk\n            else:\n                output[k] = -np.log(D) / Tk\n        else:\n            n = int(round(Tk * f))\n            times = np.array([i / f for i in range(1, n + 1)], dtype=float)\n            dfs = df_func(times)\n            output[k] = f * (1.0 - dfs[-1]) / np.sum(dfs)\n    return output\n\n\n\n3.7.3 8.2 Discount factor and forward from NSS\nOnce we have \\(z(t)\\), using continuous compounding:\n\\(D(t)=e^{-z(t),t}\\)\nand the instantaneous forward rate is:\n\\(f(t)=-\\frac{d}{dt}\\ln D(t)\\)\nWith NSS you often compute \\(f(t)\\) numerically on a grid: \\(f(t_i)\\approx -\\frac{\\ln D(t_{i+1})-\\ln D(t_{i-1})}{t_{i+1}-t_{i-1}}\\)\n\n\n\n3.7.4 8.4 Calibrating NSS to market par yields\nGiven observed par yields \\(y^{mkt}(T_j)\\) at tenors \\(T_j\\), we choose parameters \\(\\theta=(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\tau_1,\\tau_2)\\) to minimize a least-squares objective:\n\\(\\min_{\\theta}\\sum_{j}\\left(c^{model}(T_j;\\theta)-y^{mkt}(T_j)\\right)^2\\)\nOptionally add regularization to discourage extreme shapes: \\(\\lambda\\sum_j (z''(t_j))^2\\) or bounds on parameters.\n\nfrom scipy.optimize import minimize\n\n\ndef nss_curve(T, par):\n    def obj(theta):\n        b0, b1, b2, b3, tau1, tau2 = theta\n        z = nss_zero(T, b0, b1, b2, b3, tau1, tau2)\n        dfs = np.exp(-z * T)\n\n        # we use log-linear DF interpolation on pillars for keeping it positive\n        log_dfs = np.log(np.clip(dfs, min_d, None))\n\n        def df_func(t):\n            t = np.array(t, dtype=float)\n            log_df = np.interp(t, T, log_dfs, left=log_dfs[0], right=log_dfs[-1])\n            return np.exp(log_df)\n\n        par_model = par_from_d(df_func, T)\n        err = par_model - par\n        return float(np.mean(err**2))\n\n    # initializing with a first guess. for long run level we need something like the long-run yield level. that's why we use long term yields as guess\n    b0_0 = float(np.nanmedian(par[-3:])) if len(par) &gt;= 3 else float(np.nanmedian(par))\n    x0 = np.array([b0_0, -0.02, 0.02, 0.01, 1.5, 5.0], dtype=float)\n\n    # we use a type of quasi-Newton method for nonlinear optimization of parameters\n    pred = minimize(obj, x0, method=\"L-BFGS-B\")\n    theta = pred.x\n\n    nss_grid = np.linspace(max(1 / 12, T.min()), 30.0, 1000)\n    nss_z_grid = nss_zero(nss_grid, *theta)\n    nss_df_grid = np.exp(-nss_z_grid * nss_grid)\n\n    nss_fwd_grid = -np.gradient(\n        np.log(np.clip(nss_df_grid, min_d, None)), nss_grid\n    )\n\n    def nss_df_func(t):\n        t = np.array(t, dtype=float)\n        return np.exp(-nss_zero(t, *theta) * t)\n\n    curve = {\n        \"name\": \"NSS\",\n        \"grid\": nss_grid,\n        \"df_grid\": nss_df_grid,\n        \"z_grid\": nss_z_grid,\n        \"fwd_grid\": nss_fwd_grid,\n        \"df_func\": nss_df_func,\n        \"theta\": theta,\n    }\n\n    # par fit check\n    z_p = nss_zero(T, *theta)\n    d_p = np.exp(-z_p * T)\n    log_d_p = np.log(np.clip(d_p, min_d, None))\n    df_func_p = lambda tt: np.exp(\n        np.interp(np.array(tt, float), T, log_d_p, left=log_d_p[0], right=log_d_p[-1])\n    )\n    par_fit = par_from_d(df_func_p, T)\n\n    return curve, par_fit, pred\n\n\nnss_curve_data, par_fit, pred = nss_curve(T, par)\ntheta = nss_curve_data[\"theta\"]\n\nif \"curves\" not in globals():\n    curves = {}\ncurves[\"nss\"] = nss_curve_data\n\nprint(\"final MSE:\", pred.fun)\nprint(\"theta = [b0,b1,b2,b3,tau1,tau2] =\", np.round(theta, 6))\n\nplt.figure()\nplt.plot(T, par * 100.0, \"o\", label=\"Market par\")\nplt.plot(T, par_fit * 100.0, \"-o\", label=\"NSS implied par\")\nplt.title(\"NSS Fit to Par Yields\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Par Yield\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"NSS Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(nss_curve_data[\"grid\"], nss_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"NSS Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nfinal MSE: 3.5093289069366877e-07\ntheta = [b0,b1,b2,b3,tau1,tau2] = [ 0.053115 -0.014698 -0.031572 -0.007968  1.500251  5.000021]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "href": "../notebooks/01_yield_curve_construction.html#qp-curve-smooth-discount-factors-under-exact-par-bond-fit",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "3.8 9) QP curve: smooth discount factors under exact par-bond fit",
    "text": "3.8 9) QP curve: smooth discount factors under exact par-bond fit\nAnother approach for building a yield curve that:\n\nmatches par-bond pricing equations exactly (like bootstrapping) or near-exactly (like NSS),\nis smooth,\nwill result in a positive and increasing DF curve.\n\ncan come from a Quadratic Program (QP) if the variables are discount factors on a grid and constraints are linear.\n\n3.8.1 9.1 Variables\nWe Pick a grid of cashflow times (like semiannual up to 30Y): \\(t_1,t_2,\\dots,t_M\\)\nwe want to get to $ = (D(t_1),,D(t_M)) $\n\n\n3.8.2 9.2 constraints\nFor a maturity \\(T\\) (present on the grid), par yield \\(c\\) and frequency \\(f\\):\n\\(1=\\sum_{i=1}^{n}\\frac{c}{f}D(t_i)+D(T)\\)\nThis is linear in \\(D(\\cdot)\\), so it becomes one row of: \\(A\\mathbf{d}=\\mathbf{1}\\)\nPositivity: \\(D(t_k)\\ge D_{min}\\)\nMonotone decreasing: \\(D(t_{k+1})\\le D(t_k)\\)\nThese are linear inequalities, so the problem stays convex and QP-solvable.\n\nimport cvxpy as cp\n\n\ndef qp_build_t_grid(T_obs, f):\n    T_max = float(np.max(T_obs))\n    n_grid = int(round(T_max * f))\n    t_grid = np.unique(\n        np.concatenate(\n            [\n                np.array([i / f for i in range(1, n_grid + 1)], dtype=float),\n                T_obs,\n            ]\n        )\n    )\n    t_grid = np.array(sorted(t_grid), dtype=float)\n    grid_index = {float(np.round(t, 10)): i for i, t in enumerate(t_grid)}\n    return t_grid, grid_index\n\n\ndef qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d):\n    d = cp.Variable(len(t_grid))\n\n    constraints = []\n    constraints += [d &gt;= min_d]\n    constraints += [d[1:] &lt;= d[:-1]]\n\n    for Tk, yk in zip(T_obs, par_mkt):\n        if Tk &lt; 1.0:\n            key = float(np.round(Tk, 10))\n            if key in grid_index:\n                i = grid_index[key]\n                df_target = float(np.exp(-yk * Tk))\n                constraints += [d[i] == df_target]\n\n    for Tk, ck in zip(T_obs, par_mkt):\n        if Tk &lt; 1.0:\n            continue\n        keyT = float(np.round(Tk, 10))\n        if keyT not in grid_index:\n            continue\n        iT = grid_index[keyT]\n        n = int(round(Tk * f))\n\n        coupon_idx = []\n        for j in range(1, n + 1):\n            key = float(np.round(j / f, 10))\n            coupon_idx.append(grid_index[key])\n\n        constraints += [cp.sum((ck / f) * d[coupon_idx]) + d[iT] == 1.0]\n\n    return d, constraints\n\n\n\n\n\n\n3.8.3 9.3 Smoothness objective (quadratic)\nA simple convex smoothness penalty is the squared second difference of Discount Factors:\n\\(\\min_{\\mathbf{d}} \\ |\\Delta^2\\mathbf{d}|_2^2\\)\nwhere \\(\\Delta^2 d_k = d_{k+2}-2d_{k+1}+d_k\\). (Discrete version)\nThis makes the optimizer prefer sequences of discount factors that have small curvature everywhere, which results a smooth DF curve with fewer oscillations and jumps.\nwe can also add a mild “keep close to a prior curve” penalty: \\(\\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\nTotal objective: \\(\\min_{\\mathbf{d}} \\ \\lambda|\\Delta^2\\mathbf{d}|_2^2 + \\epsilon|\\mathbf{d}-\\mathbf{d}^{prior}|_2^2\\)\n\n\n3.8.4 9.4 zero and forward curves\nOnce we have \\(D(t)\\) on a grid:\n\\(z(t_k)=-\\ln D(t_k)/t_k\\)\n\\(f(t_k)\\approx -\\dfrac{\\ln D(t_{k+1})-\\ln D(t_k)}{t_{k+1}-t_k}\\)\n\ndef qp_solve(t_grid, d, constraints, par_mkt, f, min_d):\n    lam = 1e4\n    eps = 1e-4\n    prior_rate = (\n        float(np.nanmedian(par_mkt[-3:])) if len(par_mkt) &gt;= 3 else float(np.nanmedian(par_mkt))\n    )\n    d_prior = np.exp(-prior_rate * t_grid)\n\n    d2 = d[2:] - 2 * d[1:-1] + d[:-2]\n    obj = cp.Minimize(lam * cp.sum_squares(d2) + eps * cp.sum_squares(d - d_prior))\n\n    prog = cp.Problem(obj, constraints)\n    prog.solve(solver=cp.OSQP)\n\n    d_sol = np.array(d.value).astype(float)\n    d_sol = np.clip(d_sol, min_d, None)\n\n    return d_sol, prog.status, prog.value\n\n\ndef qp_build_curve(t_grid, d_sol):\n    qp_grid = np.linspace(max(1 / 12, t_grid.min()), 30.0, 1000)\n    qp_log_d = np.log(d_sol)\n    qp_log_df_grid = np.interp(qp_grid, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n\n    qp_df_grid = np.exp(qp_log_df_grid)\n    qp_z_grid = -np.log(qp_df_grid) / np.maximum(qp_grid, 1e-8)\n    qp_fwd_grid = -np.gradient(np.log(qp_df_grid), qp_grid)\n\n    def qp_df_func(t):\n        t = np.array(t, dtype=float)\n        log_df = np.interp(t, t_grid, qp_log_d, left=qp_log_d[0], right=qp_log_d[-1])\n        return np.exp(log_df)\n\n    curve = {\n        \"name\": \"QP DF\",\n        \"grid\": qp_grid,\n        \"df_grid\": qp_df_grid,\n        \"z_grid\": qp_z_grid,\n        \"fwd_grid\": qp_fwd_grid,\n        \"df_func\": qp_df_func,\n    }\n\n    return curve\n\n\ndef qp_curve(labels, par_mkt, f=2, min_d=1e-10):\n    T_obs = []\n    for lab in labels:\n        labu = lab.upper().strip()\n        T_obs.append(int(labu[:-1]) / 12.0 if labu.endswith(\"M\") else float(int(labu[:-1])))\n    T_obs = np.array(T_obs, dtype=float)\n\n    idx = np.argsort(T_obs)\n    T_obs = T_obs[idx]\n    par_mkt = par_mkt[idx]\n    labels = [labels[i] for i in idx]\n\n    t_grid, grid_index = qp_build_t_grid(T_obs, f)\n    d, constraints = qp_build_constraints(t_grid, grid_index, T_obs, par_mkt, f, min_d)\n    d_sol, status, value = qp_solve(t_grid, d, constraints, par_mkt, f, min_d)\n\n    curve = qp_build_curve(t_grid, d_sol)\n\n    state = {\n        \"t_grid\": t_grid,\n        \"d_sol\": d_sol,\n        \"constraints\": constraints,\n        \"status\": status,\n        \"value\": value,\n    }\n\n    return curve, state\n\n\nqp_curve_data, qp_state = qp_curve(labels, par)\n\ncurves[\"qp\"] = qp_curve_data\n\nprint(\"status:\", qp_state[\"status\"], \",  value:\", qp_state[\"value\"])\n\n\nplt.figure()\nplt.plot(qp_state[\"t_grid\"], qp_state[\"d_sol\"], \"o\", markersize=3, label=\"QP DF\")\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"df_grid\"], \"-\", label=\"Discount Factor (log-linear)\")\nplt.title(\"QP Discount Factors\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Discount Factor\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"z_grid\"] * 100.0)\nplt.title(\"QP Zero Curve\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Zero Rate\")\nplt.show()\n\nplt.figure()\nplt.plot(qp_curve_data[\"grid\"], qp_curve_data[\"fwd_grid\"] * 100.0)\nplt.title(\"QP Instantaneous Forward\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"Forward Rate\")\nplt.show()\n\nstatus: optimal ,  value: 1.2368745747617473",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#synthetic-bond-issuance-issued-at-par",
    "href": "../notebooks/01_yield_curve_construction.html#synthetic-bond-issuance-issued-at-par",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.1 11) Synthetic Bond Issuance (Issued at Par)",
    "text": "4.1 11) Synthetic Bond Issuance (Issued at Par)\nbecause we don’t have official data for this part, We simulate a monthly issuance program using par yields to create a rolling book of bonds so we can analyze our models and implement the next topics. Each month, we pretend the Treasury issues new par bonds at a few maturities (like 2Y, 5Y, 10Y, 30Y). The coupon of each new bond is set to that month’s par yield at that maturity (from the dataset). Because coupon = par yield at issuance, each new bond starts at price approximately equal to 1 (par).\n\n4.1.1 Details of the bond\n\nfor each month \\(t_0\\) we create a new bond with maturity \\(T\\) and frequency \\(f\\)\n\\(f\\) = monthly\n\\(T = {2Y, 5Y, 10Y, 30Y}\\)\ncoupon: \\(c_d(T)=y_d(T)\\) (the market par yield at that date and maturity)\nnotional \\(N=1\\)\n\nBasically we buy bonds with 4 different maturities every month and keep it and get interest every month until maturity of those bonds. so we have 4 books for 4 different bonds (maturities). and our portfolio is based on these four books.\nSo each month the outgoing cashflow is buying the bonds and ingoing is all the interest and maybe principal of all the bonds that we have bought.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#pricing-an-issued-bond-at-a-later-date",
    "href": "../notebooks/01_yield_curve_construction.html#pricing-an-issued-bond-at-a-later-date",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.2 12) Pricing an Issued Bond at a Later Date",
    "text": "4.2 12) Pricing an Issued Bond at a Later Date\nIf issue date be \\(t_0\\) and valuation date be \\(t\\). Elapsed time is the amount of time that has been passed from the bond issued at \\(t_0\\) in valuation time \\(t\\): \\(\\Delta=\\tau(t_0,t)\\)\nreminder: \\(\\tau(t_0,t_1)=\\dfrac{\\text{days}(t_0,t_1)}{365}\\)\nOriginal scheduled payment times from issue are \\(t_i=i/f\\). we only price the bonds that \\(t_i&gt;\\Delta\\) because these are the cashflows that have accured up until time \\(t\\) and the remaining time to payment from valuation date for each cashflow is: \\(\\tau_i(t)=t_i-\\Delta\\)\nPrice using the curve at valuation date \\(t\\): \\(P_t=\\sum_{i:t_i&gt;\\Delta} CF_i\\,D_t(\\tau_i(t))\\)\nFor a book of issues \\(\\mathcal{B}_t\\) with weights \\(w_b\\) (we don’t have weights here): \\(PV_t=\\sum_{b\\in \\mathcal{B}_t} w_b\\,P_t(b)\\)\n\nissue_maturities = [2, 5, 10, 30]\nissue_labels = {2: \"2Y\", 5: \"5Y\", 10: \"10Y\", 30: \"30Y\"}\n\n\nmonth_end_curve = df_dec[tenor_cols].resample(\"M\").last()\nissue_dates = month_end_curve.index\n\n\ndef yearfrac(t0, t1):\n    return (t1 - t0).days / 365\n\n\ndef bond_cashflows(c, T, f=2):\n    times = np.arange(1 / f, T + 1e-9, 1 / f)\n    cfs = np.full_like(times, c / f)\n    cfs[-1] += 1.0\n    return times, cfs\n\n\ndef price_bond(df_func, times, cfs, delta):\n    mask = times &gt; delta + 1e-12\n    if not np.any(mask):\n        return 0.0\n    t_rem = times[mask] - delta\n    cf_rem = cfs[mask]\n    return float(np.sum(cf_rem * df_func(t_rem)))\n\n\n\nissuance_book = {T: [] for T in issue_maturities}\nfor d in issue_dates:\n    row = month_end_curve.loc[d]\n    for T in issue_maturities:\n        label = issue_labels[T]\n        c = float(row.get(label, np.nan))\n        if not np.isfinite(c):\n            continue\n        times, cfs = bond_cashflows(c, T, f)\n        issuance_book[T].append({\n            \"issue_date\": d,\n            \"coupon\": c,\n            \"times\": times,\n            \"cfs\": cfs,\n        })\n\nissuance_summary = pd.DataFrame({\n    \"n_bonds\": {T: len(issuance_book[T]) for T in issue_maturities},\n    \"first_issue\": {T: issuance_book[T][0][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n    \"last_issue\": {T: issuance_book[T][-1][\"issue_date\"] if len(issuance_book[T]) &gt; 0 else pd.NaT for T in issue_maturities},\n})\n\n\n\n\n\nmethods = [\"loglinear\", \"pchip\", \"nss\", \"qp\"]\n\npv_rows = []\nfailed = []\n\nfor date in issue_dates:\n    if date in df_dec.index:\n        curve_date = date\n    else:\n        idx = df_dec.index.searchsorted(date, side=\"right\") - 1\n        if idx &lt; 0:\n            continue\n        curve_date = df_dec.index[idx]\n\n    out = build_curves_for_date(curve_date)\n    pillars_d, curves_d, errors = out\n    failed.extend(errors)\n\n    for method, curve in curves_d.items():\n        df_func = curve[\"df_func\"]\n        for T in issue_maturities:\n            pv = 0.0\n            for bond in issuance_book[T]:\n                if bond[\"issue_date\"] &gt; date:\n                    break\n                delta = yearfrac(bond[\"issue_date\"], date)\n                pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n            pv_rows.append({\n                \"date\": date,\n                \"method\": method,\n                \"maturity\": T,\n                \"pv\": pv,\n            })\n\npv_df = pd.DataFrame(pv_rows)\n\npv_total = pv_df.groupby([\"date\", \"method\"], as_index=False)[\"pv\"].sum()\npv_total = pv_total.pivot(index=\"date\", columns=\"method\", values=\"pv\").sort_index()\n\npv_buckets = pv_df.pivot_table(index=\"date\", columns=[\"method\", \"maturity\"], values=\"pv\").sort_index()\n\n\nprint(\"Issuance summary:\")\ndisplay(issuance_summary)\n\nprint(\"Total PV by method (last day):\")\ndisplay(pv_total)\n\nprint(\"Bucket PV (last date)\")\nlast_date = pv_df[\"date\"].max()\nlast_bucket = pv_df[pv_df[\"date\"] == last_date].pivot_table(index=\"maturity\", columns=\"method\", values=\"pv\")\ndisplay(last_bucket)\n\n\n\nplt.figure()\nfor method in pv_total.columns:\n    plt.plot(pv_total.index, pv_total[method], label=method)\nplt.title(\"Synthetic Book Total PV (Monthly)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nplt.figure()\nlast_bucket.plot(kind=\"bar\", ax=plt.gca())\nplt.title(f\"Each bucket PV by Method ({last_date.date()})\")\nplt.xlabel(\"Maturity\")\nplt.ylabel(\"PV\")\nplt.legend()\nplt.show()\n\nIssuance summary:\n\n\n\n\n\n\n\n\n\nn_bonds\nfirst_issue\nlast_issue\n\n\n\n\n2\n433\n1990-01-31\n2026-01-31\n\n\n5\n433\n1990-01-31\n2026-01-31\n\n\n10\n433\n1990-01-31\n2026-01-31\n\n\n30\n386\n1990-01-31\n2026-01-31\n\n\n\n\n\n\n\nTotal PV by method (last day):\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\ndate\n\n\n\n\n\n\n\n\n1990-01-31\n4.000000\n3.999113\n4.000103\n4.000000\n\n\n1990-02-28\n8.005366\n8.000546\n8.007987\n8.005693\n\n\n1990-03-31\n12.001280\n11.996797\n12.001671\n12.002156\n\n\n1990-04-30\n15.837224\n15.837803\n15.837591\n15.838451\n\n\n1990-05-31\n20.346313\n20.334526\n20.349033\n20.350468\n\n\n...\n...\n...\n...\n...\n\n\n2025-09-30\n482.625893\n484.090601\n482.869042\n482.742994\n\n\n2025-10-31\n484.654607\n486.322452\n484.907361\n484.774511\n\n\n2025-11-30\n486.463642\n487.747181\n486.866489\n486.661585\n\n\n2025-12-31\n481.093084\n482.916141\n481.509976\n481.302655\n\n\n2026-01-31\n479.864694\n481.350374\n480.242463\n480.073580\n\n\n\n\n433 rows × 4 columns\n\n\n\nBucket PV (last date)\n\n\n\n\n\n\n\n\nmethod\nloglinear\nnss\npchip\nqp\n\n\nmaturity\n\n\n\n\n\n\n\n\n2\n24.265859\n24.277154\n24.267620\n24.267962\n\n\n5\n60.532641\n60.568344\n60.551228\n60.550904\n\n\n10\n115.984215\n115.715577\n116.040212\n116.028511\n\n\n30\n279.081978\n280.789298\n279.383402\n279.226203\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see all the four methods we used for building the yield curves, performed mostly like each other and the result is almost the same except some parts between 2000 and 2004.\nalso this is all the portfolio PV and shows how much of each book we have in our portfolio. it doesn’t mean P&L of our portfolio since we didn’t consider the code of buying all these bonds each month\nAs we can see, with more time to maturity, we have more PV for the bonds",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#carry-and-rolldown",
    "href": "../notebooks/01_yield_curve_construction.html#carry-and-rolldown",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.3 12) Carry and Rolldown",
    "text": "4.3 12) Carry and Rolldown\n\n4.3.1 12.1 Carry approximation\nCarry is the P&L we earn just by holding the bond over a short horizon if yields don’t move and stay the same.\nOver a short horizon \\([t_0,t_1]\\) with \\(\\Delta_t=\\tau(t_0,t_1)\\), an approximation for carry from coupon is:\n\\(Carry\\approx \\sum_{b\\in\\mathcal{B}_{t_0}} w_b\\,c_b\\,dt\\) This ignores exact coupon dates and day-count nuances but captures the main accrual effect.\n\n\n4.3.2 12.2 Rolldown\nRolldown is the price change due to time passing, holding the curve fixed. It’s the price change that happens because time passes, so even if the curve shape stays exactly the same, the bond’s remaining maturity shortens, and it rolls to a different point on the curve\nif base curve be the curve at \\(t_0\\) Price at \\(t_0\\):\n\\(P_0=\\sum_{i:t_i&gt;\\Delta_0} CF_i\\,D_{t_0}(t_i-\\Delta_0)\\)\nRoll price at \\(t_1\\) on the same curve (curve doesn’t change but the time to maturity is shorter):\n\\(P_{roll}=\\sum_{i:t_i&gt;\\Delta_1} CF_i\\,D_{t_0}(t_i-\\Delta_1)\\)\n\\(Rolldown=P_{roll}-P_0\\)\nif rates stay the same, our earnings from bond will be \\(Carry + Rolldown\\)\nwe use cutoff date for each date which means we only include bonds that were already in the portfolio as of this date for calculating the PV up to that date.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#book-and-portfolio-pnl",
    "href": "../notebooks/01_yield_curve_construction.html#book-and-portfolio-pnl",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.4 13) Book and Portfolio PnL",
    "text": "4.4 13) Book and Portfolio PnL\nOne-period PnL with carry: \\(PnL_{t_1}=PV_{t_1}-PV_{t_0}+Carry_{t_0\\to t_1}\\)\nPortfolio PnL across maturity buckets: \\(PnL^{port}_{t_1}=\\sum_M PnL_{t_1}(T)\\)\n\n\n#some helper functions\n\ndef iter_bonds_through(cutoff_date):\n    for T in issue_maturities:\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            yield bond\n\ndef cashflows_in_period(times, cfs, e0, e1):\n    times = np.asarray(times, dtype=float)\n    cfs = np.asarray(cfs, dtype=float)\n    m = (times &gt; e0 + 1e-12) & (times &lt;= e1 + 1e-12)\n    return float(np.sum(cfs[m])) if np.any(m) else 0.0\n\n\ndef book_pv_cutoff(valuation_date, df_func, cutoff_date):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], valuation_date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\ndef book_realized_cashflows(start_date, end_date, cutoff_date):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        cf = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            e0 = yearfrac(bond[\"issue_date\"], start_date)\n            e1 = yearfrac(bond[\"issue_date\"], end_date)\n            if e0 &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            cf += cashflows_in_period(bond[\"times\"], bond[\"cfs\"], e0, e1)\n        buckets[T] = cf\n        total += cf\n    return total, buckets\n\n\ndef book_accrued_carry(start_date, end_date, cutoff_date):\n    dt = yearfrac(start_date, end_date)\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        carry = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; cutoff_date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], start_date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            carry += bond[\"coupon\"] * dt\n        buckets[T] = carry\n        total += carry\n    return total, buckets\n\ndef shifted_df_func(df_func, shift_func):\n    def _f(t):\n        t = np.array(t, dtype=float)\n        return df_func(t) * np.exp(-shift_func(t) * t)\n    return _f\n\n\ndef key_bump_func(key, bump_bp=1.0):\n    values = np.zeros(len(issue_maturities), dtype=float)\n    key_idx = issue_maturities.index(key)\n    values[key_idx] = bump_bp / 10000.0\n\n    def shift(t):\n        t = np.array(t, dtype=float)\n        return np.interp(t, issue_maturities, values, left=0.0, right=0.0)\n\n    return shift\n\n\ndef curve_date_for(d):\n    if d in df_dec.index:\n        return d\n    idx = df_dec.index.searchsorted(d, side=\"right\") - 1\n    if idx &lt; 0:\n        return None\n    return df_dec.index[idx]\n\n\ndef book_pv(date, df_func):\n    total = 0.0\n    buckets = {}\n    for T in issue_maturities:\n        pv = 0.0\n        for bond in issuance_book[T]:\n            if bond[\"issue_date\"] &gt; date:\n                break\n            delta = yearfrac(bond[\"issue_date\"], date)\n            if delta &gt;= bond[\"times\"][-1] - 1e-12:\n                continue\n            pv += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], delta)\n        buckets[T] = pv\n        total += pv\n    return total, buckets\n\n\n\n\ndef issuance_cost(date, df_func):\n    cost = 0.0\n    for M in issue_maturities:\n        for bond in issuance_book[M]:\n            if bond[\"issue_date\"] == date:\n                cost += price_bond(df_func, bond[\"times\"], bond[\"cfs\"], 0.0)\n            elif bond[\"issue_date\"] &gt; date:\n                break\n    return cost\n\n\ncurves_cache = {}\n\ndef get_curves_for(date):\n    if date in curves_cache:\n        return curves_cache[date]\n    out = build_curves_for_date(date)\n    if out is None:\n        curves_cache[date] = None\n        return None\n    _, curves_d, _ = out\n    curves_cache[date] = curves_d\n    return curves_d\n\n\nmetrics_rows = []\n\nfor i in range(len(issue_dates) - 1):\n    date = issue_dates[i]\n    next_date = issue_dates[i + 1]\n\n    curve_date = curve_date_for(date)\n    curve_date_next = curve_date_for(next_date)\n    if curve_date is None or curve_date_next is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    curves_next = get_curves_for(curve_date_next)\n    if curves_d is None or curves_next is None:\n        continue\n\n    for method, curve in curves_d.items():\n        df0 = curve[\"df_func\"]\n        df1 = curves_next[method][\"df_func\"]\n\n        pv0, _ = book_pv_cutoff(date, df0, cutoff_date=date)\n        pv_roll, _ = book_pv_cutoff(next_date, df0, cutoff_date=date)\n        pv1, _ = book_pv_cutoff(next_date, df1, cutoff_date=date)\n\n        cash_real, _ = book_realized_cashflows(date, next_date, cutoff_date=date)\n        carry_accrued, _ = book_accrued_carry(date, next_date, cutoff_date=date)\n\n        rolldown = pv_roll - pv0\n        curve_move = pv1 - pv_roll\n\n        pnl_total = cash_real + (pv1 - pv0)\n\n\n        buy_cost_next = issuance_cost(next_date, df1)\n\n        metrics_rows.append({\"date\": date, \"method\": method, \"pv\": pv0, \"cashflows_real\": cash_real, \"carry_accrued\": carry_accrued, \"rolldown\": rolldown,\n                              \"curve_move\": curve_move, \"buy_cost_next\": buy_cost_next, \"pnl_total\": pnl_total, \n                              \"ret_total\": (pnl_total / pv0)})\n\nmetrics_df = pd.DataFrame(metrics_rows).set_index([\"date\", \"method\"]).sort_index()\n\nprint(\"details (last date)\")\ndisplay(metrics_df.tail(4))\n\n\nfig, axes = plt.subplots(2, 2, figsize=(9, 6), sharex=True)\naxes = axes.flatten()\nfor ax, method in zip(axes, sorted(metrics_df.index.get_level_values(\"method\").unique())):\n    data = metrics_df.xs(method, level=\"method\")\n    ax.plot(data.index, data[\"carry_accrued\"], label=\"carry (accrued and smooth)\")\n    ax.plot(data.index, data[\"cashflows_real\"], label=\"cashflows\")\n    ax.plot(data.index, data[\"rolldown\"], label=\"rolldown\")\n    ax.set_title(method)\n    ax.legend()\nplt.tight_layout()\nplt.show()\n\n\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"pnl_total\"].cumsum(), label= method)\nplt.title(\"P&L of the bonds book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"P&L\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\ndetails (last date)\n\n\n\n\n\n\n\n\n\n\npv\ncashflows_real\ncarry_accrued\nrolldown\ncurve_move\nbuy_cost_next\npnl_total\nret_total\n\n\ndate\nmethod\n\n\n\n\n\n\n\n\n\n\n\n\n2025-12-31\nloglinear\n481.093084\n5.54735\n1.601333\n-3.768032\n-1.460358\n4.000000\n0.318960\n0.000663\n\n\nnss\n482.916141\n5.54735\n1.601333\n-3.766954\n-1.794779\n3.995968\n-0.014384\n-0.000030\n\n\npchip\n481.509976\n5.54735\n1.601333\n-3.768514\n-1.499964\n4.000966\n0.278872\n0.000579\n\n\nqp\n481.302655\n5.54735\n1.601333\n-3.768994\n-1.460081\n4.000000\n0.318275\n0.000661",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#pv01-dv01-parallel-sensitivity",
    "href": "../notebooks/01_yield_curve_construction.html#pv01-dv01-parallel-sensitivity",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.5 15) PV01 / DV01 (Parallel Sensitivity)",
    "text": "4.5 15) PV01 / DV01 (Parallel Sensitivity)\nLet \\(\\Delta=0.0001\\) for a 1bp move. Parallel-shifted discount factor: \\(D^{up}(t)=\\exp(-(z(t)+\\Delta)t)\\)\nPV01: \\(PV01=P_0-P_{up}\\)\nFor a book, apply the same definition to book PV.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#parallel-convexity",
    "href": "../notebooks/01_yield_curve_construction.html#parallel-convexity",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.6 16) Parallel Convexity",
    "text": "4.6 16) Parallel Convexity\nUsing central differences with shift size \\(\\Delta\\): \\(Conv=\\dfrac{P_{dn}+P_{up}-2P_0}{P_0\\Delta^2}\\) where \\(P_{up}\\) uses \\(+\\Delta\\) and \\(P_{dn}\\) uses \\(-\\Delta\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#key-rate-duration-krd",
    "href": "../notebooks/01_yield_curve_construction.html#key-rate-duration-krd",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.7 17) Key-Rate Duration (KRD)",
    "text": "4.7 17) Key-Rate Duration (KRD)\n\n4.7.1 17.1 Key tenors\nChoose keys \\(\\mathbf{K}=(K_1,\\dots,K_q)\\), e.g. (2,5,10,30).\n\n\n4.7.2 17.2 Triangular bump functions\nDefine localized bump functions \\(b_j(t)\\) such that \\(b_j(K_j)=1\\) and \\(b_j(t)=0\\) outside a neighborhood.\nA triangular bump with edges \\(L_j&lt;R_j\\): \\(b_j(t)=0\\) for \\(t\\le L_j\\)\n\\(b_j(t)=\\dfrac{t-L_j}{K_j-L_j}\\) for \\(L_j&lt;t\\le K_j\\)\n\\(b_j(t)=\\dfrac{R_j-t}{R_j-K_j}\\) for \\(K_j&lt;t\\le R_j\\)\n\\(b_j(t)=0\\) for \\(t&gt;R_j\\)\n\n\n4.7.3 17.3 Bumped curve\nFor 1bp bump size \\(\\Delta=0.0001\\): \\(z^{(j)}(t)=z(t)+\\Delta b_j(t)\\) \\(D^{(j)}(t)=\\exp(-(z(t)+\\Delta b_j(t))t)\\)\n\n\n4.7.4 17.4 KRD definition\nLet \\(P^{(j)}\\) be the price under bump \\(j\\): \\(KRD_j=\\dfrac{P_0-P^{(j)}}{P_0\\Delta}\\)\n\n\n# PV01 / DV01 / Convexity and KRD (time series)\n\nrisk_rows = []\nkrd_rows = []\n\nfor (date, method), row in metrics_df.iterrows():\n    curve_date = _curve_date_for(date)\n    if curve_date is None:\n        continue\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    df_func = curves_d[method][\"df_func\"]\n    pv0 = row[\"pv\"]\n\n    shift_up = _shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = _shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up, _ = book_pv(date, shift_up)\n    pv_dn, _ = book_pv(date, shift_dn)\n\n    dv01 = (pv_dn - pv_up) / 2.0\n    pv01 = dv01\n    convexity = (pv_up + pv_dn - 2.0 * pv0) / (pv0 * (0.0001 ** 2)) if pv0 != 0 else np.nan\n\n    risk_rows.append({\n        \"date\": date,\n        \"method\": method,\n        \"pv01\": pv01,\n        \"dv01\": dv01,\n        \"convexity\": convexity,\n    })\n\n    for k in key_rates:\n        bump = _key_bump_func(k, bump_bp=1.0)\n        df_bump = _shifted_df_func(df_func, bump)\n        pv_bump, _ = book_pv(date, df_bump)\n        krd = -(pv_bump - pv0) / 0.0001\n        krd_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"key\": k,\n            \"krd\": krd,\n        })\n\nrisk_df = pd.DataFrame(risk_rows).set_index([\"date\", \"method\"]).sort_index()\nmetrics_df = metrics_df.join(risk_df)\n\nkrd_df = pd.DataFrame(krd_rows).set_index([\"date\", \"method\", \"key\"]).sort_index()\n\nprint(\"Risk metrics (last 5 rows)\")\ndisplay(metrics_df[[\"pv01\", \"dv01\", \"convexity\"]].tail())\n\n# DV01 time series (smoothed)\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    dv01_sm = data[\"dv01\"].rolling(6, min_periods=1).mean()\n    plt.plot(data.index, dv01_sm, label=f\"{method} (6m MA)\")\nplt.title(\"DV01 of Synthetic Book (6M Moving Avg)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"DV01\")\nplt.legend()\nplt.show()\n\n# Convexity time series\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"convexity\"], label=method)\nplt.title(\"Convexity of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Convexity\")\nplt.legend()\nplt.show()\n\n\n# KRD heatmap across time (per method)\nmethods = sorted(metrics_df.index.get_level_values(\"method\").unique())\nkrd_panel = krd_df.reset_index()\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\naxes = axes.flatten()\n\nfor ax, method in zip(axes, methods):\n    data = krd_panel[krd_panel[\"method\"] == method]\n    pivot = data.pivot(index=\"date\", columns=\"key\", values=\"krd\").reindex(columns=key_rates)\n    im = ax.imshow(pivot.values.T, aspect=\"auto\", origin=\"lower\")\n    ax.set_title(method)\n    ax.set_yticks(range(len(key_rates)))\n    ax.set_yticklabels([f\"{k}Y\" for k in key_rates])\n\n    # x-axis ticks\n    tick_idx = np.linspace(0, len(pivot.index) - 1, 6).astype(int)\n    ax.set_xticks(tick_idx)\n    ax.set_xticklabels([pivot.index[i].strftime(\"%Y\") for i in tick_idx])\n\nfig.colorbar(im, ax=axes.tolist(), label=\"KRD\")\nfig.suptitle(\"KRD Heatmap Across Time by Method\", y=1.02)\nplt.tight_layout()\nplt.show()\n\nRisk metrics (last 5 rows)\n\n\n\n\n\n\n\n\n\n\npv01\ndv01\nconvexity\n\n\ndate\nmethod\n\n\n\n\n\n\n\n2025-11-30\nqp\n0.363911\n0.363911\n108.683610\n\n\n2025-12-31\nloglinear\n0.355535\n0.355535\n106.693997\n\n\nnss\n0.359894\n0.359894\n108.347020\n\n\npchip\n0.355886\n0.355886\n106.656210\n\n\nqp\n0.355299\n0.355299\n106.372110\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# P&L and DV01-hedged performance\n\n# helper: DV01 of a par bond at time t\n\ndef bond_dv01(coupon, maturity, df_func):\n    times, cfs = bond_cashflows(coupon, maturity, freq=f)\n    pv0 = price_bond(df_func, times, cfs, 0.0)\n    shift_up = _shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = _shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up = price_bond(shift_up, times, cfs, 0.0)\n    pv_dn = price_bond(shift_dn, times, cfs, 0.0)\n    dv01 = (pv_dn - pv_up) / 2.0\n    return dv01\n\n\n# --- FIXED: single-bond total return over one period (includes realized cashflows) ---\n\ndef bond_total_return_one_period(coupon, maturity, df_func, df_func_next, dt):\n    times, cfs = bond_cashflows(coupon, maturity, freq=f)\n\n    pv0 = price_bond(df_func, times, cfs, elapsed=0.0)\n    pv1 = price_bond(df_func_next, times, cfs, elapsed=dt)\n\n    cash = cashflows_in_period(times, cfs, e0=0.0, e1=dt)\n    return cash + (pv1 - pv0)\n\n\n# --- FIXED: P&L and DV01-hedged performance (uses corrected metrics_df pnl_total) ---\n\npnl_rows = []\n\nfor i in range(len(issue_dates) - 1):\n    date = issue_dates[i]\n    next_date = issue_dates[i + 1]\n    dt = yearfrac(date, next_date)\n\n    curve_date = _curve_date_for(date)\n    curve_date_next = _curve_date_for(next_date)\n    if curve_date is None or curve_date_next is None:\n        continue\n\n    curves_d = get_curves_for(curve_date)\n    curves_next = get_curves_for(curve_date_next)\n    if curves_d is None or curves_next is None:\n        continue\n\n    for method in curves_d.keys():\n        df0 = curves_d[method][\"df_func\"]\n        df1 = curves_next[method][\"df_func\"]\n\n        pnl_unhedged = metrics_df.loc[(date, method), \"pnl_total\"]\n        dv01_book = metrics_df.loc[(date, method), \"dv01\"]\n        pv0 = metrics_df.loc[(date, method), \"pv\"]\n\n        # hedge with a fresh 10Y par bond each month\n        hedge_coupon = float(df_dec.loc[curve_date, \"10Y\"]) if \"10Y\" in df_dec.columns else np.nan\n        if not np.isfinite(hedge_coupon):\n            continue\n\n        dv01_hedge = bond_dv01(hedge_coupon, 10.0, df0)\n        if dv01_hedge == 0:\n            continue\n\n        hedge_units = -dv01_book / dv01_hedge\n        hedge_pnl = hedge_units * bond_total_return_one_period(hedge_coupon, 10.0, df0, df1, dt)\n\n        pnl_hedged = pnl_unhedged + hedge_pnl\n\n        pnl_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"pnl_unhedged\": pnl_unhedged,\n            \"pnl_hedged\": pnl_hedged,\n            \"ret_unhedged\": (pnl_unhedged / pv0) if pv0 != 0 else np.nan,\n            \"ret_hedged\": (pnl_hedged / pv0) if pv0 != 0 else np.nan,\n            \"hedge_units\": hedge_units,\n        })\n\npnl_df = pd.DataFrame(pnl_rows).set_index([\"date\", \"method\"]).sort_index()\n\nprint(\"P&L (last 5 rows)\")\ndisplay(pnl_df.tail())\n\n# cumulative returns (more interpretable than raw cumulative pnl)\nmethods = sorted(pnl_df.index.get_level_values(\"method\").unique())\nfig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\naxes = axes.flatten()\n\nfor ax, method in zip(axes, methods):\n    data = pnl_df.xs(method, level=\"method\")\n    idx_un = (1.0 + data[\"ret_unhedged\"].fillna(0.0)).cumprod()\n    idx_hd = (1.0 + data[\"ret_hedged\"].fillna(0.0)).cumprod()\n    ax.plot(data.index, idx_un, label=\"unhedged (index)\")\n    ax.plot(data.index, idx_hd, linestyle=\"--\", label=\"DV01-hedged (index)\")\n    ax.set_title(method)\n    ax.legend()\n\nfig.suptitle(\"Cumulative Return Index: Unhedged vs DV01-Hedged\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True)\naxes = axes.flatten()\n\nfor ax, method in zip(axes, methods):\n    data = pnl_df.xs(method, level=\"method\")\n    vol_un = data[\"pnl_unhedged\"].rolling(12, min_periods=6).std()\n    vol_hedged = data[\"pnl_hedged\"].rolling(12, min_periods=6).std()\n    ax.plot(data.index, vol_un, label=\"unhedged\")\n    ax.plot(data.index, vol_hedged, linestyle=\"--\", label=\"DV01-hedged\")\n    ax.set_title(method)\n    ax.legend()\n\nfig.suptitle(\"12M Rolling P&L Volatility\", y=1.02)\nplt.tight_layout()\nplt.show()\n\nP&L (last 5 rows)\n\n\n\n\n\n\n\n\n\n\npnl_unhedged\npnl_hedged\nret_unhedged\nret_hedged\nhedge_units\n\n\ndate\nmethod\n\n\n\n\n\n\n\n\n\n2025-11-30\nqp\n-3.798280\n0.213678\n-0.007805\n0.000439\n-437.837990\n\n\n2025-12-31\nloglinear\n0.318960\n1.455535\n0.000663\n0.003025\n-430.904801\n\n\nnss\n-0.014384\n0.515017\n-0.000030\n0.001066\n-441.147500\n\n\npchip\n0.278872\n1.380838\n0.000579\n0.002868\n-431.267143\n\n\nqp\n0.318275\n1.420823\n0.000661\n0.002952\n-430.651846",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  },
  {
    "objectID": "../notebooks/01_yield_curve_construction.html#risk-and-sensitivity-measures-of-bonds",
    "href": "../notebooks/01_yield_curve_construction.html#risk-and-sensitivity-measures-of-bonds",
    "title": "3  1. Fixed-Income and yield curve construction",
    "section": "4.5 15) Risk and sensitivity measures of bonds",
    "text": "4.5 15) Risk and sensitivity measures of bonds\n\n4.5.1 15.1 PV01/ DV01\nfor measuring the risk of these bonds, we can use the sensitivity of the price of bond to a little change in interest rates.\nPV01 is the change in PV of the bond when the curve bumps by 1 basis point. DV01 is the value in dollar unit of that which is kind of the same concept\n\\(\\dfrac{dP}{dy}=-\\sum_{i=1}^n CF_i,t_i,D(t_i)\\)\nFor approximation, we take \\(\\Delta=0.0001\\) (1 basis point move), Parallel-shifted discount factor will be:\n\\(D^{up}(t)=\\exp(-(z(t)+\\Delta)t)\\)\nPV01 will be:\n\\(PV01=P_0-P_{up}\\)\n\n\n4.5.2 15.2 Convexity\nconvexity is basically the sensitivity of PV01 to a little change in curve. and it’s the second order partial derivative of price from yield\n\\(\\dfrac{d^2P}{dy^2}=\\sum_{i=1}^n CF_i,t_i^2,D(t_i)\\)\nUsing central differences with shift size \\(\\Delta\\):\n\\(Conv=\\dfrac{P_{down}+P_{up}-2P_0}{P_0\\Delta^2}\\) where \\(P_{up}\\) uses \\(+\\Delta\\) and \\(P_{down}\\) uses \\(-\\Delta\\).\n\n\n\n4.5.3 15.3 Key Rate Duration (KRD)\nthe problem is that we assume that yields parallel shift. but they can twist. if 2Y goes up it doesn’t mean 10Y will go up exactly the same amount. (Duration is a normalized version of PV01)\nSo we choose key tenors \\(k_1&lt;k_2&lt;\\dots&lt;k_m\\). as our key rates and measure the sensitivity of them while the other tenors in the curve stay the same.\nTent / triangular bump shape\nDefine localized bump functions \\(b_j(t)\\) such that \\(b_j(K_j)=1\\) and \\(b_j(t)=0\\) outside a neighborhood.\nA triangular bump with edges \\(L_j&lt;R_j\\):\n\\(b_j(t)=0\\) for \\(t\\le L_j\\)\n\\(b_j(t)=\\dfrac{t-L_j}{K_j-L_j}\\) for \\(L_j&lt;t\\le K_j\\)\n\\(b_j(t)=\\dfrac{R_j-t}{R_j-K_j}\\) for \\(K_j&lt;t\\le R_j\\)\n\\(b_j(t)=0\\) for \\(t&gt;R_j\\)\nBumped zero curve (only around key \\(k_j\\)):\n\\(z^{(j)}(t)=z(t)+\\Delta b_j(t)\\)\nSo bumped discount factors:\n\\(D^{(j)}(t)=\\exp(-(z(t)+\\Delta b_j(t))t)\\)\nPrice under the key bump:\n\\(P^{(j)}(\\Delta)=\\sum_{i=1}^n CF_i,D(t_i),e^{-\\Delta,b_j(t_i),t_i}\\)\nKRD definition (finite-difference)\n\\(KRD_j=\\dfrac{P_0-P^{(j)}}{P_0\\Delta}\\)\n\n\nrisk_rows = []\nkrd_rows = []\n\nfor (date, method), row in metrics_df.iterrows():\n    curve_date = curve_date_for(date)\n    if curve_date is None:\n        continue\n    curves_d = get_curves_for(curve_date)\n    if curves_d is None:\n        continue\n\n    df_func = curves_d[method][\"df_func\"]\n    pv0 = row[\"pv\"]\n\n    shift_up = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), 0.0001))\n    shift_dn = shifted_df_func(df_func, lambda t: np.full_like(np.array(t, dtype=float), -0.0001))\n    pv_up, _ = book_pv(date, shift_up)\n    pv_dn, _ = book_pv(date, shift_dn)\n\n    pv01 = (pv_dn - pv_up) / 2.0\n    convexity = (pv_up + pv_dn - 2.0 * pv0) / (pv0 * (0.0001 ** 2)) if pv0 != 0 else np.nan\n\n    risk_rows.append({\n        \"date\": date,\n        \"method\": method,\n        \"pv01\": pv01,\n        \"convexity\": convexity,\n    })\n\n    for k in issue_maturities:\n        bump = key_bump_func(k, bump_bp=1.0)\n        df_bump = shifted_df_func(df_func, bump)\n        pv_bump, _ = book_pv(date, df_bump)\n        krd = (pv0 - pv_bump) / 0.0001\n        krd_rows.append({\n            \"date\": date,\n            \"method\": method,\n            \"key\": k,\n            \"krd\": krd,\n        })\n\nrisk_df = pd.DataFrame(risk_rows).set_index([\"date\", \"method\"]).sort_index()\nmetrics_df = metrics_df.join(risk_df)\n\nkrd_df = pd.DataFrame(krd_rows).set_index([\"date\", \"method\", \"key\"]).sort_index()\n\nprint(\"Risk metrics (last date)\")\ndisplay(metrics_df[[\"pv01\", \"convexity\"]].tail(4))\n\nprint(\"KRD for last date\")\ndisplay(krd_df.tail(4))\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"pv01\"], label=method)\nplt.title(\"PV01 of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"PV01\")\nplt.legend()\nplt.show()\n\n\nplt.figure()\nfor method in sorted(metrics_df.index.get_level_values(\"method\").unique()):\n    data = metrics_df.xs(method, level=\"method\")\n    plt.plot(data.index, data[\"convexity\"], label=method)\nplt.title(\"Convexity of Synthetic Book\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Convexity\")\nplt.legend()\nplt.show()\n\n\n\nmethods = sorted(metrics_df.index.get_level_values(\"method\").unique())\nkrd_panel = krd_df.reset_index()\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\naxes = axes.flatten()\nfig.subplots_adjust(right=0.88)\n\nfor ax, method in zip(axes, methods):\n    data = krd_panel[krd_panel[\"method\"] == method]\n    pivot = data.pivot(index=\"date\", columns=\"key\", values=\"krd\").reindex(columns=issue_maturities)\n    im = ax.imshow(pivot.values.T, aspect=\"auto\", origin=\"lower\")\n    ax.set_title(method)\n    ax.set_yticks(range(len(issue_maturities)))\n    ax.set_yticklabels([f\"{k}Y\" for k in issue_maturities])\n\n    tick_idx = np.linspace(0, len(pivot.index) - 1, 6).astype(int)\n    ax.set_xticks(tick_idx)\n    ax.set_xticklabels([pivot.index[i].strftime(\"%Y\") for i in tick_idx])\n\ncax = fig.add_axes([0.90, 0.15, 0.02, 0.7])\nfig.colorbar(im, cax=cax, label=\"KRD\")\nfig.suptitle(\"KRD Heatmap Across Time\", y=1.02)\nplt.tight_layout(rect=[0, 0, 0.88, 1])\nplt.show()\n\nRisk metrics (last date)\n\n\n\n\n\n\n\n\n\n\npv01\nconvexity\n\n\ndate\nmethod\n\n\n\n\n\n\n2025-12-31\nloglinear\n0.355535\n106.693997\n\n\nnss\n0.359894\n108.347020\n\n\npchip\n0.355886\n106.656210\n\n\nqp\n0.355299\n106.372110\n\n\n\n\n\n\n\nKRD for last date\n\n\n\n\n\n\n\n\n\n\n\nkrd\n\n\ndate\nmethod\nkey\n\n\n\n\n\n2025-12-31\nqp\n2\n200.161193\n\n\n5\n591.057700\n\n\n10\n1568.888259\n\n\n30\n1065.070447",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1. Fixed-Income and yield curve construction</span>"
    ]
  }
]